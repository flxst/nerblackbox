{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview nerblackbox - a python package to fine-tune transformer-based language models for named entity recognition (NER). latest version: 0.0.13 Resources source code: https://github.com/flxst/nerblackbox documentation: https://flxst.github.io/nerblackbox PyPI: https://pypi.org/project/nerblackbox About Transformer-based language models like BERT have had a game-changing impact on natural language processing. In order to utilize HuggingFace's publicly accessible pretrained models for named entity recognition , one needs to retrain (or \"fine-tune\") them using labeled text. nerblackbox makes this easy. You give it a Dataset (labeled text) a Pretrained Model (transformers) and you get the best Fine-tuned Model its Performance on the dataset Installation pip install nerblackbox Usage initialize nerbb = NerBlackBox () fine-tune a model on a dataset nerbb . run_experiment ( \"my_experiment\" , model = \"bert-base-cased\" , dataset = \"conll2003\" ) # [output provides details on the model training] inspect the results experiment_results = nerbb . get_experiment_results ( \"my_experiment\" ) # [experiment_results contains e.g. details on the model performance] model inference model = nerbb . get_model_from_experiment ( \"my_experiment\" ) model . predict ( \"The United Nations has never recognised Jakarta's move.\" ) # [[ # {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'}, # {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'} # ]] See Usage for more details. Features adaptive fine-tuning hyperparameter search multiple runs with different random seeds detailed analysis of training results support for pretokenized and unpretokenized datasets support for various annotation schemes seamless use of datasets from HuggingFace Datasets special token support GPU support language agnosticism based on HuggingFace Transformers , PyTorch Lightning and MLflow See Features for more details. Citation @misc { nerblackbox, author = { Stollenwerk, Felix } , title = { nerblackbox: a python package to fine-tune transformer-based language models for named entity recognition } , year = { 2021 } , url = { https://github.com/flxst/nerblackbox } , }","title":"Overview"},{"location":"#overview","text":"nerblackbox - a python package to fine-tune transformer-based language models for named entity recognition (NER). latest version: 0.0.13","title":"Overview"},{"location":"#resources","text":"source code: https://github.com/flxst/nerblackbox documentation: https://flxst.github.io/nerblackbox PyPI: https://pypi.org/project/nerblackbox","title":"Resources"},{"location":"#about","text":"Transformer-based language models like BERT have had a game-changing impact on natural language processing. In order to utilize HuggingFace's publicly accessible pretrained models for named entity recognition , one needs to retrain (or \"fine-tune\") them using labeled text. nerblackbox makes this easy. You give it a Dataset (labeled text) a Pretrained Model (transformers) and you get the best Fine-tuned Model its Performance on the dataset","title":"About"},{"location":"#installation","text":"pip install nerblackbox","title":"Installation"},{"location":"#usage","text":"initialize nerbb = NerBlackBox () fine-tune a model on a dataset nerbb . run_experiment ( \"my_experiment\" , model = \"bert-base-cased\" , dataset = \"conll2003\" ) # [output provides details on the model training] inspect the results experiment_results = nerbb . get_experiment_results ( \"my_experiment\" ) # [experiment_results contains e.g. details on the model performance] model inference model = nerbb . get_model_from_experiment ( \"my_experiment\" ) model . predict ( \"The United Nations has never recognised Jakarta's move.\" ) # [[ # {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'}, # {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'} # ]] See Usage for more details.","title":"Usage"},{"location":"#features","text":"adaptive fine-tuning hyperparameter search multiple runs with different random seeds detailed analysis of training results support for pretokenized and unpretokenized datasets support for various annotation schemes seamless use of datasets from HuggingFace Datasets special token support GPU support language agnosticism based on HuggingFace Transformers , PyTorch Lightning and MLflow See Features for more details.","title":"Features"},{"location":"#citation","text":"@misc { nerblackbox, author = { Stollenwerk, Felix } , title = { nerblackbox: a python package to fine-tune transformer-based language models for named entity recognition } , year = { 2021 } , url = { https://github.com/flxst/nerblackbox } , }","title":"Citation"},{"location":"cli/cli/","text":"CLI (Command Line Interface) nerbb Usage: nerbb [OPTIONS] COMMAND [ARGS]... Options: --data_dir TEXT [str] relative path of data directory --modify / --no-modify [bool] if flag=set_up_dataset --val_fraction FLOAT [float] if flag=set_up_dataset --verbose / --no-verbose [bool] if flag=set_up_dataset --from_config / --no-from_config [bool] if flag=run_experiment --run_name TEXT [str] if flag=run_experiment --device TEXT [str] if flag=run_experiment --fp16 / --no-fp16 [bool] if flag=run_experiment --results / --no-results [bool] if flag=clear_data analyze_data analyze a dataset. Usage: nerbb analyze_data [OPTIONS] DATASET_NAME clear_data clear data (checkpoints and optionally results). Usage: nerbb clear_data [OPTIONS] download download & prepare built-in datasets, prepare experiment configuration. needs to be called exactly once before any other CLI/API commands of the package are executed in case built-in datasets shall be used. Usage: nerbb download [OPTIONS] get_experiment_results get results for a single experiment. Usage: nerbb get_experiment_results [OPTIONS] EXPERIMENT_NAME get_experiments get overview on experiments. Usage: nerbb get_experiments [OPTIONS] init initialize the data_dir directory. needs to be called exactly once before any other CLI/API commands of the package are executed. Usage: nerbb init [OPTIONS] mlflow show detailed experiment results in mlflow (port = 5000). Usage: nerbb mlflow [OPTIONS] predict predict labels for text_input using the best model of a single experiment. Usage: nerbb predict [OPTIONS] EXPERIMENT_NAME TEXT_INPUT predict_proba predict label probabilities for text_input using the best model of a single experiment. Usage: nerbb predict_proba [OPTIONS] EXPERIMENT_NAME TEXT_INPUT run_experiment run a single experiment. Usage: nerbb run_experiment [OPTIONS] EXPERIMENT_NAME set_up_dataset set up a dataset using the associated Formatter class. Usage: nerbb set_up_dataset [OPTIONS] DATASET_NAME DATASET_SUBSET_NAME show_experiment_config show a single experiment configuration in detail. Usage: nerbb show_experiment_config [OPTIONS] EXPERIMENT_NAME tensorboard show detailed experiment results in tensorboard. (port = 6006). Usage: nerbb tensorboard [OPTIONS]","title":"Overview"},{"location":"cli/cli/#cli-command-line-interface","text":"","title":"CLI (Command Line Interface)"},{"location":"cli/cli/#nerbb","text":"Usage: nerbb [OPTIONS] COMMAND [ARGS]... Options: --data_dir TEXT [str] relative path of data directory --modify / --no-modify [bool] if flag=set_up_dataset --val_fraction FLOAT [float] if flag=set_up_dataset --verbose / --no-verbose [bool] if flag=set_up_dataset --from_config / --no-from_config [bool] if flag=run_experiment --run_name TEXT [str] if flag=run_experiment --device TEXT [str] if flag=run_experiment --fp16 / --no-fp16 [bool] if flag=run_experiment --results / --no-results [bool] if flag=clear_data","title":"nerbb"},{"location":"cli/cli/#analyze_data","text":"analyze a dataset. Usage: nerbb analyze_data [OPTIONS] DATASET_NAME","title":"analyze_data"},{"location":"cli/cli/#clear_data","text":"clear data (checkpoints and optionally results). Usage: nerbb clear_data [OPTIONS]","title":"clear_data"},{"location":"cli/cli/#download","text":"download & prepare built-in datasets, prepare experiment configuration. needs to be called exactly once before any other CLI/API commands of the package are executed in case built-in datasets shall be used. Usage: nerbb download [OPTIONS]","title":"download"},{"location":"cli/cli/#get_experiment_results","text":"get results for a single experiment. Usage: nerbb get_experiment_results [OPTIONS] EXPERIMENT_NAME","title":"get_experiment_results"},{"location":"cli/cli/#get_experiments","text":"get overview on experiments. Usage: nerbb get_experiments [OPTIONS]","title":"get_experiments"},{"location":"cli/cli/#init","text":"initialize the data_dir directory. needs to be called exactly once before any other CLI/API commands of the package are executed. Usage: nerbb init [OPTIONS]","title":"init"},{"location":"cli/cli/#mlflow","text":"show detailed experiment results in mlflow (port = 5000). Usage: nerbb mlflow [OPTIONS]","title":"mlflow"},{"location":"cli/cli/#predict","text":"predict labels for text_input using the best model of a single experiment. Usage: nerbb predict [OPTIONS] EXPERIMENT_NAME TEXT_INPUT","title":"predict"},{"location":"cli/cli/#predict_proba","text":"predict label probabilities for text_input using the best model of a single experiment. Usage: nerbb predict_proba [OPTIONS] EXPERIMENT_NAME TEXT_INPUT","title":"predict_proba"},{"location":"cli/cli/#run_experiment","text":"run a single experiment. Usage: nerbb run_experiment [OPTIONS] EXPERIMENT_NAME","title":"run_experiment"},{"location":"cli/cli/#set_up_dataset","text":"set up a dataset using the associated Formatter class. Usage: nerbb set_up_dataset [OPTIONS] DATASET_NAME DATASET_SUBSET_NAME","title":"set_up_dataset"},{"location":"cli/cli/#show_experiment_config","text":"show a single experiment configuration in detail. Usage: nerbb show_experiment_config [OPTIONS] EXPERIMENT_NAME","title":"show_experiment_config"},{"location":"cli/cli/#tensorboard","text":"show detailed experiment results in tensorboard. (port = 6006). Usage: nerbb tensorboard [OPTIONS]","title":"tensorboard"},{"location":"features/adaptive_finetuning/","text":"Adaptive Fine-tuning Adaptive fine-tuning (introduced in this paper ) is a method that automatically trains for a near-optimal number of epochs. It is used by default in nerblackbox , and corresponds to the following hyperparameters : adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 The hyperparameters are also available as a preset .","title":"Adaptive Fine-tuning"},{"location":"features/adaptive_finetuning/#adaptive-fine-tuning","text":"Adaptive fine-tuning (introduced in this paper ) is a method that automatically trains for a near-optimal number of epochs. It is used by default in nerblackbox , and corresponds to the following hyperparameters : adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 The hyperparameters are also available as a preset .","title":"Adaptive Fine-tuning"},{"location":"features/detailed_results/","text":"Detailed Analysis of Training Results The main results of an experiment, especially the micro-averaged f1 score, can be accessed as follows: get main results Python experiment_results = nerbb . get_experiment_results ( \"<experiment_name>\" ) CLI nerbb get_experiment_results <experiment_name> # prints overview on runs Python: see ExperimentResults for details on how to use experiment_results In addition, on may have a look at much more detailed results of an experiment using mlflow or tensorboard . get detailed results (only CLI) CLI nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser mlflow displays precision, recall and f1 score for every single class, as well the respective micro- and macro-averages over all classes, both on the token and entity level. The following excerpt shows the micro- and macro-averages of the recall on the entity level precision, recall and f1 score for the LOC(ation) class on the token level In addition, one has access to the log file and the confusion matrices (token and entity level) of the model predictions on the test set. A small excerpt is shown below: tensorboard shows the learning curves of important metrics like the loss and the f1 score. A small excerpt is shown below:","title":"Detailed Analysis of Training Results"},{"location":"features/detailed_results/#detailed-analysis-of-training-results","text":"The main results of an experiment, especially the micro-averaged f1 score, can be accessed as follows: get main results Python experiment_results = nerbb . get_experiment_results ( \"<experiment_name>\" ) CLI nerbb get_experiment_results <experiment_name> # prints overview on runs Python: see ExperimentResults for details on how to use experiment_results In addition, on may have a look at much more detailed results of an experiment using mlflow or tensorboard . get detailed results (only CLI) CLI nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser mlflow displays precision, recall and f1 score for every single class, as well the respective micro- and macro-averages over all classes, both on the token and entity level. The following excerpt shows the micro- and macro-averages of the recall on the entity level precision, recall and f1 score for the LOC(ation) class on the token level In addition, one has access to the log file and the confusion matrices (token and entity level) of the model predictions on the test set. A small excerpt is shown below: tensorboard shows the learning curves of important metrics like the loss and the f1 score. A small excerpt is shown below:","title":"Detailed Analysis of Training Results"},{"location":"features/hyperparameter_search/","text":"Hyperparameter Search A hyperparameter grid search can easily be conducted as part of an experiment. The hyperparameters one wants to vary are to be specified in special sections [runA] , [runB] etc. in the experiment configuration file. Example: custom_experiment.ini (Hyperparameter Search) [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). See Hyperparameters for an overview of all hyperparameters.","title":"Hyperparameter Search"},{"location":"features/hyperparameter_search/#hyperparameter-search","text":"A hyperparameter grid search can easily be conducted as part of an experiment. The hyperparameters one wants to vary are to be specified in special sections [runA] , [runB] etc. in the experiment configuration file. Example: custom_experiment.ini (Hyperparameter Search) [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). See Hyperparameters for an overview of all hyperparameters.","title":"Hyperparameter Search"},{"location":"features/multiple_runs/","text":"Multiple Runs with Different Random Seeds The results of a fine-tuning run depend on the employed random seed, see e.g. this paper for a discussion. One may conduct multiple runs with different seeds that are otherwise identical, in order to get control over the uncertainties (see Detailed Analysis of Training Results ) get an improved model performance Multiple runs can easily be specified in the experiment configuration. Example: custom_experiment.ini (Settings / Multiple Runs) [settings] multiple_runs = 3 seed = 42 This creates 3 runs with seeds 43, 44 and 45.","title":"Multiple Runs with Different Random Seeds"},{"location":"features/multiple_runs/#multiple-runs-with-different-random-seeds","text":"The results of a fine-tuning run depend on the employed random seed, see e.g. this paper for a discussion. One may conduct multiple runs with different seeds that are otherwise identical, in order to get control over the uncertainties (see Detailed Analysis of Training Results ) get an improved model performance Multiple runs can easily be specified in the experiment configuration. Example: custom_experiment.ini (Settings / Multiple Runs) [settings] multiple_runs = 3 seed = 42 This creates 3 runs with seeds 43, 44 and 45.","title":"Multiple Runs with Different Random Seeds"},{"location":"features/overview/","text":"Overview In this section, we discuss some useful (and mostly unique) features of nerblackbox : Adaptive Fine-tuning Hyperparameter Search Multiple Runs with Different Random Seeds Detailed Analysis of Training Results Support for Pretokenized and Unpretokenized Datasets Support for Various Annotation Schemes Seamless Use of Datasets from HuggingFace Datasets Text Encoding","title":"Overview"},{"location":"features/overview/#overview","text":"In this section, we discuss some useful (and mostly unique) features of nerblackbox : Adaptive Fine-tuning Hyperparameter Search Multiple Runs with Different Random Seeds Detailed Analysis of Training Results Support for Pretokenized and Unpretokenized Datasets Support for Various Annotation Schemes Seamless Use of Datasets from HuggingFace Datasets Text Encoding","title":"Overview"},{"location":"features/support_annotation_schemes/","text":"Support for Various Annotation Schemes NER datasets come with different annotation schemes, e.g. IO, BIO and BILOU. nerblackbox automatically recognizes and uses the given scheme of a dataset if the (default) setting auto for the parameter annotation_scheme is used: annotation scheme auto [dataset] dataset_name = conll2003 annotation_scheme = auto The conll2003 dataset will be used in its original BIO scheme. It is also possible to let nerblackbox convert a dataset to another annotation scheme. annotation scheme conversion [dataset] dataset_name = conll2003 annotation_scheme = bilou The conll2003 dataset will be converted from its original BIO scheme to the BILOU scheme. Note however, that conversion is only possible between the BIO and BILOU schemes, as the IO scheme does not possess the same expressiveness.","title":"Support for Various Annotation Schemes"},{"location":"features/support_annotation_schemes/#support-for-various-annotation-schemes","text":"NER datasets come with different annotation schemes, e.g. IO, BIO and BILOU. nerblackbox automatically recognizes and uses the given scheme of a dataset if the (default) setting auto for the parameter annotation_scheme is used: annotation scheme auto [dataset] dataset_name = conll2003 annotation_scheme = auto The conll2003 dataset will be used in its original BIO scheme. It is also possible to let nerblackbox convert a dataset to another annotation scheme. annotation scheme conversion [dataset] dataset_name = conll2003 annotation_scheme = bilou The conll2003 dataset will be converted from its original BIO scheme to the BILOU scheme. Note however, that conversion is only possible between the BIO and BILOU schemes, as the IO scheme does not possess the same expressiveness.","title":"Support for Various Annotation Schemes"},{"location":"features/support_huggingface_datasets/","text":"Seamless Use of Datasets from HuggingFace Datasets A dataset from HuggingFace Datasets that is suitable for named entity recognition can be used just like any other built-in dataset , without further ado. static experiment definition experiment_huggingface_dataset.ini [dataset] dataset_name = ehealth_kd [model] pretrained_model_name = mrm8488/electricidad-base-discriminator run experiment with huggingface dataset (statically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , from_config = True ) dynamic experiment definition run experiment with huggingface dataset (dynamically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , model = \"mrm8488/electricidad-base-discriminator\" , dataset = \"ehealth_kd\" ) nerblackbox automatically determines whether the employed dataset uses the standard or pretokenized format.","title":"Seamless Use of Datasets from HuggingFace Datasets"},{"location":"features/support_huggingface_datasets/#seamless-use-of-datasets-from-huggingface-datasets","text":"A dataset from HuggingFace Datasets that is suitable for named entity recognition can be used just like any other built-in dataset , without further ado. static experiment definition experiment_huggingface_dataset.ini [dataset] dataset_name = ehealth_kd [model] pretrained_model_name = mrm8488/electricidad-base-discriminator run experiment with huggingface dataset (statically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , from_config = True ) dynamic experiment definition run experiment with huggingface dataset (dynamically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , model = \"mrm8488/electricidad-base-discriminator\" , dataset = \"ehealth_kd\" ) nerblackbox automatically determines whether the employed dataset uses the standard or pretokenized format.","title":"Seamless Use of Datasets from HuggingFace Datasets"},{"location":"features/support_pretokenized/","text":"Support for Different Data Formats Annotated data for named entity recognition in its purest form contains raw text together with a list of entities. Each entity is defined by its position in the raw text and the corresponding tag. A specific standard format which is commonly used in connection with annotation tools looks as follows: Example: standard format (*.jsonl) { \"text\": \"President Barack Obama went to Harvard\", \"tags\": [ { \"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22 }, { \"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38 } ] } Note that the above represents one sample that needs to constitute one line in the jsonl file. The indentations are used for convenience only. After the data is read in, the model's tokenizer is used to pretokenize it. At inference time, the model makes predictions on the pretokenized data. Subsequently, these predictions are mapped back to the original text. Often times, especially in the case of public datasets, the data already comes as pretokenized though. The pretokenized format looks as follows: Example: pretokenized format (*.csv) PER PER PER O O ORG <tab> President Barack Obama went to Harvard In the case of pretokenized data at inference time, the information to map the predictions back to the original text is missing. Hence, the last step in the above chart is skipped. nerblackbox can process both the standard and pretokenized format. See Custom Datasets for details on how to use a dataset in practice.","title":"Support for Different Data Formats"},{"location":"features/support_pretokenized/#support-for-different-data-formats","text":"Annotated data for named entity recognition in its purest form contains raw text together with a list of entities. Each entity is defined by its position in the raw text and the corresponding tag. A specific standard format which is commonly used in connection with annotation tools looks as follows: Example: standard format (*.jsonl) { \"text\": \"President Barack Obama went to Harvard\", \"tags\": [ { \"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22 }, { \"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38 } ] } Note that the above represents one sample that needs to constitute one line in the jsonl file. The indentations are used for convenience only. After the data is read in, the model's tokenizer is used to pretokenize it. At inference time, the model makes predictions on the pretokenized data. Subsequently, these predictions are mapped back to the original text. Often times, especially in the case of public datasets, the data already comes as pretokenized though. The pretokenized format looks as follows: Example: pretokenized format (*.csv) PER PER PER O O ORG <tab> President Barack Obama went to Harvard In the case of pretokenized data at inference time, the information to map the predictions back to the original text is missing. Hence, the last step in the above chart is skipped. nerblackbox can process both the standard and pretokenized format. See Custom Datasets for details on how to use a dataset in practice.","title":"Support for Different Data Formats"},{"location":"features/text_encoding/","text":"Text Encoding Text may contain whitespace characters (e.g. \"\\n\", \"\\t\") or special characters (e.g. \"\u2022\", emojis) that a pre-trained model has never seen before. While the whitespace characters are ignored in the tokenization process, the special characters lead to out-of-vocabulary tokens which get replaced by [UNK] tokens before being sent to the model. Sometimes, however, the ignored or replaced tokens contain semantic information that is valuable for the model and thus should be preserved. Therefore, nerblackbox allows to customly map selected special characters to self-defined special tokens (\"encoding\"). The encoded text may then be used during training and inference. Say we want to have the following replacements: encoding # map special characters to special tokens encoding = { ' \\n ' : '[NEWLINE]' , ' \\t ' : '[TAB]' , '\u2022' : '[DOT]' , } The first step is to save the encoding in an encoding.json file which is located in the same folder ./data/datasets/<custom_dataset> that contains the data (see Custom Datasets ). create encoding.json import json with open ( './data/datasets/<custom_dataset>/encoding.json' , 'w' ) as file : json . dump ( encoding , file ) This way, the special tokens are automatically added to the model's vocabulary during training. The second step is to apply the encoding to the data. The TextEncoder class takes care of this: TextEncoder from nerblackbox import TextEncoder text_encoder = TextEncoder ( encoding ) For training , one needs to encode the input text like so: text encoding (training) # ..load input_text # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' input_text_encoded , _ = text_encoder . encode ( input_text ) # ..save input_text_encoded and use it for training For inference , the predictions also need to be mapped back to the original text, like so: text encoding (inference) # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' # encode_decode_mappings = [(2, \"\\n\", \"[NEWLINE]\"), (13, \"\u2022\", \"[DOT]\")] input_text_encoded , encode_decode_mappings = text_encoder . encode ( input_text ) # PREDICT # e.g. predictions_encoded = {'char_start': 25, 'char_end': 34, 'token': 'Stockholm', 'tag': 'LOC'} predictions_encoded = model . predict ( input_text_encoded , level = \"entity\" ) # DECODE # e.g. input_text_decoded = 'We\\n are in \u2022 Stockholm' # predictions = {'char_start': 13, 'char_end': 22, 'token': 'Stockholm', 'tag': 'LOC'} input_text_decoded , predictions = text_encoder . decode ( input_text_encoded , encode_decode_mappings , predictions_encoded )","title":"Text Encoding"},{"location":"features/text_encoding/#text-encoding","text":"Text may contain whitespace characters (e.g. \"\\n\", \"\\t\") or special characters (e.g. \"\u2022\", emojis) that a pre-trained model has never seen before. While the whitespace characters are ignored in the tokenization process, the special characters lead to out-of-vocabulary tokens which get replaced by [UNK] tokens before being sent to the model. Sometimes, however, the ignored or replaced tokens contain semantic information that is valuable for the model and thus should be preserved. Therefore, nerblackbox allows to customly map selected special characters to self-defined special tokens (\"encoding\"). The encoded text may then be used during training and inference. Say we want to have the following replacements: encoding # map special characters to special tokens encoding = { ' \\n ' : '[NEWLINE]' , ' \\t ' : '[TAB]' , '\u2022' : '[DOT]' , } The first step is to save the encoding in an encoding.json file which is located in the same folder ./data/datasets/<custom_dataset> that contains the data (see Custom Datasets ). create encoding.json import json with open ( './data/datasets/<custom_dataset>/encoding.json' , 'w' ) as file : json . dump ( encoding , file ) This way, the special tokens are automatically added to the model's vocabulary during training. The second step is to apply the encoding to the data. The TextEncoder class takes care of this: TextEncoder from nerblackbox import TextEncoder text_encoder = TextEncoder ( encoding ) For training , one needs to encode the input text like so: text encoding (training) # ..load input_text # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' input_text_encoded , _ = text_encoder . encode ( input_text ) # ..save input_text_encoded and use it for training For inference , the predictions also need to be mapped back to the original text, like so: text encoding (inference) # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' # encode_decode_mappings = [(2, \"\\n\", \"[NEWLINE]\"), (13, \"\u2022\", \"[DOT]\")] input_text_encoded , encode_decode_mappings = text_encoder . encode ( input_text ) # PREDICT # e.g. predictions_encoded = {'char_start': 25, 'char_end': 34, 'token': 'Stockholm', 'tag': 'LOC'} predictions_encoded = model . predict ( input_text_encoded , level = \"entity\" ) # DECODE # e.g. input_text_decoded = 'We\\n are in \u2022 Stockholm' # predictions = {'char_start': 13, 'char_end': 22, 'token': 'Stockholm', 'tag': 'LOC'} input_text_decoded , predictions = text_encoder . decode ( input_text_encoded , encode_decode_mappings , predictions_encoded )","title":"Text Encoding"},{"location":"python_api/experiment_results/","text":"ExperimentResults class that contains results of a single experiment. __init__ ( self , _id = None , name = None , experiment = None , single_runs = None , average_runs = None , best_single_run = None , best_average_run = None ) special Parameters: Name Type Description Default _id Optional[str] e.g. '1' None name Optional[str] e.g. 'my_experiment' None experiment Optional[pandas.core.frame.DataFrame] overview on experiment parameters None single_runs Optional[pandas.core.frame.DataFrame] overview on run parameters & single results None average_runs Optional[pandas.core.frame.DataFrame] overview on run parameters & average results None best_single_run Optional[Dict] overview on best run parameters & single results None best_average_run Optional[Dict] overview on best run parameters & average results None from_mlflow_runs ( _runs , _experiment_id , _experiment_name ) classmethod Parameters: Name Type Description Default _runs List[mlflow.entities.run.Run] [List of mlflow.entities.Run] required _experiment_id str [str], e.g. '0' required _experiment_name str [str], e.g. 'my_experiment' required Returns: Type Description ExperimentResults ExperimentResults instance","title":"ExperimentResults"},{"location":"python_api/experiment_results/#experimentresults","text":"class that contains results of a single experiment.","title":"ExperimentResults"},{"location":"python_api/experiment_results/#nerblackbox.modules.experiment_results.ExperimentResults.__init__","text":"Parameters: Name Type Description Default _id Optional[str] e.g. '1' None name Optional[str] e.g. 'my_experiment' None experiment Optional[pandas.core.frame.DataFrame] overview on experiment parameters None single_runs Optional[pandas.core.frame.DataFrame] overview on run parameters & single results None average_runs Optional[pandas.core.frame.DataFrame] overview on run parameters & average results None best_single_run Optional[Dict] overview on best run parameters & single results None best_average_run Optional[Dict] overview on best run parameters & average results None","title":"__init__()"},{"location":"python_api/experiment_results/#nerblackbox.modules.experiment_results.ExperimentResults.from_mlflow_runs","text":"Parameters: Name Type Description Default _runs List[mlflow.entities.run.Run] [List of mlflow.entities.Run] required _experiment_id str [str], e.g. '0' required _experiment_name str [str], e.g. 'my_experiment' required Returns: Type Description ExperimentResults ExperimentResults instance","title":"from_mlflow_runs()"},{"location":"python_api/ner_model_predict/","text":"NerModelPredict class that predicts tags for given text __init__ ( self , checkpoint_directory , batch_size = 16 , max_seq_length = None ) special Parameters: Name Type Description Default checkpoint_directory str path required batch_size int used in dataloader 16 predict ( self , input_texts , level = 'entity' , autocorrect = False ) predict tags for input texts. output on entity or word level. Examples: predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=False) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"I-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"B-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=False) # [[ # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"ORG\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] Parameters: Name Type Description Default input_texts Union[str, List[str]] e.g. [\"example 1\", \"example 2\"] required level str \"entity\" or \"word\" 'entity' autocorrect bool if True, autocorrect annotation scheme (e.g. B- and I- tags). False Returns: Type Description List[List[Dict[str, Union[str, Dict]]]] predictions: [list] of predictions for the different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, tag predict_proba ( self , input_texts ) predict probability distributions for input texts. output on word level. Examples: predict_proba([\"arbetsf\u00f6rmedlingen finns i stockholm\"]) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"proba_dist: {\"O\": 0.21, \"B-ORG\": 0.56, ..}}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"proba_dist: {\"O\": 0.87, \"B-ORG\": 0.02, ..}}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"proba_dist: {\"O\": 0.95, \"B-ORG\": 0.01, ..}}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"proba_dist: {\"O\": 0.14, \"B-ORG\": 0.22, ..}}, # ]] Parameters: Name Type Description Default input_texts Union[str, List[str]] e.g. [\"example 1\", \"example 2\"] required Returns: Type Description List[List[Dict[str, Union[str, Dict]]]] predictions: [list] of probability predictions for different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, proba_dist where proba_dist = [dict] that maps self.annotation.classes to probabilities","title":"NerModelPredict"},{"location":"python_api/ner_model_predict/#nermodelpredict","text":"class that predicts tags for given text","title":"NerModelPredict"},{"location":"python_api/ner_model_predict/#nerblackbox.modules.ner_training.ner_model_predict.NerModelPredict.__init__","text":"Parameters: Name Type Description Default checkpoint_directory str path required batch_size int used in dataloader 16","title":"__init__()"},{"location":"python_api/ner_model_predict/#nerblackbox.modules.ner_training.ner_model_predict.NerModelPredict.predict","text":"predict tags for input texts. output on entity or word level. Examples: predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=False) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"I-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"B-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=False) # [[ # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"ORG\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] Parameters: Name Type Description Default input_texts Union[str, List[str]] e.g. [\"example 1\", \"example 2\"] required level str \"entity\" or \"word\" 'entity' autocorrect bool if True, autocorrect annotation scheme (e.g. B- and I- tags). False Returns: Type Description List[List[Dict[str, Union[str, Dict]]]] predictions: [list] of predictions for the different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, tag","title":"predict()"},{"location":"python_api/ner_model_predict/#nerblackbox.modules.ner_training.ner_model_predict.NerModelPredict.predict_proba","text":"predict probability distributions for input texts. output on word level. Examples: predict_proba([\"arbetsf\u00f6rmedlingen finns i stockholm\"]) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"proba_dist: {\"O\": 0.21, \"B-ORG\": 0.56, ..}}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"proba_dist: {\"O\": 0.87, \"B-ORG\": 0.02, ..}}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"proba_dist: {\"O\": 0.95, \"B-ORG\": 0.01, ..}}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"proba_dist: {\"O\": 0.14, \"B-ORG\": 0.22, ..}}, # ]] Parameters: Name Type Description Default input_texts Union[str, List[str]] e.g. [\"example 1\", \"example 2\"] required Returns: Type Description List[List[Dict[str, Union[str, Dict]]]] predictions: [list] of probability predictions for different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, proba_dist where proba_dist = [dict] that maps self.annotation.classes to probabilities","title":"predict_proba()"},{"location":"python_api/nerblackbox/","text":"NerBlackBox class that provides all nerblackbox functionalities. __init__ ( self , base_dir = '.' , data_dir = './data' ) special Parameters: Name Type Description Default base_dir str relative path of base directory with respect to current directory '.' data_dir str relative path of data directory with respect to current directory './data' analyze_data ( self , dataset_name , ** kwargs_optional ) analyze a dataset. Parameters: Name Type Description Default dataset_name str e.g. \"swedish_ner_corpus\". required kwargs_optional Any with optional key-value pairs {\"verbose\": [bool]}. {} download ( self ) download & prepare built-in datasets, prepare experiment configuration. needs to be called exactly once before any other CLI/API commands of the package are executed in case built-in datasets shall be used. get_experiment_results ( self , experiment_name ) get results for a single experiment. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" required Returns: Type Description List[nerblackbox.modules.experiment_results.ExperimentResults] see ExperimentResults get_experiments ( self , ** kwargs_optional ) show list of experiments that have been run. Parameters: Name Type Description Default kwargs_optional Any with optional key-value pairs {\"ids\": [tuple of int], \"as_df\": [bool]} {} Returns: Type Description DataFrame experiments_overview get_model_from_experiment ( self , experiment_name ) gets (best) model from experiment. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" required Returns: Type Description Optional[nerblackbox.modules.ner_training.ner_model_predict.NerModelPredict] ner_model_predict init ( self ) initialize the data_dir directory. needs to be called exactly once before any other CLI/API commands of the package are executed. predict ( self , experiment_name , text_input ) predict labels for text_input using the best model of a single experiment. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" required text_input Union[str, List[str]] e.g. \"this text needs to be tagged\" required run_experiment ( self , experiment_name , from_config = False , model = None , dataset = None , from_preset = 'adaptive' , ** kwargs_optional ) run a single experiment. Note: from_config == True -> experiment config file is used, no other optional arguments will be used from_config == False -> experiment config file is created dynamically, optional arguments will be used model and dataset are mandatory. All other arguments relate to hyperparameters and are optional. If not specified, they are taken using the following hierarchy: 1) optional argument 2) from_preset (adaptive, original, stable), which specifies e.g. the hyperparameters \"max_epochs\", \"early_stopping\", \"lr_schedule\" 3) default experiment configuration Parameters: Name Type Description Default experiment_name str e.g. 'exp0' required from_config bool e.g. False False model Optional[str] if experiment config file is to be created dynamically, e.g. 'bert-base-uncased' None dataset Optional[str] if experiment config file is to be created dynamically, e.g. 'conll-2003' None from_preset Optional[str] if experiment config file is to be created dynamically, e.g. 'adaptive' 'adaptive' kwargs_optional Any with optional key-value pairs, e.g. {\"multiple_runs\": [int], \"run_name\": [str], \"device\": [torch device], \"fp16\": [bool]} {} set_up_dataset ( self , dataset_name , dataset_subset_name = '' , ** kwargs_optional ) set up a dataset using the associated Formatter class. Parameters: Name Type Description Default dataset_name str e.g. \"swedish_ner_corpus\" required dataset_subset_name str e.g. \"simple_cased\" '' kwargs_optional Any with optional key-value pairs {\"modify\": [bool], \"val_fraction\": [float], \"verbose\": [bool]} {} show_experiment_config ( self , experiment_name ) show a single experiment configuration in detail or an overview on all available experiment configurations. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" or \"all\" required","title":"NerBlackBox"},{"location":"python_api/nerblackbox/#nerblackbox","text":"class that provides all nerblackbox functionalities.","title":"NerBlackBox"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.__init__","text":"Parameters: Name Type Description Default base_dir str relative path of base directory with respect to current directory '.' data_dir str relative path of data directory with respect to current directory './data'","title":"__init__()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.analyze_data","text":"analyze a dataset. Parameters: Name Type Description Default dataset_name str e.g. \"swedish_ner_corpus\". required kwargs_optional Any with optional key-value pairs {\"verbose\": [bool]}. {}","title":"analyze_data()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.download","text":"download & prepare built-in datasets, prepare experiment configuration. needs to be called exactly once before any other CLI/API commands of the package are executed in case built-in datasets shall be used.","title":"download()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.get_experiment_results","text":"get results for a single experiment. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" required Returns: Type Description List[nerblackbox.modules.experiment_results.ExperimentResults] see ExperimentResults","title":"get_experiment_results()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.get_experiments","text":"show list of experiments that have been run. Parameters: Name Type Description Default kwargs_optional Any with optional key-value pairs {\"ids\": [tuple of int], \"as_df\": [bool]} {} Returns: Type Description DataFrame experiments_overview","title":"get_experiments()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.get_model_from_experiment","text":"gets (best) model from experiment. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" required Returns: Type Description Optional[nerblackbox.modules.ner_training.ner_model_predict.NerModelPredict] ner_model_predict","title":"get_model_from_experiment()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.init","text":"initialize the data_dir directory. needs to be called exactly once before any other CLI/API commands of the package are executed.","title":"init()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.predict","text":"predict labels for text_input using the best model of a single experiment. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" required text_input Union[str, List[str]] e.g. \"this text needs to be tagged\" required","title":"predict()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.run_experiment","text":"run a single experiment. Note: from_config == True -> experiment config file is used, no other optional arguments will be used from_config == False -> experiment config file is created dynamically, optional arguments will be used model and dataset are mandatory. All other arguments relate to hyperparameters and are optional. If not specified, they are taken using the following hierarchy: 1) optional argument 2) from_preset (adaptive, original, stable), which specifies e.g. the hyperparameters \"max_epochs\", \"early_stopping\", \"lr_schedule\" 3) default experiment configuration Parameters: Name Type Description Default experiment_name str e.g. 'exp0' required from_config bool e.g. False False model Optional[str] if experiment config file is to be created dynamically, e.g. 'bert-base-uncased' None dataset Optional[str] if experiment config file is to be created dynamically, e.g. 'conll-2003' None from_preset Optional[str] if experiment config file is to be created dynamically, e.g. 'adaptive' 'adaptive' kwargs_optional Any with optional key-value pairs, e.g. {\"multiple_runs\": [int], \"run_name\": [str], \"device\": [torch device], \"fp16\": [bool]} {}","title":"run_experiment()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.set_up_dataset","text":"set up a dataset using the associated Formatter class. Parameters: Name Type Description Default dataset_name str e.g. \"swedish_ner_corpus\" required dataset_subset_name str e.g. \"simple_cased\" '' kwargs_optional Any with optional key-value pairs {\"modify\": [bool], \"val_fraction\": [float], \"verbose\": [bool]} {}","title":"set_up_dataset()"},{"location":"python_api/nerblackbox/#nerblackbox.api.NerBlackBox.show_experiment_config","text":"show a single experiment configuration in detail or an overview on all available experiment configurations. Parameters: Name Type Description Default experiment_name str e.g. \"exp0\" or \"all\" required","title":"show_experiment_config()"},{"location":"python_api/overview/","text":"Python API Main Classes: NerBlackBox ExperimentResults NerModelPredict Additional Classes: TextEncoder Usage: Python from nerblackbox import NerBlackBox , ExperimentResults , NerModelPredict , TextEncoder","title":"Overview"},{"location":"python_api/overview/#python-api","text":"Main Classes: NerBlackBox ExperimentResults NerModelPredict Additional Classes: TextEncoder Usage: Python from nerblackbox import NerBlackBox , ExperimentResults , NerModelPredict , TextEncoder","title":"Python API"},{"location":"python_api/text_encoder/","text":"TextEncoder __init__ ( self , encoding , model_special_tokens = None ) special Examples: TextEncoder( encoding={\"\\n\": \"[NEWLINE]\", \"\\t\": \"[TAB]\"}, model_special_tokens=[\"[NEWLINE]\", \"[TAB]\"], ) Parameters: Name Type Description Default encoding Dict[str, str] mapping to special tokens required model_special_tokens Optional[List[str]] special tokens that the model was trained on None decode ( self , text_encoded_list , encode_decode_mappings_list , predictions_encoded_list ) decodes list of text_encoded and predictions_encoded using encode_decode_mappings Examples: text_list, predictions_list = decode( text_encoded_list=[\"an[NEWLINE] example\"], encode_decode_mappings_list=[[(2, \"\\n\", \"[NEWLINE]\")]]), predictions_encoded_list=[[{\"char_start\": \"12\", \"char_end\": \"19\", \"token\": \"example\", \"tag\": \"TAG\"}]] ) # text_list = [\"an\\n example\"] # predictions_list = [[{\"char_start\": \"4\", \"char_end\": \"11\", \"token\": \"example\", \"tag\": \"TAG\"}]] Parameters: Name Type Description Default text_encoded_list List[str] encoded text required encode_decode_mappings_list List[List[Tuple[int, str, str]]] mappings (char_start, original token, encoded token) required predictions_encoded_list List[List[Dict[str, Union[str, Dict]]]] encoded predictions required Returns: Type Description Tuple[List[str], List[List[Dict[str, Union[str, Dict]]]]] text_list: original / decoded text predictions_list: original / decoded predictions encode ( self , text_list ) encodes list of text using self.encoding Examples: text_encoded_list, encode_decode_mappings_list = encode(text_list=[\"an\\n example\"]) # text_encoded_list = [\"an[NEWLINE] example\"] # encode_decode_mappings_list = [[(2, \"\\n\", \"[NEWLINE]\")]] Parameters: Name Type Description Default text_list List[str] original text required Returns: Type Description Tuple[List[str], List[List[Tuple[int, str, str]]]] text_encoded_list: encoded text encode_decode_mappings_list: mappings (char_start, original token, encoded token)","title":"TextEncoder"},{"location":"python_api/text_encoder/#textencoder","text":"","title":"TextEncoder"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.__init__","text":"Examples: TextEncoder( encoding={\"\\n\": \"[NEWLINE]\", \"\\t\": \"[TAB]\"}, model_special_tokens=[\"[NEWLINE]\", \"[TAB]\"], ) Parameters: Name Type Description Default encoding Dict[str, str] mapping to special tokens required model_special_tokens Optional[List[str]] special tokens that the model was trained on None","title":"__init__()"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.decode","text":"decodes list of text_encoded and predictions_encoded using encode_decode_mappings Examples: text_list, predictions_list = decode( text_encoded_list=[\"an[NEWLINE] example\"], encode_decode_mappings_list=[[(2, \"\\n\", \"[NEWLINE]\")]]), predictions_encoded_list=[[{\"char_start\": \"12\", \"char_end\": \"19\", \"token\": \"example\", \"tag\": \"TAG\"}]] ) # text_list = [\"an\\n example\"] # predictions_list = [[{\"char_start\": \"4\", \"char_end\": \"11\", \"token\": \"example\", \"tag\": \"TAG\"}]] Parameters: Name Type Description Default text_encoded_list List[str] encoded text required encode_decode_mappings_list List[List[Tuple[int, str, str]]] mappings (char_start, original token, encoded token) required predictions_encoded_list List[List[Dict[str, Union[str, Dict]]]] encoded predictions required Returns: Type Description Tuple[List[str], List[List[Dict[str, Union[str, Dict]]]]] text_list: original / decoded text predictions_list: original / decoded predictions","title":"decode()"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.encode","text":"encodes list of text using self.encoding Examples: text_encoded_list, encode_decode_mappings_list = encode(text_list=[\"an\\n example\"]) # text_encoded_list = [\"an[NEWLINE] example\"] # encode_decode_mappings_list = [[(2, \"\\n\", \"[NEWLINE]\")]] Parameters: Name Type Description Default text_list List[str] original text required Returns: Type Description Tuple[List[str], List[List[Tuple[int, str, str]]]] text_encoded_list: encoded text encode_decode_mappings_list: mappings (char_start, original token, encoded token)","title":"encode()"},{"location":"usage/datasets_and_models/","text":"Datasets and Models nerblackbox and its Python API & CLI work out of the box (see Getting Started ) for built-in datasets and models. Custom datasets and models can easily be included. Built-in Datasets All community-uploaded datasets of the datasets library The following datasets: Name Language Open Source Annotation Scheme Sample Type #Samples (Train, Val, Test) Directory Name Required Files Source CoNLL 2003 English Yes BIO Sentence (14041, 3250, 3453) conll2003 --- Description ; Data Swedish NER Corpus Swedish Yes IO Sentence (4819, 2066, 2453) swedish_ner_corpus --- Description+Data SIC Swedish Yes BIO Sentence (436, 188, 268) sic --- Description+Data SUC 3.0 Swedish No BIO Sentence (71046, 1546, 1568) suc suc-*.conll Description Swe-NERC Swedish Yes BIO Sentence (6841, 878, 891) swe_nerc --- Description ; Data The datasets are downloaded and set up by: Open Source Python nerbb . set_up_dataset ( \"<dataset_name>\" ) CLI nerbb set_up_dataset <dataset_name> Not Open Source create folder: mkdir ./data/datasets/<dataset_name> move Required Files manually to ./data/datasets/<dataset_name> set up dataset: Python nerbb . set_up_dataset ( < dataset_name > ) CLI nerbb set_up_dataset <dataset_name> Additional dataset details (tags, tag distribution, ..) can be found in ./data/datasets/<dataset_name>/analyze_data Built-in Models All built-in or community-uploaded models of the transformers library that use the WordPiece Tokenizer , e.g. BERT DistilBERT Electra Custom Datasets To include your own custom dataset, do the following: Create a folder ./data/datasets/<custom_dataset> and add three files train.* , val.* , test.* to it. The filename extension is either * = jsonl or * = csv , depending on the data format (standard or pretokenized). If your data consists of standard annotations, it must adhere to the following .jsonl format: {\"text\": \"President Barack Obama went to Harvard\", \"tags\": [{\"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22}, {\"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38}} Each row has to contain a single training sample in the format {\"text\": str, \"tags\": List[Dict]} , where Dict = {\"token\": str, \"tag\": str, \"char_start\": int, \"char_end\": int} This format is commonly used by annotation tools. If your data consists of pretokenized annotations, it must adhere to the following .csv format: PER PER PER O O ORG <tab> President Barack Obama went to Harvard Each row has to contain a single training sample in the format <tags> <tab> <text> , where in <tags> and <text> the tags and tokens are separated by whitespace. This format is suitable for many public datasets. Use dataset_name = <custom_dataset> as parameter when fine-tuning a model . Custom Models To include your own custom model, do the following: Create a new folder ./data/pretrained_models/<custom_model> with the following files: config.json pytorch_model.bin vocab.txt <custom_model> must include the architecture type, e.g. bert Use pretrained_model_name = <custom_model> as parameter when fine-tuning a model .","title":"Datasets and Models"},{"location":"usage/datasets_and_models/#datasets-and-models","text":"nerblackbox and its Python API & CLI work out of the box (see Getting Started ) for built-in datasets and models. Custom datasets and models can easily be included.","title":"Datasets and Models"},{"location":"usage/datasets_and_models/#built-in-datasets","text":"All community-uploaded datasets of the datasets library The following datasets: Name Language Open Source Annotation Scheme Sample Type #Samples (Train, Val, Test) Directory Name Required Files Source CoNLL 2003 English Yes BIO Sentence (14041, 3250, 3453) conll2003 --- Description ; Data Swedish NER Corpus Swedish Yes IO Sentence (4819, 2066, 2453) swedish_ner_corpus --- Description+Data SIC Swedish Yes BIO Sentence (436, 188, 268) sic --- Description+Data SUC 3.0 Swedish No BIO Sentence (71046, 1546, 1568) suc suc-*.conll Description Swe-NERC Swedish Yes BIO Sentence (6841, 878, 891) swe_nerc --- Description ; Data The datasets are downloaded and set up by: Open Source Python nerbb . set_up_dataset ( \"<dataset_name>\" ) CLI nerbb set_up_dataset <dataset_name> Not Open Source create folder: mkdir ./data/datasets/<dataset_name> move Required Files manually to ./data/datasets/<dataset_name> set up dataset: Python nerbb . set_up_dataset ( < dataset_name > ) CLI nerbb set_up_dataset <dataset_name> Additional dataset details (tags, tag distribution, ..) can be found in ./data/datasets/<dataset_name>/analyze_data","title":"Built-in Datasets"},{"location":"usage/datasets_and_models/#built-in-models","text":"All built-in or community-uploaded models of the transformers library that use the WordPiece Tokenizer , e.g. BERT DistilBERT Electra","title":"Built-in Models"},{"location":"usage/datasets_and_models/#custom-datasets","text":"To include your own custom dataset, do the following: Create a folder ./data/datasets/<custom_dataset> and add three files train.* , val.* , test.* to it. The filename extension is either * = jsonl or * = csv , depending on the data format (standard or pretokenized). If your data consists of standard annotations, it must adhere to the following .jsonl format: {\"text\": \"President Barack Obama went to Harvard\", \"tags\": [{\"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22}, {\"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38}} Each row has to contain a single training sample in the format {\"text\": str, \"tags\": List[Dict]} , where Dict = {\"token\": str, \"tag\": str, \"char_start\": int, \"char_end\": int} This format is commonly used by annotation tools. If your data consists of pretokenized annotations, it must adhere to the following .csv format: PER PER PER O O ORG <tab> President Barack Obama went to Harvard Each row has to contain a single training sample in the format <tags> <tab> <text> , where in <tags> and <text> the tags and tokens are separated by whitespace. This format is suitable for many public datasets. Use dataset_name = <custom_dataset> as parameter when fine-tuning a model .","title":"Custom Datasets"},{"location":"usage/datasets_and_models/#custom-models","text":"To include your own custom model, do the following: Create a new folder ./data/pretrained_models/<custom_model> with the following files: config.json pytorch_model.bin vocab.txt <custom_model> must include the architecture type, e.g. bert Use pretrained_model_name = <custom_model> as parameter when fine-tuning a model .","title":"Custom Models"},{"location":"usage/getting_started/","text":"Getting Started Use either the Python API or the CLI (Command Line Interface) . basic usage Python from nerblackbox import NerBlackBox nerbb = NerBlackBox () CLI nerbb --help 1. Initialization The following commands need to be executed once: initialization Python nerbb . init () CLI nerbb init This creates a ./data directory with the following structure: data/ \u2514\u2500\u2500 datasets \u2514\u2500\u2500 experiment_configs \u2514\u2500\u2500 pretrained_models \u2514\u2500\u2500 results 2. Data Built-in datasets (including HuggingFace Datasets ) can be downloaded and set up using the following command: set up dataset Python nerbb . set_up_dataset ( \"<dataset_name>\" ) CLI nerbb set_up_dataset <dataset_name> This creates data files in the folder ./data/datasets/<dataset_name> . Custom datasets can be provided using two different data formats: jsonl (raw data) csv (pretokenized data) See Custom datasets for more details. 3. Fine-tune a Model Fine-tuning a specific model on a specific dataset using specific parameters is called an experiment . An experiment is defined either statically by an experiment configuration file ./data/experiment_configs/<experiment_name>.ini . run experiment statically Python nerbb . run_experiment ( \"<experiment_name>\" , from_config = True ) CLI nerbb run_experiment <experiment_name> or dynamically using function arguments in nerbb.run_experiment() run experiment dynamically (only Python API) Python nerbb . run_experiment ( \"<experiment_name>\" , model = \"<model_name>\" , dataset = \"<dataset_name>\" ) This creates an experiment configuration on the fly, which is subsequently used. In both cases, the specification of the model and the dataset are mandatory, while the parameters are all optional. The hyperparameters that are used by default are globally applicable settings that should give close-to-optimal results for any use case. In particular, adaptive fine-tuning is employed to ensure that this holds irrespective of the size of the dataset. 4. Inspect the Results Once an experiment is finished, one can inspect its main results or have a look at detailed results (e.g. learning curves): get main results Python experiment_results = nerbb . get_experiment_results ( \"<experiment_name>\" ) # List[ExperimentResults] CLI nerbb get_experiment_results <experiment_name> # prints overview on runs Python: see ExperimentResults for details on how to use experiment_results get detailed results (only CLI) CLI nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser See Detailed Analysis of Training Results for more information. An overview of all experiments and their results can be accessed as follows: get overview of all experiments Python nerbb . get_experiments () CLI nerbb get_experiments get overview of all experiments' main results Python experiment_results_all = nerbb . get_experiment_results ( \"all\" ) # List[ExperimentResults] CLI nerbb get_experiment_results all Python: see ExperimentResults for details on how to use experiment_results_all 5. Model Inference model inference Python # e.g. <text_input> = \"annotera den h\u00e4r texten\" nerbb . predict ( \"<experiment_name>\" , < text_input > ) # same but w/o having to reload the best model for multiple predictions ner_model_predict = nerbb . get_model_from_experiment ( < experiment_name > ) ner_model_predict . predict ( < text_input > ) CLI # e.g. <text_input> = \"annotera den h\u00e4r texten\" nerbb predict <experiment_name> <text_input> Python: see NerModelPredict for further details on how to use ner_model_predict Next Steps See Datasets and Models to learn how to include your own custom datasets and custom models . See Parameters and Presets for information on how to create your own custom experiments .","title":"Getting Started"},{"location":"usage/getting_started/#getting-started","text":"Use either the Python API or the CLI (Command Line Interface) . basic usage Python from nerblackbox import NerBlackBox nerbb = NerBlackBox () CLI nerbb --help","title":"Getting Started"},{"location":"usage/getting_started/#1-initialization","text":"The following commands need to be executed once: initialization Python nerbb . init () CLI nerbb init This creates a ./data directory with the following structure: data/ \u2514\u2500\u2500 datasets \u2514\u2500\u2500 experiment_configs \u2514\u2500\u2500 pretrained_models \u2514\u2500\u2500 results","title":"1. Initialization"},{"location":"usage/getting_started/#2-data","text":"Built-in datasets (including HuggingFace Datasets ) can be downloaded and set up using the following command: set up dataset Python nerbb . set_up_dataset ( \"<dataset_name>\" ) CLI nerbb set_up_dataset <dataset_name> This creates data files in the folder ./data/datasets/<dataset_name> . Custom datasets can be provided using two different data formats: jsonl (raw data) csv (pretokenized data) See Custom datasets for more details.","title":"2. Data"},{"location":"usage/getting_started/#3-fine-tune-a-model","text":"Fine-tuning a specific model on a specific dataset using specific parameters is called an experiment . An experiment is defined either statically by an experiment configuration file ./data/experiment_configs/<experiment_name>.ini . run experiment statically Python nerbb . run_experiment ( \"<experiment_name>\" , from_config = True ) CLI nerbb run_experiment <experiment_name> or dynamically using function arguments in nerbb.run_experiment() run experiment dynamically (only Python API) Python nerbb . run_experiment ( \"<experiment_name>\" , model = \"<model_name>\" , dataset = \"<dataset_name>\" ) This creates an experiment configuration on the fly, which is subsequently used. In both cases, the specification of the model and the dataset are mandatory, while the parameters are all optional. The hyperparameters that are used by default are globally applicable settings that should give close-to-optimal results for any use case. In particular, adaptive fine-tuning is employed to ensure that this holds irrespective of the size of the dataset.","title":"3. Fine-tune a Model"},{"location":"usage/getting_started/#4-inspect-the-results","text":"Once an experiment is finished, one can inspect its main results or have a look at detailed results (e.g. learning curves): get main results Python experiment_results = nerbb . get_experiment_results ( \"<experiment_name>\" ) # List[ExperimentResults] CLI nerbb get_experiment_results <experiment_name> # prints overview on runs Python: see ExperimentResults for details on how to use experiment_results get detailed results (only CLI) CLI nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser See Detailed Analysis of Training Results for more information. An overview of all experiments and their results can be accessed as follows: get overview of all experiments Python nerbb . get_experiments () CLI nerbb get_experiments get overview of all experiments' main results Python experiment_results_all = nerbb . get_experiment_results ( \"all\" ) # List[ExperimentResults] CLI nerbb get_experiment_results all Python: see ExperimentResults for details on how to use experiment_results_all","title":"4. Inspect the Results"},{"location":"usage/getting_started/#5-model-inference","text":"model inference Python # e.g. <text_input> = \"annotera den h\u00e4r texten\" nerbb . predict ( \"<experiment_name>\" , < text_input > ) # same but w/o having to reload the best model for multiple predictions ner_model_predict = nerbb . get_model_from_experiment ( < experiment_name > ) ner_model_predict . predict ( < text_input > ) CLI # e.g. <text_input> = \"annotera den h\u00e4r texten\" nerbb predict <experiment_name> <text_input> Python: see NerModelPredict for further details on how to use ner_model_predict","title":"5. Model Inference"},{"location":"usage/getting_started/#next-steps","text":"See Datasets and Models to learn how to include your own custom datasets and custom models . See Parameters and Presets for information on how to create your own custom experiments .","title":"Next Steps"},{"location":"usage/parameters_and_presets/","text":"Parameters and Presets An experiment is defined by a set of parameters. These can be specified in a static experiment configuration file ./data/experiment_configs/<experiment_name>.ini . Example: custom_experiment.ini [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42 [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine Alternatively, the parameters can be used to define an experiment dynamically . In that case, there are several hyperparameter presets available for use with nerbb.run_experiment() . In the following, we will go through the different parameters step by step to see what they mean. Parameters An experiment configuration contains the following parameter groups : Dataset Model Settings Hyperparameters Some parameters are mandatory (i.e. they have to be included in an experiment configuration), others are optional and are set to default values if not specified. 1. Dataset Key Mandatory Default Value Type Values Comment dataset_name Yes --- str e.g. conll2003 Built-in Dataset or Custom Dataset annotation_scheme No auto str auto, plain, bio specify dataset tag format (e.g. BIO ). auto means it is inferred from data prune_ratio_train No 1.0 float 0.0 - 1.0 fraction of train dataset to be used prune_ratio_val No 1.0 float 0.0 - 1.0 fraction of val dataset to be used prune_ratio_test No 1.0 float 0.0 - 1.0 fraction of test dataset to be used train_on_val No False bool True, False whether to train additionally on validation dataset train_on_test No False bool True, False whether to train additionally on test dataset Example: custom_experiment.ini (Dataset) [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False 2. Model Key Mandatory Default Value Type Values Comment pretrained_model_name Yes --- str e.g. af-ai-center/bert-base-swedish-uncased Built-in Model or Custom Model Example: custom_experiment.ini (Model) [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased 3. Settings Key Mandatory Default Value Type Values Comment checkpoints No True bool True, False whether to save model checkpoints logging_level No info str info, debug choose logging level , debug is more verbose multiple_runs No 1 int 1+ choose how often each hyperparameter run is executed (to control for statistical uncertainties) seed No 42 int 1+ for reproducibility. multiple runs get assigned different seeds. Example: custom_experiment.ini (Settings) [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42 4. Hyperparameters Key Mandatory Default Value Type Values Comment batch_size No 16 int e.g. 16, 32, 64 number of training samples in one batch max_seq_length No 128 int e.g. 64, 128, 256 maximum sequence length used for model's input data max_epochs No 250 int 1+ (maximum) amount of training epochs early_stopping No True bool True, False whether to use early stopping monitor No val_loss str val_loss, val_acc if early stopping is True: metric to monitor (acc = accuracy) min_delta No 0.0 float 0.0+ if early stopping is True: minimum amount of improvement (w.r.t. monitored metric) required to continue training patience No 0 int 0+ if early stopping is True: number of epochs to wait for improvement w.r.t. monitored metric until training is stopped mode No min str min, max if early stopping is True: whether the optimum for the monitored metric is the minimum (val_loss) or maximum (val_acc) value lr_warmup_epochs No 2 int 0+ number of epochs to linearly increase the learning rate during the warm-up phase, gets translated to num_warmup_steps lr_max No 2e-5 float e.g. 2e-5, 3e-5 maximum learning rate (after warm-up) for AdamW optimizer lr_schedule No constant str constant, linear, cosine, cosine_with_hard_restarts, hybrid Learning Rate Schedule , i.e. how to vary the learning rate (after warm-up). hybrid = constant + linear cool-down. lr_num_cycles No 4 int 1+ num_cycles for lr_schedule = cosine or lr_schedule = cosine_with_hard_restarts lr_cooldown_restarts No True bool True, False if early stopping is True: whether to restart normal training if monitored metric improves during cool-down phase lr_cooldown_epochs No 7 int 0+ if early stopping is True or lr_schedule == hybrid: number of epochs to linearly decrease the learning rate during the cool-down phase, gets translated to num_warmup_steps Example: custom_experiment.ini (Hyperparameters) [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). Each hyperparameter run is executed multiple_runs times (see 3. Settings ). Presets When an experiment is defined dynamically , there are several hyperparameter presets available. They can be specified using the from_preset argument in nerbb.run_experiment() . In the following, we list the different presets together with the Hyperparameters that they entail: from_preset = adaptive adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 from_preset = original original fine-tuning hyperparameters [hparams] max_epochs = 5 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear from_preset = stable stable fine-tuning hyperparameters [hparams] max_epochs = 20 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear More information on the different approaches behind the presets can be found here .","title":"Parameters and Presets"},{"location":"usage/parameters_and_presets/#parameters-and-presets","text":"An experiment is defined by a set of parameters. These can be specified in a static experiment configuration file ./data/experiment_configs/<experiment_name>.ini . Example: custom_experiment.ini [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42 [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine Alternatively, the parameters can be used to define an experiment dynamically . In that case, there are several hyperparameter presets available for use with nerbb.run_experiment() . In the following, we will go through the different parameters step by step to see what they mean.","title":"Parameters and Presets"},{"location":"usage/parameters_and_presets/#parameters","text":"An experiment configuration contains the following parameter groups : Dataset Model Settings Hyperparameters Some parameters are mandatory (i.e. they have to be included in an experiment configuration), others are optional and are set to default values if not specified.","title":"Parameters"},{"location":"usage/parameters_and_presets/#1-dataset","text":"Key Mandatory Default Value Type Values Comment dataset_name Yes --- str e.g. conll2003 Built-in Dataset or Custom Dataset annotation_scheme No auto str auto, plain, bio specify dataset tag format (e.g. BIO ). auto means it is inferred from data prune_ratio_train No 1.0 float 0.0 - 1.0 fraction of train dataset to be used prune_ratio_val No 1.0 float 0.0 - 1.0 fraction of val dataset to be used prune_ratio_test No 1.0 float 0.0 - 1.0 fraction of test dataset to be used train_on_val No False bool True, False whether to train additionally on validation dataset train_on_test No False bool True, False whether to train additionally on test dataset Example: custom_experiment.ini (Dataset) [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False","title":"1. Dataset"},{"location":"usage/parameters_and_presets/#2-model","text":"Key Mandatory Default Value Type Values Comment pretrained_model_name Yes --- str e.g. af-ai-center/bert-base-swedish-uncased Built-in Model or Custom Model Example: custom_experiment.ini (Model) [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased","title":"2. Model"},{"location":"usage/parameters_and_presets/#3-settings","text":"Key Mandatory Default Value Type Values Comment checkpoints No True bool True, False whether to save model checkpoints logging_level No info str info, debug choose logging level , debug is more verbose multiple_runs No 1 int 1+ choose how often each hyperparameter run is executed (to control for statistical uncertainties) seed No 42 int 1+ for reproducibility. multiple runs get assigned different seeds. Example: custom_experiment.ini (Settings) [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42","title":"3. Settings"},{"location":"usage/parameters_and_presets/#4-hyperparameters","text":"Key Mandatory Default Value Type Values Comment batch_size No 16 int e.g. 16, 32, 64 number of training samples in one batch max_seq_length No 128 int e.g. 64, 128, 256 maximum sequence length used for model's input data max_epochs No 250 int 1+ (maximum) amount of training epochs early_stopping No True bool True, False whether to use early stopping monitor No val_loss str val_loss, val_acc if early stopping is True: metric to monitor (acc = accuracy) min_delta No 0.0 float 0.0+ if early stopping is True: minimum amount of improvement (w.r.t. monitored metric) required to continue training patience No 0 int 0+ if early stopping is True: number of epochs to wait for improvement w.r.t. monitored metric until training is stopped mode No min str min, max if early stopping is True: whether the optimum for the monitored metric is the minimum (val_loss) or maximum (val_acc) value lr_warmup_epochs No 2 int 0+ number of epochs to linearly increase the learning rate during the warm-up phase, gets translated to num_warmup_steps lr_max No 2e-5 float e.g. 2e-5, 3e-5 maximum learning rate (after warm-up) for AdamW optimizer lr_schedule No constant str constant, linear, cosine, cosine_with_hard_restarts, hybrid Learning Rate Schedule , i.e. how to vary the learning rate (after warm-up). hybrid = constant + linear cool-down. lr_num_cycles No 4 int 1+ num_cycles for lr_schedule = cosine or lr_schedule = cosine_with_hard_restarts lr_cooldown_restarts No True bool True, False if early stopping is True: whether to restart normal training if monitored metric improves during cool-down phase lr_cooldown_epochs No 7 int 0+ if early stopping is True or lr_schedule == hybrid: number of epochs to linearly decrease the learning rate during the cool-down phase, gets translated to num_warmup_steps Example: custom_experiment.ini (Hyperparameters) [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). Each hyperparameter run is executed multiple_runs times (see 3. Settings ).","title":"4. Hyperparameters"},{"location":"usage/parameters_and_presets/#presets","text":"When an experiment is defined dynamically , there are several hyperparameter presets available. They can be specified using the from_preset argument in nerbb.run_experiment() . In the following, we list the different presets together with the Hyperparameters that they entail: from_preset = adaptive adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 from_preset = original original fine-tuning hyperparameters [hparams] max_epochs = 5 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear from_preset = stable stable fine-tuning hyperparameters [hparams] max_epochs = 20 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear More information on the different approaches behind the presets can be found here .","title":"Presets"}]}