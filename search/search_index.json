{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview nerblackbox - a python package to fine-tune transformer-based language models for named entity recognition (NER). latest version: 0.0.14 Resources source code: https://github.com/flxst/nerblackbox documentation: https://flxst.github.io/nerblackbox PyPI: https://pypi.org/project/nerblackbox Installation pip install nerblackbox About Fine-tune a language model for named entity recognition in a few simple steps: Define a fine-tuning experiment by choosing a pretrained model and a dataset experiment = Experiment ( \"my_experiment\" , model = \"bert-base-cased\" , dataset = \"conll2003\" ) Run the experiment and get the performance of the fine-tuned model experiment . run () experiment . get_result ( metric = \"f1\" , level = \"entity\" , phase = \"test\" ) # 0.9045 Use the fine-tuned model for inference model = Model . from_experiment ( \"my_experiment\" ) model . predict ( \"The United Nations has never recognised Jakarta's move.\" ) # [[ # {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'}, # {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'} # ]] There is much more to it than that! See Usage to get started. Features Data Support for Different Data Formats Support for Different Annotation Schemes Integration of HuggingFace Datasets Text Encoding Training Adaptive Fine-tuning Hyperparameter Search Multiple Runs with Different Random Seeds Detailed Analysis of Training Results Evaluation Evaluation of a Model on a Dataset Inference Versatile Model Inference Other Compatibility with HuggingFace GPU Support Language Agnosticism See Features for more details. Citation @misc { nerblackbox, author = { Stollenwerk, Felix } , title = { nerblackbox: a python package to fine-tune transformer-based language models for named entity recognition } , year = { 2021 } , url = { https://github.com/flxst/nerblackbox } , }","title":"Overview"},{"location":"#overview","text":"nerblackbox - a python package to fine-tune transformer-based language models for named entity recognition (NER). latest version: 0.0.14","title":"Overview"},{"location":"#resources","text":"source code: https://github.com/flxst/nerblackbox documentation: https://flxst.github.io/nerblackbox PyPI: https://pypi.org/project/nerblackbox","title":"Resources"},{"location":"#installation","text":"pip install nerblackbox","title":"Installation"},{"location":"#about","text":"Fine-tune a language model for named entity recognition in a few simple steps: Define a fine-tuning experiment by choosing a pretrained model and a dataset experiment = Experiment ( \"my_experiment\" , model = \"bert-base-cased\" , dataset = \"conll2003\" ) Run the experiment and get the performance of the fine-tuned model experiment . run () experiment . get_result ( metric = \"f1\" , level = \"entity\" , phase = \"test\" ) # 0.9045 Use the fine-tuned model for inference model = Model . from_experiment ( \"my_experiment\" ) model . predict ( \"The United Nations has never recognised Jakarta's move.\" ) # [[ # {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'}, # {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'} # ]] There is much more to it than that! See Usage to get started.","title":"About"},{"location":"#features","text":"Data Support for Different Data Formats Support for Different Annotation Schemes Integration of HuggingFace Datasets Text Encoding Training Adaptive Fine-tuning Hyperparameter Search Multiple Runs with Different Random Seeds Detailed Analysis of Training Results Evaluation Evaluation of a Model on a Dataset Inference Versatile Model Inference Other Compatibility with HuggingFace GPU Support Language Agnosticism See Features for more details.","title":"Features"},{"location":"#citation","text":"@misc { nerblackbox, author = { Stollenwerk, Felix } , title = { nerblackbox: a python package to fine-tune transformer-based language models for named entity recognition } , year = { 2021 } , url = { https://github.com/flxst/nerblackbox } , }","title":"Citation"},{"location":"cli/cli/","text":"CLI (Command Line Interface) nerbb Usage: nerbb [OPTIONS] COMMAND [ARGS]... Options: --store_dir TEXT [str] relative path of store directory --modify / --no-modify [bool] if flag=set_up_dataset --val_fraction FLOAT [float] if flag=set_up_dataset --verbose / --no-verbose [bool] if flag=set_up_dataset --run_name TEXT [str] if flag=run_experiment --device TEXT [str] if flag=run_experiment --fp16 / --no-fp16 [bool] if flag=run_experiment --results / --no-results [bool] if flag=clear_data --help Show this message and exit. nerbb analyze_data analyze a dataset. Usage: nerbb analyze_data [OPTIONS] DATASET_NAME Options: --help Show this message and exit. nerbb clear_data clear data (checkpoints and optionally results). Usage: nerbb clear_data [OPTIONS] Options: --help Show this message and exit. nerbb create initialize the data_dir directory. needs to be called exactly once before any other CLI/API commands of the package are executed. Usage: nerbb create [OPTIONS] Options: --help Show this message and exit. nerbb get_experiment_results get results for a single experiment. Usage: nerbb get_experiment_results [OPTIONS] EXPERIMENT_NAME Options: --help Show this message and exit. nerbb mlflow show detailed experiment results in mlflow (port = 5000). Usage: nerbb mlflow [OPTIONS] Options: --help Show this message and exit. nerbb predict predict labels for text_input using the best model of a single experiment. Usage: nerbb predict [OPTIONS] EXPERIMENT_NAME TEXT_INPUT Options: --help Show this message and exit. nerbb predict_proba predict label probabilities for text_input using the best model of a single experiment. Usage: nerbb predict_proba [OPTIONS] EXPERIMENT_NAME TEXT_INPUT Options: --help Show this message and exit. nerbb run_experiment run a single experiment. Usage: nerbb run_experiment [OPTIONS] EXPERIMENT_NAME Options: --help Show this message and exit. nerbb set_up_dataset set up a dataset using the associated Formatter class. Usage: nerbb set_up_dataset [OPTIONS] DATASET_AND_SUBSET_NAME Options: --help Show this message and exit. nerbb show_experiment_config show a single experiment configuration in detail. Usage: nerbb show_experiment_config [OPTIONS] EXPERIMENT_NAME Options: --help Show this message and exit. nerbb show_experiments get overview on experiments. Usage: nerbb show_experiments [OPTIONS] Options: --help Show this message and exit. nerbb tensorboard show detailed experiment results in tensorboard. (port = 6006). Usage: nerbb tensorboard [OPTIONS] Options: --help Show this message and exit.","title":"Overview"},{"location":"cli/cli/#cli-command-line-interface","text":"","title":"CLI (Command Line Interface)"},{"location":"cli/cli/#nerbb","text":"Usage: nerbb [OPTIONS] COMMAND [ARGS]... Options: --store_dir TEXT [str] relative path of store directory --modify / --no-modify [bool] if flag=set_up_dataset --val_fraction FLOAT [float] if flag=set_up_dataset --verbose / --no-verbose [bool] if flag=set_up_dataset --run_name TEXT [str] if flag=run_experiment --device TEXT [str] if flag=run_experiment --fp16 / --no-fp16 [bool] if flag=run_experiment --results / --no-results [bool] if flag=clear_data --help Show this message and exit.","title":"nerbb"},{"location":"cli/cli/#nerbb-analyze_data","text":"analyze a dataset. Usage: nerbb analyze_data [OPTIONS] DATASET_NAME Options: --help Show this message and exit.","title":"analyze_data"},{"location":"cli/cli/#nerbb-clear_data","text":"clear data (checkpoints and optionally results). Usage: nerbb clear_data [OPTIONS] Options: --help Show this message and exit.","title":"clear_data"},{"location":"cli/cli/#nerbb-create","text":"initialize the data_dir directory. needs to be called exactly once before any other CLI/API commands of the package are executed. Usage: nerbb create [OPTIONS] Options: --help Show this message and exit.","title":"create"},{"location":"cli/cli/#nerbb-get_experiment_results","text":"get results for a single experiment. Usage: nerbb get_experiment_results [OPTIONS] EXPERIMENT_NAME Options: --help Show this message and exit.","title":"get_experiment_results"},{"location":"cli/cli/#nerbb-mlflow","text":"show detailed experiment results in mlflow (port = 5000). Usage: nerbb mlflow [OPTIONS] Options: --help Show this message and exit.","title":"mlflow"},{"location":"cli/cli/#nerbb-predict","text":"predict labels for text_input using the best model of a single experiment. Usage: nerbb predict [OPTIONS] EXPERIMENT_NAME TEXT_INPUT Options: --help Show this message and exit.","title":"predict"},{"location":"cli/cli/#nerbb-predict_proba","text":"predict label probabilities for text_input using the best model of a single experiment. Usage: nerbb predict_proba [OPTIONS] EXPERIMENT_NAME TEXT_INPUT Options: --help Show this message and exit.","title":"predict_proba"},{"location":"cli/cli/#nerbb-run_experiment","text":"run a single experiment. Usage: nerbb run_experiment [OPTIONS] EXPERIMENT_NAME Options: --help Show this message and exit.","title":"run_experiment"},{"location":"cli/cli/#nerbb-set_up_dataset","text":"set up a dataset using the associated Formatter class. Usage: nerbb set_up_dataset [OPTIONS] DATASET_AND_SUBSET_NAME Options: --help Show this message and exit.","title":"set_up_dataset"},{"location":"cli/cli/#nerbb-show_experiment_config","text":"show a single experiment configuration in detail. Usage: nerbb show_experiment_config [OPTIONS] EXPERIMENT_NAME Options: --help Show this message and exit.","title":"show_experiment_config"},{"location":"cli/cli/#nerbb-show_experiments","text":"get overview on experiments. Usage: nerbb show_experiments [OPTIONS] Options: --help Show this message and exit.","title":"show_experiments"},{"location":"cli/cli/#nerbb-tensorboard","text":"show detailed experiment results in tensorboard. (port = 6006). Usage: nerbb tensorboard [OPTIONS] Options: --help Show this message and exit.","title":"tensorboard"},{"location":"features/overview/","text":"Overview In this section, we discuss some useful (and partly unique) features of nerblackbox : Data: Support for Different Data Formats Support for Different Annotation Schemes Integration of HuggingFace Datasets Text Encoding Training: Adaptive Fine-tuning Hyperparameter Search Multiple Runs with Different Random Seeds Detailed Analysis of Training Results Evaluation: Evaluation of a Model on a Dataset Inference: Versatile Model Inference Other: Compatibility with HuggingFace","title":"Overview"},{"location":"features/overview/#overview","text":"In this section, we discuss some useful (and partly unique) features of nerblackbox : Data: Support for Different Data Formats Support for Different Annotation Schemes Integration of HuggingFace Datasets Text Encoding Training: Adaptive Fine-tuning Hyperparameter Search Multiple Runs with Different Random Seeds Detailed Analysis of Training Results Evaluation: Evaluation of a Model on a Dataset Inference: Versatile Model Inference Other: Compatibility with HuggingFace","title":"Overview"},{"location":"features/data/support_annotation_schemes/","text":"[Data] Support for Different Annotation Schemes NER datasets come with different annotation schemes, e.g. IO, BIO and BILOU. nerblackbox automatically recognizes and uses the given scheme of a dataset if the (default) setting auto for the parameter annotation_scheme is used: annotation scheme auto [dataset] dataset_name = conll2003 annotation_scheme = auto The conll2003 dataset will be used in its original BIO scheme. It is also possible to let nerblackbox convert a dataset to another annotation scheme. annotation scheme conversion [dataset] dataset_name = conll2003 annotation_scheme = bilou The conll2003 dataset will be converted from its original BIO scheme to the BILOU scheme. Note however, that conversion is only possible between the BIO and BILOU schemes, as the IO scheme does not possess the same expressiveness.","title":"[Data] Support for Different Annotation Schemes"},{"location":"features/data/support_annotation_schemes/#data-support-for-different-annotation-schemes","text":"NER datasets come with different annotation schemes, e.g. IO, BIO and BILOU. nerblackbox automatically recognizes and uses the given scheme of a dataset if the (default) setting auto for the parameter annotation_scheme is used: annotation scheme auto [dataset] dataset_name = conll2003 annotation_scheme = auto The conll2003 dataset will be used in its original BIO scheme. It is also possible to let nerblackbox convert a dataset to another annotation scheme. annotation scheme conversion [dataset] dataset_name = conll2003 annotation_scheme = bilou The conll2003 dataset will be converted from its original BIO scheme to the BILOU scheme. Note however, that conversion is only possible between the BIO and BILOU schemes, as the IO scheme does not possess the same expressiveness.","title":"[Data] Support for Different Annotation Schemes"},{"location":"features/data/support_huggingface_datasets/","text":"[Data] Integration of HuggingFace Datasets A dataset from HuggingFace Datasets that is suitable for named entity recognition can be used just like any other built-in dataset , without further ado. static experiment definition experiment_huggingface_dataset.ini [dataset] dataset_name = ehealth_kd [model] pretrained_model_name = mrm8488/electricidad-base-discriminator run experiment with huggingface dataset (statically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , from_config = True ) dynamic experiment definition run experiment with huggingface dataset (dynamically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , model = \"mrm8488/electricidad-base-discriminator\" , dataset = \"ehealth_kd\" ) nerblackbox automatically determines whether the employed dataset uses the standard or pretokenized format.","title":"[Data] Integration of HuggingFace Datasets"},{"location":"features/data/support_huggingface_datasets/#data-integration-of-huggingface-datasets","text":"A dataset from HuggingFace Datasets that is suitable for named entity recognition can be used just like any other built-in dataset , without further ado. static experiment definition experiment_huggingface_dataset.ini [dataset] dataset_name = ehealth_kd [model] pretrained_model_name = mrm8488/electricidad-base-discriminator run experiment with huggingface dataset (statically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , from_config = True ) dynamic experiment definition run experiment with huggingface dataset (dynamically) nerbb . run_experiment ( \"experiment_huggingface_dataset\" , model = \"mrm8488/electricidad-base-discriminator\" , dataset = \"ehealth_kd\" ) nerblackbox automatically determines whether the employed dataset uses the standard or pretokenized format.","title":"[Data] Integration of HuggingFace Datasets"},{"location":"features/data/support_pretokenized/","text":"[Data] Support for Different Data Formats Annotated data for named entity recognition in its purest form contains raw text together with a list of entities. Each entity is defined by its position in the raw text and the corresponding tag. A specific standard format which is commonly used in connection with annotation tools looks as follows: Example: standard format (*.jsonl) { \"text\": \"President Barack Obama went to Harvard\", \"tags\": [ { \"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22 }, { \"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38 } ] } Note that the above represents one sample that needs to constitute one line in the jsonl file. The indentations are used for convenience only. After the data is read in, the model's tokenizer is used to pretokenize it. At inference time, the model makes predictions on the pretokenized data. Subsequently, these predictions are mapped back to the original text. Often times, especially in the case of public datasets, the data already comes as pretokenized though. The pretokenized format looks as follows: Example: pretokenized format (*.csv) PER PER PER O O ORG <tab> President Barack Obama went to Harvard In the case of pretokenized data at inference time, the information to map the predictions back to the original text is missing. Hence, the last step in the above chart is skipped. nerblackbox can process both the standard and pretokenized format. See Custom Datasets for details on how to use a dataset in practice.","title":"[Data] Support for Different Data Formats"},{"location":"features/data/support_pretokenized/#data-support-for-different-data-formats","text":"Annotated data for named entity recognition in its purest form contains raw text together with a list of entities. Each entity is defined by its position in the raw text and the corresponding tag. A specific standard format which is commonly used in connection with annotation tools looks as follows: Example: standard format (*.jsonl) { \"text\": \"President Barack Obama went to Harvard\", \"tags\": [ { \"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22 }, { \"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38 } ] } Note that the above represents one sample that needs to constitute one line in the jsonl file. The indentations are used for convenience only. After the data is read in, the model's tokenizer is used to pretokenize it. At inference time, the model makes predictions on the pretokenized data. Subsequently, these predictions are mapped back to the original text. Often times, especially in the case of public datasets, the data already comes as pretokenized though. The pretokenized format looks as follows: Example: pretokenized format (*.csv) PER PER PER O O ORG <tab> President Barack Obama went to Harvard In the case of pretokenized data at inference time, the information to map the predictions back to the original text is missing. Hence, the last step in the above chart is skipped. nerblackbox can process both the standard and pretokenized format. See Custom Datasets for details on how to use a dataset in practice.","title":"[Data] Support for Different Data Formats"},{"location":"features/data/text_encoding/","text":"[Data] Text Encoding Text may contain whitespace characters (e.g. \"\\n\", \"\\t\") or special characters (e.g. \"\u2022\", emojis) that a pre-trained model has never seen before. While the whitespace characters are ignored in the tokenization process, the special characters lead to out-of-vocabulary tokens which get replaced by [UNK] tokens before being sent to the model. Sometimes, however, the ignored or replaced tokens contain semantic information that is valuable for the model and thus should be preserved. Therefore, nerblackbox allows to customly map selected special characters to self-defined special tokens (\"encoding\"). The encoded text may then be used during training and inference. Say we want to have the following replacements: encoding # map special characters to special tokens encoding = { ' \\n ' : '[NEWLINE]' , ' \\t ' : '[TAB]' , '\u2022' : '[DOT]' , } The first step is to save the encoding in an encoding.json file which is located in the same folder ./store/datasets/<custom_dataset> that contains the data (see Custom Datasets ). create encoding.json import json with open ( './store/datasets/<custom_dataset>/encoding.json' , 'w' ) as file : json . dump ( encoding , file ) This way, the special tokens are automatically added to the model's vocabulary during training. The second step is to apply the encoding to the data. The TextEncoder class takes care of this: TextEncoder from nerblackbox import TextEncoder text_encoder = TextEncoder ( encoding ) For training , one needs to encode the input text like so: text encoding (training) # ..load input_text # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' input_text_encoded , _ = text_encoder . encode ( input_text ) # ..save input_text_encoded and use it for training For inference , the predictions also need to be mapped back to the original text, like so: text encoding (inference) # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' # encode_decode_mappings = [(2, \"\\n\", \"[NEWLINE]\"), (13, \"\u2022\", \"[DOT]\")] input_text_encoded , encode_decode_mappings = text_encoder . encode ( input_text ) # PREDICT # e.g. predictions_encoded = {'char_start': 25, 'char_end': 34, 'token': 'Stockholm', 'tag': 'LOC'} predictions_encoded = model . predict ( input_text_encoded , level = \"entity\" ) # DECODE # e.g. input_text_decoded = 'We\\n are in \u2022 Stockholm' # predictions = {'char_start': 13, 'char_end': 22, 'token': 'Stockholm', 'tag': 'LOC'} input_text_decoded , predictions = text_encoder . decode ( input_text_encoded , encode_decode_mappings , predictions_encoded )","title":"[Data] Text Encoding"},{"location":"features/data/text_encoding/#data-text-encoding","text":"Text may contain whitespace characters (e.g. \"\\n\", \"\\t\") or special characters (e.g. \"\u2022\", emojis) that a pre-trained model has never seen before. While the whitespace characters are ignored in the tokenization process, the special characters lead to out-of-vocabulary tokens which get replaced by [UNK] tokens before being sent to the model. Sometimes, however, the ignored or replaced tokens contain semantic information that is valuable for the model and thus should be preserved. Therefore, nerblackbox allows to customly map selected special characters to self-defined special tokens (\"encoding\"). The encoded text may then be used during training and inference. Say we want to have the following replacements: encoding # map special characters to special tokens encoding = { ' \\n ' : '[NEWLINE]' , ' \\t ' : '[TAB]' , '\u2022' : '[DOT]' , } The first step is to save the encoding in an encoding.json file which is located in the same folder ./store/datasets/<custom_dataset> that contains the data (see Custom Datasets ). create encoding.json import json with open ( './store/datasets/<custom_dataset>/encoding.json' , 'w' ) as file : json . dump ( encoding , file ) This way, the special tokens are automatically added to the model's vocabulary during training. The second step is to apply the encoding to the data. The TextEncoder class takes care of this: TextEncoder from nerblackbox import TextEncoder text_encoder = TextEncoder ( encoding ) For training , one needs to encode the input text like so: text encoding (training) # ..load input_text # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' input_text_encoded , _ = text_encoder . encode ( input_text ) # ..save input_text_encoded and use it for training For inference , the predictions also need to be mapped back to the original text, like so: text encoding (inference) # ENCODE # e.g. input_text = 'We\\n are in \u2022 Stockholm' # input_text_encoded = 'We[NEWLINE] are in [DOT] Stockholm' # encode_decode_mappings = [(2, \"\\n\", \"[NEWLINE]\"), (13, \"\u2022\", \"[DOT]\")] input_text_encoded , encode_decode_mappings = text_encoder . encode ( input_text ) # PREDICT # e.g. predictions_encoded = {'char_start': 25, 'char_end': 34, 'token': 'Stockholm', 'tag': 'LOC'} predictions_encoded = model . predict ( input_text_encoded , level = \"entity\" ) # DECODE # e.g. input_text_decoded = 'We\\n are in \u2022 Stockholm' # predictions = {'char_start': 13, 'char_end': 22, 'token': 'Stockholm', 'tag': 'LOC'} input_text_decoded , predictions = text_encoder . decode ( input_text_encoded , encode_decode_mappings , predictions_encoded )","title":"[Data] Text Encoding"},{"location":"features/evaluation/support_evaluation/","text":"[Evaluation] Evaluation of a Model on a Dataset The Model class provides the functionalities to evaluate a model on a dataset. Both the model and the dataset can either be loaded from local files or directly from the HuggingFace Hub . 1) load the Model instance: from local checkpoint directory model = Model . from_checkpoint ( \"<checkpoint_directory>\" ) from local experiment model = Model . from_experiment ( \"<experiment_name>\" ) from huggingface model = Model . from_huggingface ( \"<repo_id>\" ) 2) use the evaluate_on_dataset() method: on local dataset in standard format evaluation_dict = model . evaluate_on_dataset ( \"<local_dataset_in_standard_format>\" , \"jsonl\" , phase = \"test\" ) on local dataset in pretokenized format evaluation_dict = model . evaluate_on_dataset ( \"<local_dataset_in_pretokenized_format>\" , \"csv\" , phase = \"test\" ) on huggingface dataset in pretokenized format evaluation_dict = model . evaluate_on_dataset ( \"<huggingface_dataset_in_pretokenized_format>\" , \"huggingface\" , phase = \"test\" ) The returned object evaluation_dict is a nested dictionary evaluation_dict[label][level][metric] where label in ['micro', 'macro'] level in ['entity', 'token'] metric in ['precision', 'recall', 'f1', 'precision_seqeval', 'recall_seqeval', 'f1_seqeval'] evaluation_dict evaluation_dict [ \"micro\" ][ \"entity\" ] # { # 'precision': 0.912, # 'recall': 0.919, # 'f1': 0.916, # 'precision_seqeval': 0.907, # 'recall_seqeval': 0.919, # 'f1_seqeval': 0.913} # } The metrics precision , recall and f1 are nerblackbox 's evaluation results, whereas their counterparts with a _seqeval suffix correspond to the results you would get using the seqeval library (which is also used by and HuggingFace evaluate ). The difference lies in the way model predictions which are inconsistent with the employed annotation scheme are handled. While nerblackbox 's evaluation ignores them, seqeval takes them into account. A complete example of an evaluation using both the model and the dataset from the HuggingFace Hub: complete evaluation example # 1. load the model model = Model . from_huggingface ( \"dslim/bert-base-NER\" ) # 2. evaluate the model on the dataset evaluation_dict = model . evaluate_on_dataset ( \"conll2003\" , \"huggingface\" , phase = \"test\" ) # 3. inspect the results evaluation_dict [ \"micro\" ][ \"entity\" ] # { # 'precision': 0.912, # 'recall': 0.919, # 'f1': 0.916, # 'precision_seqeval': 0.907, # 'recall_seqeval': 0.919, # 'f1_seqeval': 0.913} # } Note that the seqeval results are in accordance with the official results . The nerblackbox results have a slightly higher precision (and f1 score).","title":"[Evaluation] Evaluation of a Model on a Dataset"},{"location":"features/evaluation/support_evaluation/#evaluation-evaluation-of-a-model-on-a-dataset","text":"The Model class provides the functionalities to evaluate a model on a dataset. Both the model and the dataset can either be loaded from local files or directly from the HuggingFace Hub . 1) load the Model instance: from local checkpoint directory model = Model . from_checkpoint ( \"<checkpoint_directory>\" ) from local experiment model = Model . from_experiment ( \"<experiment_name>\" ) from huggingface model = Model . from_huggingface ( \"<repo_id>\" ) 2) use the evaluate_on_dataset() method: on local dataset in standard format evaluation_dict = model . evaluate_on_dataset ( \"<local_dataset_in_standard_format>\" , \"jsonl\" , phase = \"test\" ) on local dataset in pretokenized format evaluation_dict = model . evaluate_on_dataset ( \"<local_dataset_in_pretokenized_format>\" , \"csv\" , phase = \"test\" ) on huggingface dataset in pretokenized format evaluation_dict = model . evaluate_on_dataset ( \"<huggingface_dataset_in_pretokenized_format>\" , \"huggingface\" , phase = \"test\" ) The returned object evaluation_dict is a nested dictionary evaluation_dict[label][level][metric] where label in ['micro', 'macro'] level in ['entity', 'token'] metric in ['precision', 'recall', 'f1', 'precision_seqeval', 'recall_seqeval', 'f1_seqeval'] evaluation_dict evaluation_dict [ \"micro\" ][ \"entity\" ] # { # 'precision': 0.912, # 'recall': 0.919, # 'f1': 0.916, # 'precision_seqeval': 0.907, # 'recall_seqeval': 0.919, # 'f1_seqeval': 0.913} # } The metrics precision , recall and f1 are nerblackbox 's evaluation results, whereas their counterparts with a _seqeval suffix correspond to the results you would get using the seqeval library (which is also used by and HuggingFace evaluate ). The difference lies in the way model predictions which are inconsistent with the employed annotation scheme are handled. While nerblackbox 's evaluation ignores them, seqeval takes them into account. A complete example of an evaluation using both the model and the dataset from the HuggingFace Hub: complete evaluation example # 1. load the model model = Model . from_huggingface ( \"dslim/bert-base-NER\" ) # 2. evaluate the model on the dataset evaluation_dict = model . evaluate_on_dataset ( \"conll2003\" , \"huggingface\" , phase = \"test\" ) # 3. inspect the results evaluation_dict [ \"micro\" ][ \"entity\" ] # { # 'precision': 0.912, # 'recall': 0.919, # 'f1': 0.916, # 'precision_seqeval': 0.907, # 'recall_seqeval': 0.919, # 'f1_seqeval': 0.913} # } Note that the seqeval results are in accordance with the official results . The nerblackbox results have a slightly higher precision (and f1 score).","title":"[Evaluation] Evaluation of a Model on a Dataset"},{"location":"features/inference/versatile_model_inference/","text":"[Inference] Versatile Model Inference The Model class provides three methods for versative model inference: method input level probabilities predict_on_file() jsonl file with documents entity no predict() one or multiple documents entity, word no predict_proba() one or multiple documents word yes Note that: predict_on_file() takes a jsonl file as input and writes another jsonl file with predictions on the entity level. This is for instance useful if one wants to annotate (large) amounts of text in production. predict() takes a single string or a list of strings as input. It allows to inspect the model predictions on the entity or word level. This is useful for instance for development and debugging. predict_proba() is similar to predict(), but returns predictions on the word level only, together with their probabilities. This can be useful for instance in conjunction with active learning.","title":"[Inference] Versatile Model Inference"},{"location":"features/inference/versatile_model_inference/#inference-versatile-model-inference","text":"The Model class provides three methods for versative model inference: method input level probabilities predict_on_file() jsonl file with documents entity no predict() one or multiple documents entity, word no predict_proba() one or multiple documents word yes Note that: predict_on_file() takes a jsonl file as input and writes another jsonl file with predictions on the entity level. This is for instance useful if one wants to annotate (large) amounts of text in production. predict() takes a single string or a list of strings as input. It allows to inspect the model predictions on the entity or word level. This is useful for instance for development and debugging. predict_proba() is similar to predict(), but returns predictions on the word level only, together with their probabilities. This can be useful for instance in conjunction with active learning.","title":"[Inference] Versatile Model Inference"},{"location":"features/other/compatibility_with_huggingface/","text":"[Other] Compatibility with HuggingFace nerblackbox is heavily based on HuggingFace Transformers . Moreover, HuggingFace Datasets and HuggingFace Evaluate are well-integrated, see Integration of HuggingFace Datasets and Evaluation of a Model on a Dataset , respectively. Therefore, compatibility with HuggingFace is given. In particular, nerblackbox 's model checkpoints (and tokenizer files) are identical to the ones from HuggingFace. model checkpoint directory ls <checkpoint_directory> # config.json # pytorch_model.bin # special_tokens_map.json # tokenizer.json # tokenizer_config.json # vocab.txt After a Model instance is created from a checkpoint, it contains a HuggingFace model and tokenizer as attributes: model attributes model = Model ( < checkpoint_directory > ) print ( type ( model . model )) # <class 'transformers.models.bert.modeling_bert.BertForTokenClassification'> print ( type ( model . tokenizer )) # <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'> Hence, model.model and model.tokenizer can be used like any other transformers model and tokenizer, respectively.","title":"[Other] Compatibility with HuggingFace"},{"location":"features/other/compatibility_with_huggingface/#other-compatibility-with-huggingface","text":"nerblackbox is heavily based on HuggingFace Transformers . Moreover, HuggingFace Datasets and HuggingFace Evaluate are well-integrated, see Integration of HuggingFace Datasets and Evaluation of a Model on a Dataset , respectively. Therefore, compatibility with HuggingFace is given. In particular, nerblackbox 's model checkpoints (and tokenizer files) are identical to the ones from HuggingFace. model checkpoint directory ls <checkpoint_directory> # config.json # pytorch_model.bin # special_tokens_map.json # tokenizer.json # tokenizer_config.json # vocab.txt After a Model instance is created from a checkpoint, it contains a HuggingFace model and tokenizer as attributes: model attributes model = Model ( < checkpoint_directory > ) print ( type ( model . model )) # <class 'transformers.models.bert.modeling_bert.BertForTokenClassification'> print ( type ( model . tokenizer )) # <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'> Hence, model.model and model.tokenizer can be used like any other transformers model and tokenizer, respectively.","title":"[Other] Compatibility with HuggingFace"},{"location":"features/training/adaptive_finetuning/","text":"[Training] Adaptive Fine-tuning Adaptive fine-tuning (introduced in this paper ) is a method that automatically trains for a near-optimal number of epochs. It is used by default in nerblackbox , and corresponds to the following hyperparameters : adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 The hyperparameters are also available as a preset .","title":"[Training] Adaptive Fine-tuning"},{"location":"features/training/adaptive_finetuning/#training-adaptive-fine-tuning","text":"Adaptive fine-tuning (introduced in this paper ) is a method that automatically trains for a near-optimal number of epochs. It is used by default in nerblackbox , and corresponds to the following hyperparameters : adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 The hyperparameters are also available as a preset .","title":"[Training] Adaptive Fine-tuning"},{"location":"features/training/detailed_results/","text":"[Training] Detailed Analysis of Training Results The main results of an experiment, especially the micro-averaged f1 score, can be accessed as follows: get main results Python CLI experiment_results = nerbb . get_experiment_results ( \"<experiment_name>\" ) nerbb get_experiment_results <experiment_name> # prints overview on runs Python: see ExperimentResults for details on how to use experiment_results In addition, on may have a look at much more detailed results of an experiment using mlflow or tensorboard . get detailed results Python CLI Store . mlflow ( \"start\" ) # + enter http://localhost:5000 in your browser Store . tensorboard ( \"start\" ) # + enter http://localhost:6006 in your browser nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser Python: The underlying processes can be stopped using Store.mlflow(\"stop\") and Store.tensorboard(\"stop\") . mlflow displays precision, recall and f1 score for every single class, as well the respective micro- and macro-averages over all classes, both on the token and entity level. The following excerpt shows the micro- and macro-averages of the recall on the entity level precision, recall and f1 score for the LOC(ation) class on the token level In addition, one has access to the log file and the confusion matrices (token and entity level) of the model predictions on the test set. A small excerpt is shown below: tensorboard shows the learning curves of important metrics like the loss and the f1 score. A small excerpt is shown below:","title":"[Training] Detailed Analysis of Training Results"},{"location":"features/training/detailed_results/#training-detailed-analysis-of-training-results","text":"The main results of an experiment, especially the micro-averaged f1 score, can be accessed as follows: get main results Python CLI experiment_results = nerbb . get_experiment_results ( \"<experiment_name>\" ) nerbb get_experiment_results <experiment_name> # prints overview on runs Python: see ExperimentResults for details on how to use experiment_results In addition, on may have a look at much more detailed results of an experiment using mlflow or tensorboard . get detailed results Python CLI Store . mlflow ( \"start\" ) # + enter http://localhost:5000 in your browser Store . tensorboard ( \"start\" ) # + enter http://localhost:6006 in your browser nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser Python: The underlying processes can be stopped using Store.mlflow(\"stop\") and Store.tensorboard(\"stop\") . mlflow displays precision, recall and f1 score for every single class, as well the respective micro- and macro-averages over all classes, both on the token and entity level. The following excerpt shows the micro- and macro-averages of the recall on the entity level precision, recall and f1 score for the LOC(ation) class on the token level In addition, one has access to the log file and the confusion matrices (token and entity level) of the model predictions on the test set. A small excerpt is shown below: tensorboard shows the learning curves of important metrics like the loss and the f1 score. A small excerpt is shown below:","title":"[Training] Detailed Analysis of Training Results"},{"location":"features/training/hyperparameter_search/","text":"[Training] Hyperparameter Search A hyperparameter grid search can easily be conducted as part of an experiment. The hyperparameters one wants to vary are to be specified in special sections [runA] , [runB] etc. in the experiment configuration file. Example: custom_experiment.ini (Hyperparameter Search) [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). See Hyperparameters for an overview of all hyperparameters.","title":"[Training] Hyperparameter Search"},{"location":"features/training/hyperparameter_search/#training-hyperparameter-search","text":"A hyperparameter grid search can easily be conducted as part of an experiment. The hyperparameters one wants to vary are to be specified in special sections [runA] , [runB] etc. in the experiment configuration file. Example: custom_experiment.ini (Hyperparameter Search) [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). See Hyperparameters for an overview of all hyperparameters.","title":"[Training] Hyperparameter Search"},{"location":"features/training/multiple_runs/","text":"[Training] Multiple Runs with Different Random Seeds The results of a fine-tuning run depend on the employed random seed, see e.g. this paper for a discussion. One may conduct multiple runs with different seeds that are otherwise identical, in order to get control over the uncertainties (see Detailed Analysis of Training Results ) get an improved model performance Multiple runs can easily be specified in the experiment configuration. Example: custom_experiment.ini (Settings / Multiple Runs) [settings] multiple_runs = 3 seed = 42 This creates 3 runs with seeds 43, 44 and 45.","title":"[Training] Multiple Runs with Different Random Seeds"},{"location":"features/training/multiple_runs/#training-multiple-runs-with-different-random-seeds","text":"The results of a fine-tuning run depend on the employed random seed, see e.g. this paper for a discussion. One may conduct multiple runs with different seeds that are otherwise identical, in order to get control over the uncertainties (see Detailed Analysis of Training Results ) get an improved model performance Multiple runs can easily be specified in the experiment configuration. Example: custom_experiment.ini (Settings / Multiple Runs) [settings] multiple_runs = 3 seed = 42 This creates 3 runs with seeds 43, 44 and 45.","title":"[Training] Multiple Runs with Different Random Seeds"},{"location":"python_api/dataset/","text":"Dataset class to download, set up and inspect a single dataset __init__ ( dataset_name , dataset_subset_name = '' ) Parameters: Name Type Description Default dataset_name str name of dataset, e.g. \"swedish_ner_corpus\" required dataset_subset_name str name of subset if applicable (for HuggingFace Datasets), e.g. \"simple_cased\" '' from_file ( dataset_name , file_path ) classmethod create a Dataset instance by reading a file Parameters: Name Type Description Default dataset_name str name of dataset, e.g. \"strangnas\" required file_path str e.g. \"strangnas/strangnas.jsonl\" required Returns: Type Description Optional [ Dataset ] dataset instance set_up ( modify = True , val_fraction = 0.3 , verbose = False , shuffle = False ) downloads and sets up the dataset. creates the following files: <STORE_DIR>/datasets/<dataset_name>/train.* <STORE_DIR>/datasets/<dataset_name>/val.* <STORE_DIR>/datasets/<dataset_name>/test.* where * = json or * = csv , depending on whether the data is pretokenized or not Parameters: Name Type Description Default modify bool if True: modify tags as specified in method modify_ner_tag_mapping() TODO: better explanation True val_fraction float fraction of the validation dataset if it is split off from the training dataset 0.3 verbose bool verbose output False shuffle bool whether to shuffle train/val/test datasets False split ( val_fraction = 0.1 , test_fraction = 0.2 ) read dataset from self.file_path split dataset into train/val/test subsets write subsets to PHASE.jsonl Parameters: Name Type Description Default val_fraction float e.g. 0.1 0.1 test_fraction float e.g. 0.2 0.2","title":"Dataset"},{"location":"python_api/dataset/#dataset","text":"class to download, set up and inspect a single dataset","title":"Dataset"},{"location":"python_api/dataset/#nerblackbox.api.dataset.Dataset.__init__","text":"Parameters: Name Type Description Default dataset_name str name of dataset, e.g. \"swedish_ner_corpus\" required dataset_subset_name str name of subset if applicable (for HuggingFace Datasets), e.g. \"simple_cased\" ''","title":"__init__()"},{"location":"python_api/dataset/#nerblackbox.api.dataset.Dataset.from_file","text":"create a Dataset instance by reading a file Parameters: Name Type Description Default dataset_name str name of dataset, e.g. \"strangnas\" required file_path str e.g. \"strangnas/strangnas.jsonl\" required Returns: Type Description Optional [ Dataset ] dataset instance","title":"from_file()"},{"location":"python_api/dataset/#nerblackbox.api.dataset.Dataset.set_up","text":"downloads and sets up the dataset. creates the following files: <STORE_DIR>/datasets/<dataset_name>/train.* <STORE_DIR>/datasets/<dataset_name>/val.* <STORE_DIR>/datasets/<dataset_name>/test.* where * = json or * = csv , depending on whether the data is pretokenized or not Parameters: Name Type Description Default modify bool if True: modify tags as specified in method modify_ner_tag_mapping() TODO: better explanation True val_fraction float fraction of the validation dataset if it is split off from the training dataset 0.3 verbose bool verbose output False shuffle bool whether to shuffle train/val/test datasets False","title":"set_up()"},{"location":"python_api/dataset/#nerblackbox.api.dataset.Dataset.split","text":"read dataset from self.file_path split dataset into train/val/test subsets write subsets to PHASE.jsonl Parameters: Name Type Description Default val_fraction float e.g. 0.1 0.1 test_fraction float e.g. 0.2 0.2","title":"split()"},{"location":"python_api/experiment/","text":"Experiment get_result ( metric = 'f1' , level = 'entity' , label = 'micro' , phase = 'test' , average = False ) Parameters: Name Type Description Default metric str \"f1\", \"precision\", \"recall\" 'f1' level str \"entity\" or \"token\" 'entity' label str \"micro\", \"macro\", \"PER\", .. 'micro' phase str \"val\" or \"test\" 'test' average bool if True, return average result of all runs. if False, return result of best run. False Returns: Name Type Description result Optional [ str ] e.g. \"0.9011 +- 0.0023\" (average = True) or \"0.9045\" (average = False) run () run a single experiment. Note: from_config == True -> experiment config file is used, no other optional arguments will be used from_config == False -> experiment config file is created dynamically, optional arguments will be used model and dataset are mandatory. All other arguments relate to hyperparameters and are optional. They are determined using the following hierarchy: 1) optional argument 2) from_preset (adaptive, original, stable), which specifies e.g. the hyperparameters \"max_epochs\", \"early_stopping\", \"lr_schedule\" 3) default experiment configuration show_config () print experiment config Used Attr experiment_name: [str] e.g. 'exp0' or 'all'","title":"Experiment"},{"location":"python_api/experiment/#experiment","text":"","title":"Experiment"},{"location":"python_api/experiment/#nerblackbox.api.experiment.Experiment.get_result","text":"Parameters: Name Type Description Default metric str \"f1\", \"precision\", \"recall\" 'f1' level str \"entity\" or \"token\" 'entity' label str \"micro\", \"macro\", \"PER\", .. 'micro' phase str \"val\" or \"test\" 'test' average bool if True, return average result of all runs. if False, return result of best run. False Returns: Name Type Description result Optional [ str ] e.g. \"0.9011 +- 0.0023\" (average = True) or \"0.9045\" (average = False)","title":"get_result()"},{"location":"python_api/experiment/#nerblackbox.api.experiment.Experiment.run","text":"run a single experiment. Note: from_config == True -> experiment config file is used, no other optional arguments will be used from_config == False -> experiment config file is created dynamically, optional arguments will be used model and dataset are mandatory. All other arguments relate to hyperparameters and are optional. They are determined using the following hierarchy: 1) optional argument 2) from_preset (adaptive, original, stable), which specifies e.g. the hyperparameters \"max_epochs\", \"early_stopping\", \"lr_schedule\" 3) default experiment configuration","title":"run()"},{"location":"python_api/experiment/#nerblackbox.api.experiment.Experiment.show_config","text":"print experiment config Used Attr experiment_name: [str] e.g. 'exp0' or 'all'","title":"show_config()"},{"location":"python_api/experiment_results/","text":"ExperimentResults class that contains results of a single experiment. __init__ ( _id = None , name = None , experiment = None , single_runs = None , average_runs = None , best_single_run = None , best_average_run = None ) Parameters: Name Type Description Default _id Optional [ str ] e.g. '1' None name Optional [ str ] e.g. 'my_experiment' None experiment Optional [ pd . DataFrame ] overview on experiment parameters None single_runs Optional [ pd . DataFrame ] overview on run parameters & single results None average_runs Optional [ pd . DataFrame ] overview on run parameters & average results None best_single_run Optional [ Dict ] overview on best run parameters & single results None best_average_run Optional [ Dict ] overview on best run parameters & average results None from_mlflow_runs ( _runs , _experiment_id , _experiment_name ) classmethod Parameters: Name Type Description Default _runs List [ Run ] [List of mlflow.entities.Run] required _experiment_id str [str], e.g. '0' required _experiment_name str [str], e.g. 'my_experiment' required Returns: Type Description ExperimentResults ExperimentResults instance","title":"ExperimentResults"},{"location":"python_api/experiment_results/#experimentresults","text":"class that contains results of a single experiment.","title":"ExperimentResults"},{"location":"python_api/experiment_results/#nerblackbox.modules.experiment_results.ExperimentResults.__init__","text":"Parameters: Name Type Description Default _id Optional [ str ] e.g. '1' None name Optional [ str ] e.g. 'my_experiment' None experiment Optional [ pd . DataFrame ] overview on experiment parameters None single_runs Optional [ pd . DataFrame ] overview on run parameters & single results None average_runs Optional [ pd . DataFrame ] overview on run parameters & average results None best_single_run Optional [ Dict ] overview on best run parameters & single results None best_average_run Optional [ Dict ] overview on best run parameters & average results None","title":"__init__()"},{"location":"python_api/experiment_results/#nerblackbox.modules.experiment_results.ExperimentResults.from_mlflow_runs","text":"Parameters: Name Type Description Default _runs List [ Run ] [List of mlflow.entities.Run] required _experiment_id str [str], e.g. '0' required _experiment_name str [str], e.g. 'my_experiment' required Returns: Type Description ExperimentResults ExperimentResults instance","title":"from_mlflow_runs()"},{"location":"python_api/model/","text":"Model model that predicts tags for given input text __init__ ( checkpoint_directory , batch_size = 16 , max_seq_length = None ) Parameters: Name Type Description Default checkpoint_directory str path to the checkpoint directory required batch_size int batch size used by dataloader 16 max_seq_length Optional [ int ] maximum sequence length (Optional). Loaded from checkpoint if not specified. None evaluate_on_dataset ( dataset_name , dataset_format = 'infer' , phase = 'test' , class_mapping = None , number = None , derived_from_jsonl = False , rounded_decimals = 3 ) evaluate model on dataset from huggingface or local dataset in jsonl or csv format Parameters: Name Type Description Default dataset_name str e.g. 'conll2003' required dataset_format str 'huggingface', 'jsonl', 'csv' 'infer' phase str e.g. 'test' 'test' class_mapping Optional [ Dict [ str , str ]] e.g. {\"PER\": \"PI\", \"ORG\": \"PI} None number Optional [ int ] e.g. 100 None derived_from_jsonl bool False rounded_decimals Optional [ int ] if not None, results will be rounded to provided decimals 3 Returns: Name Type Description evaluation_dict EVALUATION_DICT Dict with keys label [metric] where label in ['micro', 'macro'], level in ['entity', 'token'] metric in ['precision', 'recall', 'f1', 'precision_seqeval', 'recall_seqeval', 'f1_seqeval'] and values = float between 0 and 1 from_checkpoint ( checkpoint_directory ) classmethod Parameters: Name Type Description Default checkpoint_directory str path to the checkpoint directory required Returns: Name Type Description model Optional [ Model ] best model from experiment from_experiment ( experiment_name ) classmethod load best model from experiment. Parameters: Name Type Description Default experiment_name str name of the experiment, e.g. \"exp0\" required Returns: Name Type Description model Optional [ Model ] best model from experiment from_huggingface ( repo_id ) classmethod Parameters: Name Type Description Default repo_id str id of the huggingface hub repo id, e.g. 'KB/bert-base-swedish-cased-ner' required Returns: Name Type Description model Optional [ Model ] model predict ( input_texts , level = 'entity' , autocorrect = False , is_pretokenized = False ) predict tags for input texts. output on entity or word level. Examples: predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=False) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"I-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"B-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=False) # [[ # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"ORG\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] Parameters: Name Type Description Default input_texts Union [ str , List [ str ]] e.g. [\"example 1\", \"example 2\"] required level str \"entity\" or \"word\" 'entity' autocorrect bool if True, autocorrect annotation scheme (e.g. B- and I- tags). False is_pretokenized bool True if input_texts are pretokenized False Returns: Name Type Description predictions PREDICTIONS [list] of predictions for the different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, tag predict_on_file ( input_file , output_file ) predict tags for all input texts in input file, write results to output file Parameters: Name Type Description Default input_file str e.g. strangnas/test.jsonl required output_file str e.g. strangnas/test_anonymized.jsonl required predict_proba ( input_texts , is_pretokenized = False ) predict probability distributions for input texts. output on word level. Examples: predict_proba([\"arbetsf\u00f6rmedlingen finns i stockholm\"]) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"proba_dist: {\"O\": 0.21, \"B-ORG\": 0.56, ..}}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"proba_dist: {\"O\": 0.87, \"B-ORG\": 0.02, ..}}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"proba_dist: {\"O\": 0.95, \"B-ORG\": 0.01, ..}}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"proba_dist: {\"O\": 0.14, \"B-ORG\": 0.22, ..}}, # ]] Parameters: Name Type Description Default input_texts Union [ str , List [ str ]] e.g. [\"example 1\", \"example 2\"] required is_pretokenized bool True if input_texts are pretokenized False Returns: Name Type Description predictions PREDICTIONS [list] of probability predictions for different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, proba_dist where proba_dist = [dict] that maps self.annotation.classes to probabilities","title":"Model"},{"location":"python_api/model/#model","text":"model that predicts tags for given input text","title":"Model"},{"location":"python_api/model/#nerblackbox.api.model.Model.__init__","text":"Parameters: Name Type Description Default checkpoint_directory str path to the checkpoint directory required batch_size int batch size used by dataloader 16 max_seq_length Optional [ int ] maximum sequence length (Optional). Loaded from checkpoint if not specified. None","title":"__init__()"},{"location":"python_api/model/#nerblackbox.api.model.Model.evaluate_on_dataset","text":"evaluate model on dataset from huggingface or local dataset in jsonl or csv format Parameters: Name Type Description Default dataset_name str e.g. 'conll2003' required dataset_format str 'huggingface', 'jsonl', 'csv' 'infer' phase str e.g. 'test' 'test' class_mapping Optional [ Dict [ str , str ]] e.g. {\"PER\": \"PI\", \"ORG\": \"PI} None number Optional [ int ] e.g. 100 None derived_from_jsonl bool False rounded_decimals Optional [ int ] if not None, results will be rounded to provided decimals 3 Returns: Name Type Description evaluation_dict EVALUATION_DICT Dict with keys label [metric] where label in ['micro', 'macro'], level in ['entity', 'token'] metric in ['precision', 'recall', 'f1', 'precision_seqeval', 'recall_seqeval', 'f1_seqeval'] and values = float between 0 and 1","title":"evaluate_on_dataset()"},{"location":"python_api/model/#nerblackbox.api.model.Model.from_checkpoint","text":"Parameters: Name Type Description Default checkpoint_directory str path to the checkpoint directory required Returns: Name Type Description model Optional [ Model ] best model from experiment","title":"from_checkpoint()"},{"location":"python_api/model/#nerblackbox.api.model.Model.from_experiment","text":"load best model from experiment. Parameters: Name Type Description Default experiment_name str name of the experiment, e.g. \"exp0\" required Returns: Name Type Description model Optional [ Model ] best model from experiment","title":"from_experiment()"},{"location":"python_api/model/#nerblackbox.api.model.Model.from_huggingface","text":"Parameters: Name Type Description Default repo_id str id of the huggingface hub repo id, e.g. 'KB/bert-base-swedish-cased-ner' required Returns: Name Type Description model Optional [ Model ] model","title":"from_huggingface()"},{"location":"python_api/model/#nerblackbox.api.model.Model.predict","text":"predict tags for input texts. output on entity or word level. Examples: predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=False) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"I-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"B-ORG\"}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=False) # [[ # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=True) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"ORG\"}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"}, # ]] Parameters: Name Type Description Default input_texts Union [ str , List [ str ]] e.g. [\"example 1\", \"example 2\"] required level str \"entity\" or \"word\" 'entity' autocorrect bool if True, autocorrect annotation scheme (e.g. B- and I- tags). False is_pretokenized bool True if input_texts are pretokenized False Returns: Name Type Description predictions PREDICTIONS [list] of predictions for the different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, tag","title":"predict()"},{"location":"python_api/model/#nerblackbox.api.model.Model.predict_on_file","text":"predict tags for all input texts in input file, write results to output file Parameters: Name Type Description Default input_file str e.g. strangnas/test.jsonl required output_file str e.g. strangnas/test_anonymized.jsonl required","title":"predict_on_file()"},{"location":"python_api/model/#nerblackbox.api.model.Model.predict_proba","text":"predict probability distributions for input texts. output on word level. Examples: predict_proba([\"arbetsf\u00f6rmedlingen finns i stockholm\"]) # [[ # {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"proba_dist: {\"O\": 0.21, \"B-ORG\": 0.56, ..}}, # {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"proba_dist: {\"O\": 0.87, \"B-ORG\": 0.02, ..}}, # {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"proba_dist: {\"O\": 0.95, \"B-ORG\": 0.01, ..}}, # {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"proba_dist: {\"O\": 0.14, \"B-ORG\": 0.22, ..}}, # ]] Parameters: Name Type Description Default input_texts Union [ str , List [ str ]] e.g. [\"example 1\", \"example 2\"] required is_pretokenized bool True if input_texts are pretokenized False Returns: Name Type Description predictions PREDICTIONS [list] of probability predictions for different examples. each list contains a [list] of [dict] w/ keys = char_start, char_end, word, proba_dist where proba_dist = [dict] that maps self.annotation.classes to probabilities","title":"predict_proba()"},{"location":"python_api/overview/","text":"Python API Usage: Python from nerblackbox import Store , Dataset , Experiment , Model , [ .. ] Main Classes: Store Dataset Experiment Model Additional Classes: TextEncoder ExperimentResults","title":"Overview"},{"location":"python_api/overview/#python-api","text":"Usage: Python from nerblackbox import Store , Dataset , Experiment , Model , [ .. ] Main Classes: Store Dataset Experiment Model Additional Classes: TextEncoder ExperimentResults","title":"Python API"},{"location":"python_api/store/","text":"Store client for the store that contains all data (datasets, experiment configuration files, models, results) Attributes: Name Type Description path Optional [ str ] path to store's main directory clear_data ( results = False ) classmethod :used attr: clear_all [bool] if True, clear not only checkpoints but also mlflow, tensorboard and logs create () classmethod create store at cls.path get_experiment_results () classmethod get results for all experiments Returns: Name Type Description experiment_results_list List [ ExperimentResults ] TODO: instead of list return dict that maps experiment_name to ExperimentResults? get_experiment_results_single ( experiment_name , update_experiments = True ) classmethod get results for single experiment Parameters: Name Type Description Default experiment_name str e.g. 'exp0' required update_experiments bool whether to update cls.experiment_id2name & cls.experiment_name2id True Returns: Name Type Description experiment_results Tuple [ bool , ExperimentResults ] for experiment with experiment_name get_path () classmethod Returns: Type Description Optional [ str ] cls.path: path to store's main directory mlflow ( action ) classmethod start or stop the mlflow server at http://127.0.0.1:5000 or check its status Parameters: Name Type Description Default action str \"start\", \"status\", \"stop\" required set_path ( path ) classmethod Parameters: Name Type Description Default path str path to store's main directory required Returns: Type Description Optional [ str ] cls.path: path to store's main directory show_experiments ( as_df = True ) classmethod Parameters: Name Type Description Default as_df bool if True, return pandas DataFrame. if False, return dict True Returns: Name Type Description experiments Union [ pd . DataFrame , Dict [ str , str ]] overview of experiments that have been run tensorboard ( action ) classmethod start or stop the tensorboard server at http://127.0.0.1:6006 or check its status Parameters: Name Type Description Default action str \"start\", \"status\", \"stop\" required","title":"Store"},{"location":"python_api/store/#store","text":"client for the store that contains all data (datasets, experiment configuration files, models, results) Attributes: Name Type Description path Optional [ str ] path to store's main directory","title":"Store"},{"location":"python_api/store/#nerblackbox.api.store.Store.clear_data","text":":used attr: clear_all [bool] if True, clear not only checkpoints but also mlflow, tensorboard and logs","title":"clear_data()"},{"location":"python_api/store/#nerblackbox.api.store.Store.create","text":"create store at cls.path","title":"create()"},{"location":"python_api/store/#nerblackbox.api.store.Store.get_experiment_results","text":"get results for all experiments Returns: Name Type Description experiment_results_list List [ ExperimentResults ] TODO: instead of list return dict that maps experiment_name to ExperimentResults?","title":"get_experiment_results()"},{"location":"python_api/store/#nerblackbox.api.store.Store.get_experiment_results_single","text":"get results for single experiment Parameters: Name Type Description Default experiment_name str e.g. 'exp0' required update_experiments bool whether to update cls.experiment_id2name & cls.experiment_name2id True Returns: Name Type Description experiment_results Tuple [ bool , ExperimentResults ] for experiment with experiment_name","title":"get_experiment_results_single()"},{"location":"python_api/store/#nerblackbox.api.store.Store.get_path","text":"Returns: Type Description Optional [ str ] cls.path: path to store's main directory","title":"get_path()"},{"location":"python_api/store/#nerblackbox.api.store.Store.mlflow","text":"start or stop the mlflow server at http://127.0.0.1:5000 or check its status Parameters: Name Type Description Default action str \"start\", \"status\", \"stop\" required","title":"mlflow()"},{"location":"python_api/store/#nerblackbox.api.store.Store.set_path","text":"Parameters: Name Type Description Default path str path to store's main directory required Returns: Type Description Optional [ str ] cls.path: path to store's main directory","title":"set_path()"},{"location":"python_api/store/#nerblackbox.api.store.Store.show_experiments","text":"Parameters: Name Type Description Default as_df bool if True, return pandas DataFrame. if False, return dict True Returns: Name Type Description experiments Union [ pd . DataFrame , Dict [ str , str ]] overview of experiments that have been run","title":"show_experiments()"},{"location":"python_api/store/#nerblackbox.api.store.Store.tensorboard","text":"start or stop the tensorboard server at http://127.0.0.1:6006 or check its status Parameters: Name Type Description Default action str \"start\", \"status\", \"stop\" required","title":"tensorboard()"},{"location":"python_api/text_encoder/","text":"TextEncoder __init__ ( encoding , model_special_tokens = None ) Examples: TextEncoder( encoding={\"\\n\": \"[NEWLINE]\", \"\\t\": \"[TAB]\"}, model_special_tokens=[\"[NEWLINE]\", \"[TAB]\"], ) Parameters: Name Type Description Default encoding Dict [ str , str ] mapping to special tokens required model_special_tokens Optional [ List [ str ]] special tokens that the model was trained on None decode ( text_encoded_list , encode_decode_mappings_list , predictions_encoded_list ) decodes list of text_encoded and predictions_encoded using encode_decode_mappings Examples: text_list, predictions_list = decode( text_encoded_list=[\"an[NEWLINE] example\"], encode_decode_mappings_list=[[(2, \"\\n\", \"[NEWLINE]\")]]), predictions_encoded_list=[[{\"char_start\": \"12\", \"char_end\": \"19\", \"token\": \"example\", \"tag\": \"TAG\"}]] ) # text_list = [\"an\\n example\"] # predictions_list = [[{\"char_start\": \"4\", \"char_end\": \"11\", \"token\": \"example\", \"tag\": \"TAG\"}]] Parameters: Name Type Description Default text_encoded_list List [ str ] encoded text required encode_decode_mappings_list List [ EncodeDecodeMappings ] mappings (char_start, original token, encoded token) required predictions_encoded_list List [ Predictions ] encoded predictions required Returns: Name Type Description text_list List [ str ] original / decoded text predictions_list List [ Predictions ] original / decoded predictions encode ( text_list ) encodes list of text using self.encoding Examples: text_encoded_list, encode_decode_mappings_list = encode(text_list=[\"an\\n example\"]) # text_encoded_list = [\"an[NEWLINE] example\"] # encode_decode_mappings_list = [[(2, \"\\n\", \"[NEWLINE]\")]] Parameters: Name Type Description Default text_list List [ str ] original text required Returns: Name Type Description text_encoded_list List [ str ] encoded text encode_decode_mappings_list List [ EncodeDecodeMappings ] mappings (char_start, original token, encoded token)","title":"TextEncoder"},{"location":"python_api/text_encoder/#textencoder","text":"","title":"TextEncoder"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.__init__","text":"Examples: TextEncoder( encoding={\"\\n\": \"[NEWLINE]\", \"\\t\": \"[TAB]\"}, model_special_tokens=[\"[NEWLINE]\", \"[TAB]\"], ) Parameters: Name Type Description Default encoding Dict [ str , str ] mapping to special tokens required model_special_tokens Optional [ List [ str ]] special tokens that the model was trained on None","title":"__init__()"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.decode","text":"decodes list of text_encoded and predictions_encoded using encode_decode_mappings Examples: text_list, predictions_list = decode( text_encoded_list=[\"an[NEWLINE] example\"], encode_decode_mappings_list=[[(2, \"\\n\", \"[NEWLINE]\")]]), predictions_encoded_list=[[{\"char_start\": \"12\", \"char_end\": \"19\", \"token\": \"example\", \"tag\": \"TAG\"}]] ) # text_list = [\"an\\n example\"] # predictions_list = [[{\"char_start\": \"4\", \"char_end\": \"11\", \"token\": \"example\", \"tag\": \"TAG\"}]] Parameters: Name Type Description Default text_encoded_list List [ str ] encoded text required encode_decode_mappings_list List [ EncodeDecodeMappings ] mappings (char_start, original token, encoded token) required predictions_encoded_list List [ Predictions ] encoded predictions required Returns: Name Type Description text_list List [ str ] original / decoded text predictions_list List [ Predictions ] original / decoded predictions","title":"decode()"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.encode","text":"encodes list of text using self.encoding Examples: text_encoded_list, encode_decode_mappings_list = encode(text_list=[\"an\\n example\"]) # text_encoded_list = [\"an[NEWLINE] example\"] # encode_decode_mappings_list = [[(2, \"\\n\", \"[NEWLINE]\")]] Parameters: Name Type Description Default text_list List [ str ] original text required Returns: Name Type Description text_encoded_list List [ str ] encoded text encode_decode_mappings_list List [ EncodeDecodeMappings ] mappings (char_start, original token, encoded token)","title":"encode()"},{"location":"usage/datasets_and_models/","text":"Datasets and Models nerblackbox and its Python API & CLI work out of the box (see Getting Started ) for built-in datasets and models. Custom datasets and models can easily be included. Built-in Datasets All community-uploaded datasets of the datasets library The following datasets: Name Language Open Source Annotation Scheme Sample Type #Samples (Train, Val, Test) Directory Name Required Files Source CoNLL 2003 English Yes BIO Sentence (14041, 3250, 3453) conll2003 --- Description ; Data Swedish NER Corpus Swedish Yes IO Sentence (4819, 2066, 2453) swedish_ner_corpus --- Description+Data SIC Swedish Yes BIO Sentence (436, 188, 268) sic --- Description+Data SUC 3.0 Swedish No BIO Sentence (71046, 1546, 1568) suc suc-*.conll Description Swe-NERC Swedish Yes BIO Sentence (6841, 878, 891) swe_nerc --- Description ; Data The datasets are downloaded and set up by: Open Source Python CLI nerbb . set_up_dataset ( \"<dataset_name>\" ) nerbb set_up_dataset <dataset_name> Not Open Source create folder: mkdir ./data/datasets/<dataset_name> move Required Files manually to ./data/datasets/<dataset_name> set up dataset: Python CLI nerbb . set_up_dataset ( < dataset_name > ) nerbb set_up_dataset <dataset_name> Additional dataset details (tags, tag distribution, ..) can be found in ./data/datasets/<dataset_name>/analyze_data Built-in Models All built-in or community-uploaded models of the transformers library that use the WordPiece Tokenizer , e.g. BERT DistilBERT Electra Custom Datasets To include your own custom dataset, do the following: Create a folder ./data/datasets/<custom_dataset> and add three files train.* , val.* , test.* to it. The filename extension is either * = jsonl or * = csv , depending on the data format (standard or pretokenized). If your data consists of standard annotations, it must adhere to the following .jsonl format: {\"text\": \"President Barack Obama went to Harvard\", \"tags\": [{\"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22}, {\"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38}} Each row has to contain a single training sample in the format {\"text\": str, \"tags\": List[Dict]} , where Dict = {\"token\": str, \"tag\": str, \"char_start\": int, \"char_end\": int} This format is commonly used by annotation tools. If your data consists of pretokenized annotations, it must adhere to the following .csv format: PER PER PER O O ORG <tab> President Barack Obama went to Harvard Each row has to contain a single training sample in the format <tags> <tab> <text> , where in <tags> and <text> the tags and tokens are separated by whitespace. This format is suitable for many public datasets. Use dataset_name = <custom_dataset> as parameter when fine-tuning a model . Custom Models To include your own custom model, do the following: Create a new folder ./data/pretrained_models/<custom_model> with the following files: config.json pytorch_model.bin vocab.txt <custom_model> must include the architecture type, e.g. bert Use pretrained_model_name = <custom_model> as parameter when fine-tuning a model .","title":"Datasets and Models"},{"location":"usage/datasets_and_models/#datasets-and-models","text":"nerblackbox and its Python API & CLI work out of the box (see Getting Started ) for built-in datasets and models. Custom datasets and models can easily be included.","title":"Datasets and Models"},{"location":"usage/datasets_and_models/#built-in-datasets","text":"All community-uploaded datasets of the datasets library The following datasets: Name Language Open Source Annotation Scheme Sample Type #Samples (Train, Val, Test) Directory Name Required Files Source CoNLL 2003 English Yes BIO Sentence (14041, 3250, 3453) conll2003 --- Description ; Data Swedish NER Corpus Swedish Yes IO Sentence (4819, 2066, 2453) swedish_ner_corpus --- Description+Data SIC Swedish Yes BIO Sentence (436, 188, 268) sic --- Description+Data SUC 3.0 Swedish No BIO Sentence (71046, 1546, 1568) suc suc-*.conll Description Swe-NERC Swedish Yes BIO Sentence (6841, 878, 891) swe_nerc --- Description ; Data The datasets are downloaded and set up by: Open Source Python CLI nerbb . set_up_dataset ( \"<dataset_name>\" ) nerbb set_up_dataset <dataset_name> Not Open Source create folder: mkdir ./data/datasets/<dataset_name> move Required Files manually to ./data/datasets/<dataset_name> set up dataset: Python CLI nerbb . set_up_dataset ( < dataset_name > ) nerbb set_up_dataset <dataset_name> Additional dataset details (tags, tag distribution, ..) can be found in ./data/datasets/<dataset_name>/analyze_data","title":"Built-in Datasets"},{"location":"usage/datasets_and_models/#built-in-models","text":"All built-in or community-uploaded models of the transformers library that use the WordPiece Tokenizer , e.g. BERT DistilBERT Electra","title":"Built-in Models"},{"location":"usage/datasets_and_models/#custom-datasets","text":"To include your own custom dataset, do the following: Create a folder ./data/datasets/<custom_dataset> and add three files train.* , val.* , test.* to it. The filename extension is either * = jsonl or * = csv , depending on the data format (standard or pretokenized). If your data consists of standard annotations, it must adhere to the following .jsonl format: {\"text\": \"President Barack Obama went to Harvard\", \"tags\": [{\"token\": \"President Barack Obama\", \"tag\": \"PER\", \"char_start\": 0, \"char_end\": 22}, {\"token\": \"Harvard\", \"tag\": \"ORG\", \"char_start\": 31, \"char_end\": 38}} Each row has to contain a single training sample in the format {\"text\": str, \"tags\": List[Dict]} , where Dict = {\"token\": str, \"tag\": str, \"char_start\": int, \"char_end\": int} This format is commonly used by annotation tools. If your data consists of pretokenized annotations, it must adhere to the following .csv format: PER PER PER O O ORG <tab> President Barack Obama went to Harvard Each row has to contain a single training sample in the format <tags> <tab> <text> , where in <tags> and <text> the tags and tokens are separated by whitespace. This format is suitable for many public datasets. Use dataset_name = <custom_dataset> as parameter when fine-tuning a model .","title":"Custom Datasets"},{"location":"usage/datasets_and_models/#custom-models","text":"To include your own custom model, do the following: Create a new folder ./data/pretrained_models/<custom_model> with the following files: config.json pytorch_model.bin vocab.txt <custom_model> must include the architecture type, e.g. bert Use pretrained_model_name = <custom_model> as parameter when fine-tuning a model .","title":"Custom Models"},{"location":"usage/getting_started/","text":"Getting Started nerblackbox provides a Python API with four main classes Store , Dataset , Experiment and Model . Alternatively, a CLI (Command Line Interface) with the command nerbb is available. basic usage Python CLI from nerblackbox import Store , Dataset , Experiment , Model nerbb --help 1. Store First, a store has to be created. The store is a directory that contains all the data (datasets, experiment configurations, results, model checkpoints) that nerblackbox needs access to. It is handled by the Store class: create store Python CLI Store . create () nerbb create By default, the store is located at ./store and has the following subdirectories: store/ \u2514\u2500\u2500 datasets \u2514\u2500\u2500 experiment_configs \u2514\u2500\u2500 pretrained_models \u2514\u2500\u2500 results If wanted, the path of the store can be adjusted (before creation) like this: adjust store path Python CLI Store . set_path ( \"<store_path>\" ) nerbb --store_dir <store_dir> create 2. Dataset Next, a dataset needs to be prepared. Built-in datasets (including HuggingFace Datasets ) can easily be downloaded and set up using the Dataset class: set up a built-in dataset Python CLI dataset = Dataset ( \"<dataset_name>\" ) dataset . set_up () nerbb set_up_dataset <dataset_name> This creates dataset files in the folder ./store/datasets/<dataset_name> . Custom datasets can be added manually using two different data formats: jsonl (raw data) csv (pretokenized data) See Custom datasets for more details. 3. Experiment (fine-tune a model) Fine-tuning a specific model on a specific dataset using specific parameters is called an experiment . Everything around an experiment is handled by the Experiment class. 3a. Define an experiment An experiment is defined either dynamically through arguments when an Experiment instance is created define experiment dynamically (only Python API) Python experiment = Experiment ( \"<experiment_name>\" , model = \"<model_name>\" , dataset = \"<dataset_name>\" ) or statically by an experiment configuration file ./store/experiment_configs/<experiment_name>.ini . define experiment statically Python CLI experiment = Experiment ( \"<experiment_name>\" , from_config = True ) see 3b. Note that the dynamic variant also creates an experiment configuration, which is subsequently used. In both cases, the specification of the model and the dataset are mandatory, while the parameters are all optional. The hyperparameters that are used by default are globally applicable settings that should give close-to-optimal results for any use case. In particular, adaptive fine-tuning is employed to ensure that this holds irrespective of the size of the dataset. 3b. Run an experiment A fine-tuning experiment is run using the following command: run experiment Python CLI experiment . run () nerbb run_experiment <experiment_name> # CLI: only static experiment definition See Experiment.run() for further details. 3c. Get the results When an experiment is finished, one can get its main results like so: get main results Python experiment . get_result ( metric = \"f1\" , level = \"entity\" , phase = \"test\" ) See Experiment.get_result() for further details. 4. Model 4a. Inference The best model of an experiment can be loaded and used for inference using the following commands: model inference Python CLI model = Model . from_experiment ( < experiment_name > ) model . predict ( < text_input > ) nerbb predict <experiment_name> \"<text_input>\" See Model for further details on how to use the model 5. Store (advanced) The Store class provides a few additional useful methods. An overview of all experiments and their results can be accessed as follows: get overview of all experiments Python CLI Store . show_experiments () nerbb show_experiments Detailed experiment results (e.g. learning curves) can be accessed using mlflow or tensorboard : get detailed results Python CLI Store . mlflow ( \"start\" ) # + enter http://localhost:5000 in your browser Store . tensorboard ( \"start\" ) # + enter http://localhost:6006 in your browser nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser Python: The underlying processes can be stopped using Store.mlflow(\"stop\") and Store.tensorboard(\"stop\") . See Detailed Analysis of Training Results for more information. Next Steps See Datasets and Models to learn how to include your own custom datasets and custom models . See Parameters and Presets for information on how to create your own custom experiments .","title":"Getting Started"},{"location":"usage/getting_started/#getting-started","text":"nerblackbox provides a Python API with four main classes Store , Dataset , Experiment and Model . Alternatively, a CLI (Command Line Interface) with the command nerbb is available. basic usage Python CLI from nerblackbox import Store , Dataset , Experiment , Model nerbb --help","title":"Getting Started"},{"location":"usage/getting_started/#1-store","text":"First, a store has to be created. The store is a directory that contains all the data (datasets, experiment configurations, results, model checkpoints) that nerblackbox needs access to. It is handled by the Store class: create store Python CLI Store . create () nerbb create By default, the store is located at ./store and has the following subdirectories: store/ \u2514\u2500\u2500 datasets \u2514\u2500\u2500 experiment_configs \u2514\u2500\u2500 pretrained_models \u2514\u2500\u2500 results If wanted, the path of the store can be adjusted (before creation) like this: adjust store path Python CLI Store . set_path ( \"<store_path>\" ) nerbb --store_dir <store_dir> create","title":"1. Store"},{"location":"usage/getting_started/#2-dataset","text":"Next, a dataset needs to be prepared. Built-in datasets (including HuggingFace Datasets ) can easily be downloaded and set up using the Dataset class: set up a built-in dataset Python CLI dataset = Dataset ( \"<dataset_name>\" ) dataset . set_up () nerbb set_up_dataset <dataset_name> This creates dataset files in the folder ./store/datasets/<dataset_name> . Custom datasets can be added manually using two different data formats: jsonl (raw data) csv (pretokenized data) See Custom datasets for more details.","title":"2. Dataset"},{"location":"usage/getting_started/#3-experiment-fine-tune-a-model","text":"Fine-tuning a specific model on a specific dataset using specific parameters is called an experiment . Everything around an experiment is handled by the Experiment class.","title":"3. Experiment (fine-tune a model)"},{"location":"usage/getting_started/#3a-define-an-experiment","text":"An experiment is defined either dynamically through arguments when an Experiment instance is created define experiment dynamically (only Python API) Python experiment = Experiment ( \"<experiment_name>\" , model = \"<model_name>\" , dataset = \"<dataset_name>\" ) or statically by an experiment configuration file ./store/experiment_configs/<experiment_name>.ini . define experiment statically Python CLI experiment = Experiment ( \"<experiment_name>\" , from_config = True ) see 3b. Note that the dynamic variant also creates an experiment configuration, which is subsequently used. In both cases, the specification of the model and the dataset are mandatory, while the parameters are all optional. The hyperparameters that are used by default are globally applicable settings that should give close-to-optimal results for any use case. In particular, adaptive fine-tuning is employed to ensure that this holds irrespective of the size of the dataset.","title":"3a. Define an experiment"},{"location":"usage/getting_started/#3b-run-an-experiment","text":"A fine-tuning experiment is run using the following command: run experiment Python CLI experiment . run () nerbb run_experiment <experiment_name> # CLI: only static experiment definition See Experiment.run() for further details.","title":"3b. Run an experiment"},{"location":"usage/getting_started/#3c-get-the-results","text":"When an experiment is finished, one can get its main results like so: get main results Python experiment . get_result ( metric = \"f1\" , level = \"entity\" , phase = \"test\" ) See Experiment.get_result() for further details.","title":"3c. Get the results"},{"location":"usage/getting_started/#4-model","text":"","title":"4. Model"},{"location":"usage/getting_started/#4a-inference","text":"The best model of an experiment can be loaded and used for inference using the following commands: model inference Python CLI model = Model . from_experiment ( < experiment_name > ) model . predict ( < text_input > ) nerbb predict <experiment_name> \"<text_input>\" See Model for further details on how to use the model","title":"4a. Inference"},{"location":"usage/getting_started/#5-store-advanced","text":"The Store class provides a few additional useful methods. An overview of all experiments and their results can be accessed as follows: get overview of all experiments Python CLI Store . show_experiments () nerbb show_experiments Detailed experiment results (e.g. learning curves) can be accessed using mlflow or tensorboard : get detailed results Python CLI Store . mlflow ( \"start\" ) # + enter http://localhost:5000 in your browser Store . tensorboard ( \"start\" ) # + enter http://localhost:6006 in your browser nerbb mlflow # + enter http://localhost:5000 in your browser nerbb tensorboard # + enter http://localhost:6006 in your browser Python: The underlying processes can be stopped using Store.mlflow(\"stop\") and Store.tensorboard(\"stop\") . See Detailed Analysis of Training Results for more information.","title":"5. Store (advanced)"},{"location":"usage/getting_started/#next-steps","text":"See Datasets and Models to learn how to include your own custom datasets and custom models . See Parameters and Presets for information on how to create your own custom experiments .","title":"Next Steps"},{"location":"usage/parameters_and_presets/","text":"Parameters and Presets An experiment is defined by a set of parameters. These can be specified in a static experiment configuration file ./store/experiment_configs/<experiment_name>.ini . Example: custom_experiment.ini [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42 [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine Alternatively, the parameters can be used to define an experiment dynamically . In that case, there are several hyperparameter presets available for use with Experiment() . In the following, we will go through the different parameters step by step to see what they mean. Parameters An experiment configuration contains the following parameter groups : Dataset Model Settings Hyperparameters Some parameters are mandatory (i.e. they have to be included in an experiment configuration), others are optional and are set to default values if not specified. 1. Dataset Key Mandatory Default Value Type Values Comment dataset_name Yes --- str e.g. conll2003 Built-in Dataset or Custom Dataset annotation_scheme No auto str auto, plain, bio specify dataset tag format (e.g. BIO ). auto means it is inferred from data prune_ratio_train No 1.0 float 0.0 - 1.0 fraction of train dataset to be used prune_ratio_val No 1.0 float 0.0 - 1.0 fraction of val dataset to be used prune_ratio_test No 1.0 float 0.0 - 1.0 fraction of test dataset to be used train_on_val No False bool True, False whether to train additionally on validation dataset train_on_test No False bool True, False whether to train additionally on test dataset Example: custom_experiment.ini (Dataset) [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False 2. Model Key Mandatory Default Value Type Values Comment pretrained_model_name Yes --- str e.g. af-ai-center/bert-base-swedish-uncased Built-in Model or Custom Model Example: custom_experiment.ini (Model) [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased 3. Settings Key Mandatory Default Value Type Values Comment checkpoints No True bool True, False whether to save model checkpoints logging_level No info str info, debug choose logging level , debug is more verbose multiple_runs No 1 int 1+ choose how often each hyperparameter run is executed (to control for statistical uncertainties) seed No 42 int 1+ for reproducibility. multiple runs get assigned different seeds. Example: custom_experiment.ini (Settings) [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42 4. Hyperparameters Key Mandatory Default Value Type Values Comment batch_size No 16 int e.g. 16, 32, 64 number of training samples in one batch max_seq_length No 128 int e.g. 64, 128, 256 maximum sequence length used for model's input data max_epochs No 250 int 1+ (maximum) amount of training epochs early_stopping No True bool True, False whether to use early stopping monitor No val_loss str val_loss, val_acc if early stopping is True: metric to monitor (acc = accuracy) min_delta No 0.0 float 0.0+ if early stopping is True: minimum amount of improvement (w.r.t. monitored metric) required to continue training patience No 0 int 0+ if early stopping is True: number of epochs to wait for improvement w.r.t. monitored metric until training is stopped mode No min str min, max if early stopping is True: whether the optimum for the monitored metric is the minimum (val_loss) or maximum (val_acc) value lr_warmup_epochs No 2 int 0+ number of epochs to linearly increase the learning rate during the warm-up phase, gets translated to num_warmup_steps lr_max No 2e-5 float e.g. 2e-5, 3e-5 maximum learning rate (after warm-up) for AdamW optimizer lr_schedule No constant str constant, linear, cosine, cosine_with_hard_restarts, hybrid Learning Rate Schedule , i.e. how to vary the learning rate (after warm-up). hybrid = constant + linear cool-down. lr_num_cycles No 4 int 1+ num_cycles for lr_schedule = cosine or lr_schedule = cosine_with_hard_restarts lr_cooldown_restarts No True bool True, False if early stopping is True: whether to restart normal training if monitored metric improves during cool-down phase lr_cooldown_epochs No 7 int 0+ if early stopping is True or lr_schedule == hybrid: number of epochs to linearly decrease the learning rate during the cool-down phase, gets translated to num_warmup_steps Example: custom_experiment.ini (Hyperparameters) [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). Each hyperparameter run is executed multiple_runs times (see 3. Settings ). Presets When an experiment is defined dynamically , there are several hyperparameter presets available. They can be specified using the from_preset argument in Experiment() . In the following, we list the different presets together with the Hyperparameters that they entail: from_preset = adaptive adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 from_preset = original original fine-tuning hyperparameters [hparams] max_epochs = 5 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear from_preset = stable stable fine-tuning hyperparameters [hparams] max_epochs = 20 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear More information on the different approaches behind the presets can be found here .","title":"Parameters and Presets"},{"location":"usage/parameters_and_presets/#parameters-and-presets","text":"An experiment is defined by a set of parameters. These can be specified in a static experiment configuration file ./store/experiment_configs/<experiment_name>.ini . Example: custom_experiment.ini [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42 [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine Alternatively, the parameters can be used to define an experiment dynamically . In that case, there are several hyperparameter presets available for use with Experiment() . In the following, we will go through the different parameters step by step to see what they mean.","title":"Parameters and Presets"},{"location":"usage/parameters_and_presets/#parameters","text":"An experiment configuration contains the following parameter groups : Dataset Model Settings Hyperparameters Some parameters are mandatory (i.e. they have to be included in an experiment configuration), others are optional and are set to default values if not specified.","title":"Parameters"},{"location":"usage/parameters_and_presets/#1-dataset","text":"Key Mandatory Default Value Type Values Comment dataset_name Yes --- str e.g. conll2003 Built-in Dataset or Custom Dataset annotation_scheme No auto str auto, plain, bio specify dataset tag format (e.g. BIO ). auto means it is inferred from data prune_ratio_train No 1.0 float 0.0 - 1.0 fraction of train dataset to be used prune_ratio_val No 1.0 float 0.0 - 1.0 fraction of val dataset to be used prune_ratio_test No 1.0 float 0.0 - 1.0 fraction of test dataset to be used train_on_val No False bool True, False whether to train additionally on validation dataset train_on_test No False bool True, False whether to train additionally on test dataset Example: custom_experiment.ini (Dataset) [dataset] dataset_name = swedish_ner_corpus annotation_scheme = plain prune_ratio_train = 0.1 # for testing prune_ratio_val = 1.0 prune_ratio_test = 1.0 train_on_val = False train_on_test = False","title":"1. Dataset"},{"location":"usage/parameters_and_presets/#2-model","text":"Key Mandatory Default Value Type Values Comment pretrained_model_name Yes --- str e.g. af-ai-center/bert-base-swedish-uncased Built-in Model or Custom Model Example: custom_experiment.ini (Model) [model] pretrained_model_name = af-ai-center/bert-base-swedish-uncased","title":"2. Model"},{"location":"usage/parameters_and_presets/#3-settings","text":"Key Mandatory Default Value Type Values Comment checkpoints No True bool True, False whether to save model checkpoints logging_level No info str info, debug choose logging level , debug is more verbose multiple_runs No 1 int 1+ choose how often each hyperparameter run is executed (to control for statistical uncertainties) seed No 42 int 1+ for reproducibility. multiple runs get assigned different seeds. Example: custom_experiment.ini (Settings) [settings] checkpoints = True logging_level = info multiple_runs = 1 seed = 42","title":"3. Settings"},{"location":"usage/parameters_and_presets/#4-hyperparameters","text":"Key Mandatory Default Value Type Values Comment batch_size No 16 int e.g. 16, 32, 64 number of training samples in one batch max_seq_length No 128 int e.g. 64, 128, 256 maximum sequence length used for model's input data max_epochs No 250 int 1+ (maximum) amount of training epochs early_stopping No True bool True, False whether to use early stopping monitor No val_loss str val_loss, val_acc if early stopping is True: metric to monitor (acc = accuracy) min_delta No 0.0 float 0.0+ if early stopping is True: minimum amount of improvement (w.r.t. monitored metric) required to continue training patience No 0 int 0+ if early stopping is True: number of epochs to wait for improvement w.r.t. monitored metric until training is stopped mode No min str min, max if early stopping is True: whether the optimum for the monitored metric is the minimum (val_loss) or maximum (val_acc) value lr_warmup_epochs No 2 int 0+ number of epochs to linearly increase the learning rate during the warm-up phase, gets translated to num_warmup_steps lr_max No 2e-5 float e.g. 2e-5, 3e-5 maximum learning rate (after warm-up) for AdamW optimizer lr_schedule No constant str constant, linear, cosine, cosine_with_hard_restarts, hybrid Learning Rate Schedule , i.e. how to vary the learning rate (after warm-up). hybrid = constant + linear cool-down. lr_num_cycles No 4 int 1+ num_cycles for lr_schedule = cosine or lr_schedule = cosine_with_hard_restarts lr_cooldown_restarts No True bool True, False if early stopping is True: whether to restart normal training if monitored metric improves during cool-down phase lr_cooldown_epochs No 7 int 0+ if early stopping is True or lr_schedule == hybrid: number of epochs to linearly decrease the learning rate during the cool-down phase, gets translated to num_warmup_steps Example: custom_experiment.ini (Hyperparameters) [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_num_cycles = 4 lr_cooldown_restarts = True lr_cooldown_epochs = 7 [runA] batch_size = 16 max_seq_length = 128 lr_max = 2e-5 lr_schedule = constant [runB] batch_size = 32 max_seq_length = 64 lr_max = 3e-5 lr_schedule = cosine This creates 2 hyperparameter runs ( runA & runB ). Each hyperparameter run is executed multiple_runs times (see 3. Settings ).","title":"4. Hyperparameters"},{"location":"usage/parameters_and_presets/#presets","text":"When an experiment is defined dynamically , there are several hyperparameter presets available. They can be specified using the from_preset argument in Experiment() . In the following, we list the different presets together with the Hyperparameters that they entail: from_preset = adaptive adaptive fine-tuning hyperparameters [hparams] max_epochs = 250 early_stopping = True monitor = val_loss min_delta = 0.0 patience = 0 mode = min lr_warmup_epochs = 2 lr_schedule = constant lr_cooldown_epochs = 7 from_preset = original original fine-tuning hyperparameters [hparams] max_epochs = 5 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear from_preset = stable stable fine-tuning hyperparameters [hparams] max_epochs = 20 early_stopping = False lr_warmup_epochs = 2 lr_schedule = linear More information on the different approaches behind the presets can be found here .","title":"Presets"}]}