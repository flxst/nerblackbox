{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Start","text":"<p>nerblackbox - a high-level library for named entity recognition in python</p> <p>latest version: 1.0.0</p>"},{"location":"#resources","title":"Resources","text":"<ul> <li>source code: https://github.com/flxst/nerblackbox</li> <li>documentation: https://flxst.github.io/nerblackbox</li> <li>PyPI: https://pypi.org/project/nerblackbox</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install nerblackbox\n</code></pre>"},{"location":"#about","title":"About","text":"<p>Take a dataset from one of many available sources. Then train, evaluate and apply a language model  in a few simple steps.</p> DataTrainingEvaluationInference <ul> <li> <p>Choose a dataset from HuggingFace (HF), the Local Filesystem (LF), or a Built-in (BI) dataset <pre><code>dataset = Dataset(\"conll2003\",  source=\"HF\")  # HuggingFace\ndataset = Dataset(\"my_dataset\", source=\"LF\")  # Local Filesystem\ndataset = Dataset(\"swe_nerc\",   source=\"BI\")  # Built-in\n</code></pre></p> </li> <li> <p>Set up the dataset <pre><code>dataset.set_up()\n</code></pre> </p> </li> </ul> <p>Datasets from an Annotation Tool (AT) server can also be used. See Data for more details.</p> <ul> <li> <p>Define the training by choosing a pretrained model and a dataset <pre><code>training = Training(\"my_training\", model=\"bert-base-cased\", dataset=\"conll2003\")\n</code></pre></p> </li> <li> <p>Run the training and get the performance of the fine-tuned model <pre><code>training.run()\ntraining.get_result(metric=\"f1\", level=\"entity\", phase=\"test\")\n# 0.9045\n</code></pre> </p> </li> </ul> <p>See Training for more details.</p> <ul> <li> <p>Load the model <pre><code>model = Model.from_training(\"my_training\")\n</code></pre></p> </li> <li> <p>Evaluate the model <pre><code>results = model.evaluate_on_dataset(\"conll2003\", phase=\"test\")\nresults[\"micro\"][\"entity\"][\"f1\"]\n# 0.9045\n</code></pre> </p> </li> </ul> <p>See Evaluation for more details.</p> <ul> <li> <p>Load the model <pre><code>model = Model.from_training(\"my_training\")\n</code></pre></p> </li> <li> <p>Let the model predict <pre><code>model.predict(\"The United Nations has never recognised Jakarta's move.\")  \n# [[\n#  {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'},\n#  {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'}\n# ]]\n</code></pre></p> </li> </ul> <p>See Inference for more details.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>In order to get familiar with nerblackbox, it is recommended to </p> <ol> <li> <p>read the doc sections  Preparation,  Data, Training, Evaluation and Inference</p> </li> <li> <p>go through one of the example notebooks </p> </li> <li> <p>check out the Python API documentation</p> </li> </ol>"},{"location":"#features","title":"Features","text":"<p>Data</p> <ul> <li>Integration of Datasets from Multiple Sources (HuggingFace, Annotation Tools, ..)</li> <li>Support for Multiple Dataset Types (Standard, Pretokenized)</li> <li>Support for Multiple Annotation Schemes (IO, BIO, BILOU)</li> <li>Text Encoding</li> </ul> <p>Training</p> <ul> <li>Adaptive Fine-tuning</li> <li>Hyperparameter Search</li> <li>Multiple Runs with Different Random Seeds</li> <li>Detailed Analysis of Training Results</li> </ul> <p>Evaluation</p> <ul> <li>Evaluation of Any Model on Any Dataset</li> </ul> <p>Inference</p> <ul> <li>Versatile Model Inference (Entity/Word Level, Probabilities, ..)</li> </ul> <p>Other</p> <ul> <li>Full Compatibility with HuggingFace</li> <li>GPU Support</li> <li>Language Agnosticism</li> </ul>"},{"location":"#citation","title":"Citation","text":"<pre><code>@misc{nerblackbox,\n  author = {Stollenwerk, Felix},\n  title  = {nerblackbox: a high-level library for named entity recognition in python},\n  year   = {2021},\n  url    = {https://github.com/flxst/nerblackbox},\n}\n</code></pre>"},{"location":"cli/","title":"CLI (Command Line Interface)","text":""},{"location":"cli/#nerblackbox","title":"nerblackbox","text":"<p>Usage:</p> <pre><code>nerblackbox [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --store_dir TEXT  [str] relative path of store directory\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#nerblackbox-mlflow","title":"nerblackbox mlflow","text":"<p>show detailed training results in mlflow (port = 5000).</p> <p>Usage:</p> <pre><code>nerblackbox mlflow [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#nerblackbox-tensorboard","title":"nerblackbox tensorboard","text":"<p>show detailed training results in tensorboard. (port = 6006).</p> <p>Usage:</p> <pre><code>nerblackbox tensorboard [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"data/","title":"Data","text":""},{"location":"data/#overview","title":"Overview","text":"<p>The first step in the nerblackbox workflow (after the mandatory Preparation) is  to choose a dataset. Datasets for NER can differ in many regards:</p> <ul> <li>Dataset Source: Many datasets are publicly available on HuggingFace or in GitHub repositories. You may also have private datasets on your local filesystem or an annotation tool server.</li> <li>Dataset Type: Some datasets are pretokenized while others come as plain text together with a list of entities and their spans. </li> <li>Dataset Format: Even for two datasets of the same Dataset Type, the specific formatting of the data may differ.</li> <li>Dataset Splits: Some datasets are divided into train/validation/test subsets, while others only have train/test subsets, or aren't split into subsets at all.</li> <li>Annotation Scheme: There exist different annotation schemes like IO, BIO, BILOU and variations thereof.</li> </ul> <p>nerblackbox can handle all these different cases automatically.  It provides a simple Dataset class which allows to easily set up any dataset and make it readily available for training and evaluation:</p> set up a dataset Python <pre><code>dataset = Dataset(name=\"&lt;dataset_name&gt;\", source=\"&lt;source&gt;\")  # &lt;source&gt; = HF, BI, LF\ndataset.set_up()\n</code></pre> <p>This creates dataset files in the folder <code>./store/datasets/&lt;dataset_name&gt;</code>.</p> <p>The above procedure may---depending on the dataset---include downloading, reformatting, train/validation/test splitting and an analysis of the data. In the following, we will describe the details of this procedure.</p>"},{"location":"data/#dataset-sources","title":"Dataset Sources","text":"<p>Datasets can be taken directly from HuggingFace (HF),  the pool of Built-in (BI) datasets, or the Local Filesystem (LF). Integration of data from Annotation Tool (AT) servers is also possible.</p>"},{"location":"data/#huggingface-hf","title":"HuggingFace (HF)","text":"<p>To employ a dataset from HuggingFace, specify <code>\"HF\"</code> as the source in addition to the name of the dataset:</p> set up a dataset from HuggingFace (HF) Python <pre><code>dataset = Dataset(name=\"conll2003\", source=\"HF\")\ndataset.set_up()\n</code></pre> <p>Dataset subsets, if existent, can also be chosen.  For more details, we refer to the Python API documentation.</p>"},{"location":"data/#built-in-bi","title":"Built-in (BI)","text":"<p>The following datasets are built-in:</p> Name Full Name Language Open Source Annotation Scheme Sample Type #Samples (Train, Val, Test) Required Files Source swedish_ner_corpus Swedish NER Corpus Swedish Yes IO Sentence (4819, 2066, 2453) --- Description+Data sic SIC Swedish Yes BIO Sentence (436, 188, 268) --- Description+Data suc SUC 3.0 Swedish No BIO Sentence (71046, 1546, 1568) <code>suc-*.conll</code> Description swe_nerc Swe-NERC Swedish Yes BIO Sentence (6841, 878, 891) --- Description; Data <p>To employ a built-in dataset, specify <code>\"BI\"</code> as the source in addition to the name of the dataset:</p> set up a dataset that is Built-in (BI) Python <pre><code>dataset = Dataset(name=\"swe_nerc\", source=\"BI\")\ndataset.set_up()\n</code></pre> <p>Note that for datasets which are not open source, the <code>Required Files</code> need to be provided manually, see Local Filesystem (LF).</p>"},{"location":"data/#local-filesystem-lf","title":"Local Filesystem (LF)","text":"<p>Data from the Local Filesystem needs to be stored either as <code>.jsonl</code> or <code>.csv</code>, depending on the Dataset Type. Make sure that the correct Dataset Format is followed.  You can either provide a single file or three separate files for the train/val/test Dataset Splits. A single file dataset will automatically be split as part of the setup (reformatting step).</p> <p>The following name conventions need to be obeyed.</p> Dataset Type Dataset Splits File Name Standard No <code>&lt;local_dataset&gt;.jsonl</code> Standard Yes <code>train.jsonl</code>, <code>val.jsonl</code>, <code>test.jsonl</code> Pretokenized No <code>&lt;local_dataset&gt;.csv</code> Pretokenized Yes <code>train.csv</code>, <code>val.csv</code>, <code>test.csv</code> <p>To include a dataset from your local filesystem, do the following:</p> <ul> <li> <p>Create a folder <code>./store/datasets/&lt;local_dataset&gt;</code></p> </li> <li> <p>Add one or three <code>.jsonl</code> or <code>.csv</code> file(s) to the folder. </p> </li> </ul> <p>To employ the local dataset, specify its name, <code>\"LF\"</code> as the source and whether it is pretokenized and/or split:</p> set up a dataset from Local Filesystem (LF) Python <pre><code>dataset = Dataset(name=\"my_dataset\", source=\"LF\", pretokenized=False, split=False)\ndataset.set_up()\n</code></pre>"},{"location":"data/#annotation-tool-at","title":"Annotation Tool (AT)","text":"<p>nerblackbox provides tools to seamlessly integrate two popular (open source) annotation tools, LabelStudio and Doccano. Datasets can be downloaded from a server like this:</p> Download Data from Annotation Tool Python <p><pre><code>annotation_tool = AnnotationTool.from_config(dataset_name=\"&lt;dataset_name&gt;\", config_file=f\"&lt;config_file_path&gt;\")\nannotation_tool.download(project_name=\"&lt;project_name&gt;\")\n</code></pre> <code>&lt;project_name&gt;</code> has to correspond to a project name in the annotation tool.  <code>&lt;dataset_name&gt;</code> is used within nerblackbox and may be chosen at discretion.</p> <p>In order for the above to work, a config file (<code>*.ini</code>) with the annotation tool server's access information needs to be provided. They differ slightly for LabelStudio and Doccano, as can be seen in the following examples:</p> LabelStudio Integration Config FilePython <pre><code># labelstudio.ini\n[main]\ntool = labelstudio\nurl = http://localhost:8081\napi_key = &lt;ADD_API_KEY_HERE&gt;\n</code></pre> <pre><code>annotation_tool = AnnotationTool.from_config(dataset_name=\"my_dataset\", config_file=f\"labelstudio.ini\")\nannotation_tool.download(project_name=\"my_project\")\n</code></pre> Doccano Integration Config FilePython <pre><code># doccano.ini\n[main]\ntool = doccano\nurl = http://localhost:8082\nusername = admin\npassword = password\n</code></pre> <pre><code>annotation_tool = AnnotationTool.from_config(dataset_name=\"my_dataset\", config_file=f\"doccano.ini\")\nannotation_tool.download(project_name=\"my_project\")\n</code></pre> <p>After the dataset is downloaded, it will consist of a single <code>.json</code> file of the standard Dataset Type. It can be treated just like a dataset from the Local Filesystem (LF).  Note, however, that an additional argument <code>file_path</code> needs to be provided, as shown in the following example:</p> set up dataset from Annotation Tool (AT) Python <pre><code># download\nannotation_tool = AnnotationTool.from_config(dataset_name=\"my_dataset\", config_file=f\"labelstudio.ini\")\nannotation_tool.download(project_name=\"my_project\")\nfile_path = annotation_tool.get_file_path(project_name=\"my_project\")\n\n# set up\ndataset = Dataset(name=\"my_dataset\", source=\"LF\", pretokenized=False, split=False, file_path=file_path)  \ndataset.set_up()\n</code></pre> <p>For further details, we refer to the Python API documentation and the example notebooks.</p>"},{"location":"data/#dataset-types","title":"Dataset Types","text":"<p>Annotated data for named entity recognition in its purest form contains raw text together with a list of entities. Each entity is defined by its position in the raw text and the corresponding tag. We call this the standard type.</p> <p>After the data is read in, the model's tokenizer is used to pretokenize it. At inference time, the model makes predictions on the pretokenized data. Subsequently, these predictions are mapped back to the original text.</p> <p></p> <p>Often times, especially in the case of public datasets, the data already comes as pretokenized though.  We call this the pretokenized type. In the case of pretokenized data at inference time, the information to map the predictions back to the original text is missing. Hence, the last step in the above chart is skipped.</p> <p>nerblackbox can process datasets in both the standard and pretokenized type.  Both are assigned specific formats, which we will discuss next.</p>"},{"location":"data/#dataset-formats","title":"Dataset Formats","text":"<p>nerblackbox uses a single, well-defined data format for each Dataset Type.  Datasets which are downloaded from the internet (<code>source = HF, BI</code>) are automatically reformatted as part of the setup, if needed. </p> <p>The standard format, which is commonly used by annotation tools, looks as follows:</p> Example: standard format (*.jsonl) <pre><code>{\n    \"text\": \"President Barack Obama went to Harvard\",\n    \"tags\": [\n        {\n            \"token\": \"President Barack Obama\",\n            \"tag\": \"PER\",\n            \"char_start\": 0,\n            \"char_end\": 22\n        },\n        {\n            \"token\": \"Harvard\",\n            \"tag\": \"ORG\",\n            \"char_start\": 31,\n            \"char_end\": 38\n        }\n    ]\n}\n</code></pre> <p>Note that the above represents a single training sample that constitutes a single line in the <code>jsonl</code> file.  The indentations are shown for convenience only.</p> <p>The pretokenized format, which is commonly used in public datasets, looks as follows:</p> Example: pretokenized format (*.csv) <pre><code>PER PER PER O O ORG &lt;tab&gt; President Barack Obama went to Harvard\n</code></pre> <p>Note that each row has to contain a single training sample in the format <code>&lt;tags&gt; &lt;tab&gt; &lt;text&gt;</code>, where in <code>&lt;tags&gt;</code> and <code>&lt;text&gt;</code> the tags and tokens are separated by whitespace.</p>"},{"location":"data/#dataset-splits","title":"Dataset Splits","text":"<p>nerblackbox always requires a dataset to be split into train/validation/test subsets. If the original dataset does not fulfill this condition, it is automatically split as part of the setup.</p> <p>By default, a single file dataset is split into 80% training, 10% validation and 10% test subsets.  If the original dataset already consists of train/test subsets, 11.1% of the train subset is used for validation  (which, assuming that the test subset amounts to 10% of the whole data, leads to the same 80/10/10 split).</p> <p>The relative sizes of the subsets can be specified as arguments of the <code>set_up()</code> method.</p> specify size of validation subset Python <pre><code>dataset = Dataset(name=\"swedish_ner_corpus\", source=\"HF\")  # contains train/test subsets\ndataset.set_up(val_fraction=0.2)                           # use 20% of the train subset for validation\n</code></pre> <p>For more details, we refer to the Python API documentation.</p>"},{"location":"data/#annotation-schemes","title":"Annotation Schemes","text":"<p>NER datasets come with different annotation schemes.  nerblackbox uses the 3 standardized schemes IO, BIO and BILOU:</p> <p></p> <p>For the BIO and BILOU schemes, there exist many variations (e.g. IOB1, IOB2 for BIO) and equivalent schemes (e.g. IOBES for BILOU). nerblackbox automatically recognizes a dataset's annotation scheme (e.g. IOB1) and translates it to the corresponding standardized scheme (e.g. BIO) as part of the setup, if needed.  Note that the standardized BIO scheme corresponds to the variation IOB2.</p> <p>nerblackbox allows to switch between the annotation schemes at Training time, via the Parameter <code>annotation_scheme</code>. Note, however, that the information content of the IO scheme is inferior compared to the BIO and BILOU schemes.</p>"},{"location":"data/#dataset-analysis","title":"Dataset Analysis","text":"<p>After a dataset is set up, some useful features and statistics (e.g. tags, tag distribution) can be found at <code>./store/datasets/&lt;dataset_name&gt;/analyze_data</code></p>"},{"location":"evaluation/","title":"Evaluation","text":"<p>The Model class provides the functionality to evaluate any NER model on any NER dataset. Both the fine-tuned NER model and the dataset can either be loaded from HuggingFace (HF) or the Local Filesystem (LF).</p>"},{"location":"evaluation/#usage","title":"Usage","text":"<p>1) load a Model instance:</p> load model Python <pre><code># from local checkpoint directory\nmodel = Model.from_checkpoint(\"&lt;checkpoint_directory&gt;\")\n\n# from training\nmodel = Model.from_training(\"&lt;training_name&gt;\")\n\n# from HuggingFace\nmodel = Model.from_huggingface(\"&lt;repo_id&gt;\")\n</code></pre> <p>2) use the evaluate_on_dataset() method:</p> model evaluation on dataset Python <pre><code># local dataset in standard format (jsonl)\nresults = model.evaluate_on_dataset(\"&lt;local_dataset_in_standard_format&gt;\", \"jsonl\", phase=\"test\")\n\n# local dataset in pretokenized format (csv)\nresults = model.evaluate_on_dataset(\"&lt;local_dataset_in_pretokenized_format&gt;\", \"csv\", phase=\"test\")\n\n# huggingface dataset in pretokenized format\nresults = model.evaluate_on_dataset(\"&lt;huggingface_dataset_in_pretokenized_format&gt;\", \"huggingface\", phase=\"test\")\n</code></pre>"},{"location":"evaluation/#interpretation","title":"Interpretation","text":"<p>The returned object <code>results</code> is a nested dictionary <code>results[label][level][metric]</code> where</p> <ul> <li><code>label</code> in <code>['micro', 'macro']</code></li> <li><code>level</code> in <code>['entity', 'token']</code></li> <li><code>metric</code> in <code>['precision', 'recall', 'f1', 'precision_seqeval', 'recall_seqeval', 'f1_seqeval']</code></li> </ul> results <pre><code>results[\"micro\"][\"entity\"]\n# {\n#   'precision': 0.912,\n#   'recall': 0.919,\n#   'f1': 0.916,\n#   'precision_seqeval': 0.907,\n#   'recall_seqeval': 0.919,\n#   'f1_seqeval': 0.913}\n# }\n</code></pre> <p>The metrics <code>precision</code>, <code>recall</code> and <code>f1</code> are nerblackbox's evaluation results, whereas their counterparts with a <code>_seqeval</code> suffix correspond to the results you would get using the seqeval library (which is also used by and HuggingFace evaluate). The difference lies in the way model predictions which are inconsistent with the employed Annotation Scheme are handled. While nerblackbox's evaluation ignores them, seqeval takes them into account.</p>"},{"location":"evaluation/#example","title":"Example","text":"<p>A complete example of an evaluation using both the model and the dataset from HuggingFace:</p> complete evaluation example Python <pre><code># 1. load the model\nmodel = Model.from_huggingface(\"dslim/bert-base-NER\")\n\n# 2. evaluate the model on the dataset\nresults = model.evaluate_on_dataset(\"conll2003\", \"huggingface\", phase=\"test\")\n\n# 3. inspect the results\nresults[\"micro\"][\"entity\"]\n# {\n#   'precision': 0.912,\n#   'recall': 0.919,\n#   'f1': 0.916,\n#   'precision_seqeval': 0.907,\n#   'recall_seqeval': 0.919,\n#   'f1_seqeval': 0.913}\n# }\n</code></pre> <p>Note that the seqeval results are in accordance with the official results. The nerblackbox results have a slightly higher precision (and f1 score).</p>"},{"location":"inference/","title":"Inference","text":"<p>The Model class provides the functionality for versatile model predictions on one or multiple documents. The corresponding fine-tuned NER model can either be loaded from HuggingFace (HF) or the Local Filesystem (LF).</p>"},{"location":"inference/#basic-usage","title":"Basic Usage","text":"<p>1) load a Model instance:</p> load model Python <pre><code># from local checkpoint directory\nmodel = Model.from_checkpoint(\"&lt;checkpoint_directory&gt;\")\n\n# from training\nmodel = Model.from_training(\"&lt;training_name&gt;\")\n\n# from HuggingFace\nmodel = Model.from_huggingface(\"&lt;repo_id&gt;\")\n</code></pre> <p>2) use the predict() method:</p> model inference Python <pre><code>model.predict(&lt;text_input&gt;)\n</code></pre>"},{"location":"inference/#example","title":"Example","text":"Basic Inference Python <pre><code>model = Model.from_training(\"my_training\")\n\n# predict on entity level\nmodel.predict(\"The United Nations has never recognised Jakarta's move.\", level=\"entity\")  \n# [[\n#  {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'},\n#  {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'}\n# ]]\n</code></pre>"},{"location":"inference/#advanced-usage","title":"Advanced Usage","text":"<p>The predict() method used in the example above might be the most important tool for inference.  However, the Model class provides multiple methods to cover different use cases:</p> <ul> <li>predict_on_file() takes a <code>jsonl</code> file as input and writes another <code>jsonl</code> file with predictions on the entity level.   This is for instance useful if one wants to annotate (large) amounts of text in production.</li> <li>predict() takes a single <code>string</code> or a <code>list of strings</code> as input. It allows to inspect the model predictions on the entity or word level. This is useful for instance for development and debugging.</li> <li>predict_proba() is similar to predict(), but returns predictions on the word level only, together with their probabilities. This can be useful for instance in conjunction with active learning.</li> </ul> <p>An overview is given in the following table:</p> method input level probabilities predict_on_file() jsonl file with documents entity no predict() one or multiple documents entity, word no predict_proba() one or multiple documents word yes"},{"location":"inference/#example_1","title":"Example","text":"Advanced Inference Python <pre><code>model = Model.from_training(\"my_training\")\n\n# predict on entity level using file \nmodel.predict_on_file(\"&lt;input_file&gt;\", \"&lt;output_file&gt;\")  \n\n# predict on word level \nmodel.predict(\"The United Nations has never recognised Jakarta's move.\", level=\"word\")  \n# [[\n#  {'char_start': '4', 'char_end': '18', 'token': 'United Nations', 'tag': 'ORG'},\n#  {'char_start': '40', 'char_end': '47', 'token': 'Jakarta', 'tag': 'LOC'}\n# ]]\n\n# predict probabilities on word level \nmodel.predict_proba([\"arbetsf\u00f6rmedlingen finns i stockholm\"])\n# [[\n#     {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"proba_dist: {\"O\": 0.21, \"B-ORG\": 0.56, ..}},\n#     {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"proba_dist: {\"O\": 0.87, \"B-ORG\": 0.02, ..}},\n#     {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"proba_dist: {\"O\": 0.95, \"B-ORG\": 0.01, ..}},\n#     {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"proba_dist: {\"O\": 0.14, \"B-ORG\": 0.22, ..}},\n# ]]\n</code></pre> <p>See Model for further details.</p>"},{"location":"preparation/","title":"Preparation","text":""},{"location":"preparation/#basic-usage","title":"Basic Usage","text":"<p>nerblackbox provides a Python API with four main classes  <code>Store</code>, <code>Dataset</code>, <code>Training</code> and <code>Model</code>. It is complemented by a CLI.</p> basic usage PythonCLI <pre><code>from nerblackbox import Store, Dataset, Training, Model\n</code></pre> <pre><code>nerblackbox --help\n</code></pre>"},{"location":"preparation/#store","title":"Store","text":"<p>As a mandatory first step, a store has to be created.  The store is a directory that contains all the data  (datasets, training configurations, results, model checkpoints) that nerblackbox needs access to.  It is handled by the Store class:</p> create store Python <pre><code>Store.create()\n</code></pre> <p>By default, the store is located at <code>./store</code> and has the following subdirectories:</p> <pre><code>store/\n\u2514\u2500\u2500 datasets\n\u2514\u2500\u2500 training_configs\n\u2514\u2500\u2500 pretrained_models\n\u2514\u2500\u2500 results\n</code></pre> <p>If wanted, the path of the store can be adjusted (before creation) like this:</p> adjust store path Python <pre><code>Store.set_path(\"&lt;store_path&gt;\")\n</code></pre>"},{"location":"training/","title":"Training","text":"<p>Given a dataset that is properly set up, we can fine-tune a pretrained model for Named Entity Recognition.</p>"},{"location":"training/#model-sources","title":"Model Sources","text":"<p>nerblackbox works with PyTorch transformer models only.  They can either be taken straight from HuggingFace (HF) or the Local Filesystem (LF). In order to employ models from HF, it is sufficient to specify the name of the model (see Basic Training).</p> <p>Local models need to be stored in a directory <code>./store/pretrained_models/&lt;my_model&gt;</code> and (at least) include the following files:</p> <ul> <li><code>config.json</code></li> <li><code>pytorch_model.bin</code></li> <li><code>tokenizer_config.json</code>, <code>tokenizer.json</code>, <code>vocab.json</code> (or <code>vocab.txt</code>) </li> </ul> <p>Note that the name for <code>&lt;my_model&gt;</code> must include the architecture type, e.g. <code>bert</code>.</p>"},{"location":"training/#basic-training","title":"Basic Training","text":"<p>A specific model can be trained on a specific dataset using specific parameters using the Training class.</p>"},{"location":"training/#define-the-training","title":"Define the Training","text":"<p>The training is defined </p> <ul> <li> <p>either dynamically through arguments when a Training instance is created</p> define training dynamically Python <pre><code>training = Training(\"&lt;training_name&gt;\", model=\"&lt;model_name&gt;\", dataset=\"&lt;dataset_name&gt;\")\n</code></pre> </li> <li> <p>or statically by a training configuration file <code>./store/training_configs/&lt;training_name&gt;.ini</code>.</p> define training statically Python <pre><code>training = Training(\"&lt;training_name&gt;\", from_config=True)\n</code></pre> </li> </ul> <p>Note that the dynamic variant also creates a training configuration, which is subsequently used. In both cases, the specification of the <code>model</code> and the <code>dataset</code> are mandatory and sufficient. Training Parameters may be specified but are optional. The hyperparameters that are used by default are globally applicable settings that should give close-to-optimal results for any use case. In particular, adaptive fine-tuning is employed to ensure that this holds irrespective of the size of the dataset.  </p>"},{"location":"training/#run-the-training","title":"Run the Training","text":"<p>The training is run using the following command:</p> run training Python <pre><code>training.run()\n</code></pre> <p>See the Python API documentation for further details.</p>"},{"location":"training/#main-results","title":"Main Results","text":"<p>When the training is finished, one can get its main results like so:</p> Main Results (single training) Python <pre><code>training.get_result(metric=\"f1\", level=\"entity\", phase=\"test\")\n</code></pre> <p>See the Python API documentation for further details.</p> <p>An overview of all conducted trainings and their main results can be accessed using the Store class:</p> Main Results (all trainings) Python <pre><code>Store.show_trainings()\n</code></pre>"},{"location":"training/#example","title":"Example","text":"<p>An English BERT model can be trained on the CoNLL-2003 dataset like this:</p> Example: Training <pre><code>training = Training(\"my_training\", model=\"bert-base-cased\", dataset=\"conll2003\") \ntraining.run()                                                                       \ntraining.get_result(metric=\"f1\", level=\"entity\", phase=\"test\")                       \n# 0.9045\n</code></pre>"},{"location":"training/#advanced-training","title":"Advanced Training","text":""},{"location":"training/#parameters","title":"Parameters","text":"<p>nerblackbox uses a large amount of default (hyper)parameters that can be customized as needed.  The concerned parameters just need to be specified when the training is defined,  either statically or dynamically.</p> <ul> <li> <p>In the static case, a training configuration file may look like this:</p> Example: static training configuration file with parameters <pre><code># my_training.ini\n\n[dataset]\ndataset_name = swedish_ner_corpus\nannotation_scheme = plain\ntrain_fraction = 0.1  # for testing\nval_fraction = 1.0\ntest_fraction = 1.0\ntrain_on_val = False\ntrain_on_test = False\n\n[model]\npretrained_model_name = af-ai-center/bert-base-swedish-uncased\n\n[settings]\ncheckpoints = True\nlogging_level = info\nmultiple_runs = 1\nseed = 42\n\n[hparams]\nmax_epochs = 250\nearly_stopping = True\nmonitor = val_loss\nmin_delta = 0.0\npatience = 0\nmode = min\nlr_warmup_epochs = 2\nlr_num_cycles = 4\nlr_cooldown_restarts = True\nlr_cooldown_epochs = 7\n\n[runA]\nbatch_size = 16\nmax_seq_length = 128\nlr_max = 2e-5\nlr_schedule = constant\n</code></pre> </li> <li> <p>In the dynamic case, the equivalent example is:</p> Example: dynamic training with parameters <pre><code>training = Training(\n    \"my_training\", \n    model=\"af-ai-center/bert-base-swedish-uncased\",  # model = model_name\n    dataset=\"swedish_ner_corpus\",                    # dataset = dataset_name\n    annotation_scheme=\"plain\",\n    train_fraction=0.1,                              # for testing\n    val_fraction=1.0,\n    test_fraction=1.0,\n    train_on_val=False,\n    train_on_test=False,\n    checkpoints=True,\n    logging_level=\"info\",\n    multiple_runs=1,\n    seed=42,\n    max_epochs=250,\n    early_stopping=True,\n    monitor=\"val_loss\",\n    min_delta=0.0,\n    patience=0,\n    mode=\"min\",\n    lr_warmup_epochs=2,\n    lr_num_cycles=4,\n    lr_cooldown_restarts=True,\n    lr_cooldown_epochs=7,\n    batch_size=16,\n    max_seq_length=128,\n    lr_max=2e-5,\n    lr_schedule=\"constant\",\n)\n</code></pre> </li> </ul> <p>The parameters can be divided into 4 parameter groups:</p> <ol> <li>Dataset</li> <li>Model</li> <li>Settings</li> <li>Hyperparameters</li> </ol> <p>In the following, we will go through the different parameters step by step to see what they mean.</p> <p>1. Dataset</p> Key Mandatory Default Value Type Values Comment dataset_name Yes --- str e.g. conll2003 key = dataset can be used instead annotation_scheme No auto str auto, plain, bio, bilou specify annotation scheme (e.g. BIO). auto means it is inferred from data train_fraction No 1.0 float 0.0 - 1.0 fraction of train dataset to be used val_fraction No 1.0 float 0.0 - 1.0 fraction of val   dataset to be used test_fraction No 1.0 float 0.0 - 1.0 fraction of test  dataset to be used train_on_val No False bool True, False whether to train additionally on validation dataset train_on_test No False bool True, False whether to train additionally on test dataset Example: static training configuration file with parameters (Dataset) <pre><code># my_training.ini\n# ..\n\n[dataset]\ndataset_name = swedish_ner_corpus\nannotation_scheme = plain\ntrain_fraction = 0.1  # for testing\nval_fraction = 1.0\ntest_fraction = 1.0\ntrain_on_val = False\ntrain_on_test = False\n</code></pre> <p>2. Model</p> Key Mandatory Default Value Type Values Comment pretrained_model_name Yes --- str e.g. af-ai-center/bert-base-swedish-uncased key = model can be used instead Example: static training configuration file with parameters (Model) <pre><code># my_training.ini\n# ..\n\n[model]\npretrained_model_name = af-ai-center/bert-base-swedish-uncased\n</code></pre> <p>3. Settings</p> Key Mandatory Default Value Type Values Comment checkpoints No True bool True, False whether to save model checkpoints logging_level No info str info, debug choose logging level, debug is more verbose multiple_runs No 1 int 1+ choose how often each hyperparameter run is executed (to control for statistical uncertainties) seed No 42 int 1+ for reproducibility. multiple runs get assigned different seeds. Example: static training configuration file with parameters (Settings) <pre><code># my_training.ini\n# ..\n\n[settings]\ncheckpoints = True\nlogging_level = info\nmultiple_runs = 1\nseed = 42\n</code></pre> <p>4. Hyperparameters</p> Key Mandatory Default Value Type Values Comment batch_size No 16 int e.g. 16, 32, 64 number of training samples in one batch max_seq_length No 128 int e.g. 64, 128, 256 maximum sequence length used for model's input data max_epochs No 250 int 1+ (maximum) amount of training epochs early_stopping No True bool True, False whether to use early stopping monitor No val_loss str val_loss, val_acc if early stopping is True: metric to monitor (acc = accuracy) min_delta No 0.0 float 0.0+ if early stopping is True: minimum amount of improvement (w.r.t. monitored metric) required to continue training patience No 0 int 0+ if early stopping is True: number of epochs to wait for improvement w.r.t. monitored metric until training is stopped mode No min str min, max if early stopping is True: whether the optimum for the monitored metric is the minimum (val_loss) or maximum (val_acc) value lr_warmup_epochs No 2 int 0+ number of epochs to linearly increase the learning rate during the warm-up phase, gets translated to num_warmup_steps lr_max No 2e-5 float e.g. 2e-5, 3e-5 maximum learning rate (after warm-up) for AdamW optimizer lr_schedule No constant str constant, linear, cosine, cosine_with_hard_restarts, hybrid Learning Rate Schedule, i.e. how to vary the learning rate (after warm-up). hybrid = constant + linear cool-down. lr_num_cycles No 4 int 1+ num_cycles for lr_schedule = cosine or lr_schedule = cosine_with_hard_restarts lr_cooldown_restarts No True bool True, False if early stopping is True: whether to restart normal training if monitored metric improves during cool-down phase lr_cooldown_epochs No 7 int 0+ if early stopping is True or lr_schedule == hybrid: number of epochs to linearly decrease the learning rate during the cool-down phase Example: static training configuration file with parameters (Hyperparameters) <pre><code># my_training.ini\n# ..\n\n[hparams]\nmax_epochs = 250\nearly_stopping = True\nmonitor = val_loss\nmin_delta = 0.0\npatience = 0\nmode = min\nlr_warmup_epochs = 2\nlr_num_cycles = 4\nlr_cooldown_restarts = True\nlr_cooldown_epochs = 7\n\n[runA]\nbatch_size = 16\nmax_seq_length = 128\nlr_max = 2e-5\nlr_schedule = constant\n</code></pre>"},{"location":"training/#presets","title":"Presets","text":"<p>In addition to the manual specification of the parameters discussed above,  the dynamic training definition allows for the use of several hyperparameter presets. They can be specified using the <code>from_preset</code> argument in Training() like so:</p> define training dynamically using preset Python <pre><code>training = Training(\"&lt;training_name&gt;\", model=\"&lt;model_name&gt;\", dataset=\"&lt;dataset_name&gt;\", from_preset=\"adaptive\")\n</code></pre> <p>In the following, we list the different presets together with the Hyperparameters that they entail:</p> <ul> <li> <p><code>from_preset = adaptive</code></p> <p>Adaptive fine-tuning (introduced in this paper) is a method that automatically trains for a near-optimal number of epochs. It is used by default in nerblackbox.</p> adaptive fine-tuning preset <pre><code>[hparams]\nmax_epochs = 250\nearly_stopping = True\nmonitor = val_loss\nmin_delta = 0.0\npatience = 0\nmode = min\nlr_warmup_epochs = 2\nlr_schedule = constant\nlr_cooldown_epochs = 7\n</code></pre> </li> <li> <p><code>from_preset = original</code></p> <p>Original fine-tuning uses the hyperparameters from the original BERT paper. The hyperparameters are suitable for large datasets.</p> original fine-tuning preset <pre><code>[hparams]\nmax_epochs = 5\nearly_stopping = False\nlr_warmup_epochs = 2\nlr_schedule = linear\n</code></pre> </li> <li> <p><code>from_preset = stable</code></p> <p>Stable fine-tuning is a method based on this paper. It is suitable for both small and large datasets.</p> stable fine-tuning preset <pre><code>[hparams]\nmax_epochs = 20\nearly_stopping = False\nlr_warmup_epochs = 2\nlr_schedule = linear\n</code></pre> </li> </ul>"},{"location":"training/#hyperparameter-search","title":"Hyperparameter Search","text":"<p>A hyperparameter grid search can easily be conducted as part of a training (currently only using the static definition). The hyperparameters one wants to vary are to be specified in special sections <code>[runA]</code>, <code>[runB]</code> etc. in the training configuration file.</p> Example: Hyperparameter Search <p><pre><code># my_training.ini\n# ..\n\n[runA]\nbatch_size = 16\nmax_seq_length = 128\nlr_max = 2e-5\nlr_schedule = constant\n\n[runB]\nbatch_size = 32\nmax_seq_length = 64\nlr_max = 3e-5\nlr_schedule = cosine\n</code></pre> This creates 2 hyperparameter runs (<code>runA</code> &amp; <code>runB</code>). Each hyperparameter run is executed <code>multiple_runs</code> times (see Parameters).</p>"},{"location":"training/#multiple-seeds","title":"Multiple Seeds","text":"<p>The results of a training run depend on the employed random seed, see e.g. this paper for a discussion. One may conduct multiple runs with different seeds that are otherwise identical, in order to</p> <ul> <li> <p>get control over the uncertainties (see Detailed Results)</p> </li> <li> <p>get an improved model performance</p> </li> </ul> <p>Multiple runs can easily be specified in the training configuration.</p> Example: Settings / Multiple Runs <pre><code># my_training.ini\n# ..\n\n[settings]\nmultiple_runs = 3\nseed = 42\n</code></pre> <p>This creates 3 runs with seeds 43, 44 and 45.</p>"},{"location":"training/#detailed-results","title":"Detailed Results","text":"<p>In addition to the Main Results, one may have a look at much more detailed results of a training run using <code>mlflow</code> or <code>tensorboard</code>.</p> Detailed Results PythonCLI <pre><code>Store.mlflow(\"start\")       # + enter http://127.0.0.1:5000 in your browser\nStore.tensorboard(\"start\")  # + enter http://127.0.0.1:6006 in your browser\n</code></pre> <pre><code>nerblackbox mlflow         # + enter http://127.0.0.1:5000 in your browser\nnerblackbox tensorboard    # + enter http://127.0.0.1:6006 in your browser\n</code></pre> <p>Python: The underlying processes can be stopped using <code>Store.mlflow(\"stop\")</code> and <code>Store.tensorboard(\"stop\")</code>.</p> <ul> <li> <p><code>mlflow</code> displays precision, recall and f1 score for every single class, as well the respective micro- and macro-averages over all classes, both on the token and entity level.</p> <p>The following excerpt shows</p> <ul> <li> <p>the micro- and macro-averages of the recall on the entity level</p> </li> <li> <p>precision, recall and f1 score for the LOC(ation) class on the token level</p> </li> </ul> <p></p> <p>In addition, one has access to the log file and the confusion matrices (token and entity level) of the model predictions on the test set. A small excerpt is shown below:</p> <p></p> </li> <li> <p><code>tensorboard</code> shows the learning curves of important metrics like the loss and the f1 score.</p> <p>A small excerpt is shown below:</p> <p></p> </li> </ul>"},{"location":"advanced/compatibility_with_huggingface/","title":"Compatibility with HuggingFace","text":""},{"location":"advanced/compatibility_with_huggingface/#general","title":"General","text":"<p>nerblackbox is heavily based on HuggingFace Transformers.  Moreover, HuggingFace Datasets and HuggingFace Evaluate are well-integrated,  see Data and Evaluation, respectively.</p> <p>Therefore, compatibility with HuggingFace is generally given. In particular, </p> <ul> <li> <p>nerblackbox's model checkpoints (and tokenizer files)  are identical to the ones from HuggingFace.</p> model checkpoint directory Bash <pre><code>ls &lt;checkpoint_directory&gt;\n# config.json             \n# pytorch_model.bin\n# special_tokens_map.json \n# tokenizer.json          \n# tokenizer_config.json   \n# vocab.txt\n</code></pre> </li> <li> <p>After a Model instance is created from a checkpoint, it contains a HuggingFace model and tokenizer as attributes:</p> model attributes Python <pre><code>model = Model(&lt;checkpoint_directory&gt;)\n\nprint(type(model.model))\n# &lt;class 'transformers.models.bert.modeling_bert.BertForTokenClassification'&gt;\n\nprint(type(model.tokenizer))\n# &lt;class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'&gt;\n</code></pre> <p>Hence, <code>model.model</code> and <code>model.tokenizer</code> can be used like any other transformers model and tokenizer, respectively.</p> </li> </ul>"},{"location":"advanced/compatibility_with_huggingface/#models","title":"Models","text":"<ul> <li> <p>Model architectures that have successfully been tested (see Reproduction of Results) with nerblackbox are:</p> <ul> <li><code>BERT</code></li> <li><code>DistilBERT</code></li> <li><code>RoBERTa</code></li> <li><code>DeBERTa</code></li> <li><code>ELECTRA</code></li> </ul> </li> <li> <p>Model architectures that are known to currently not work with nerblackbox are:</p> <ul> <li><code>XLM-RoBERTa</code></li> <li><code>ALBERT</code></li> </ul> </li> </ul>"},{"location":"advanced/reproduction_of_results/","title":"Reproduction of Results","text":"<p>Results reported on HuggingFace are reproduced with nerblackbox as a cross-check.  We do this separately for:</p> <ul> <li>Training (pretrained models are fine-tuned for NER and subsequently evaluated on the validation set)</li> <li>Evaluation (NER models are evaluated on either the validation or test set)</li> </ul> <p>Note that all numbers refer to the micro-averaged f1 score as computed by the <code>seqeval</code> library, see Evaluation.</p>"},{"location":"advanced/reproduction_of_results/#training","title":"Training","text":"Model Dataset Parameters version nerblackbox reported bert-base-cased conll2003 {\"from_preset\": \"original\", \"multiple_runs\": 3, \"max_epochs\": 5, \"lr_warmup_epochs\": 0} <code>1.0.0</code> 0.946(1) 0.951 reference distilbert-base-multilingual-cased conll2003 {\"from_preset\": \"original\", \"multiple_runs\": 3, \"max_epochs\": 3, \"lr_warmup_epochs\": 0} <code>1.0.0</code> 0.943(1) 0.941 reference dmis-lab/biobert-base-cased-v1.2 ncbi_disease {\"from_preset\": \"original\", \"multiple_runs\": 3, \"max_epochs\": 3, \"lr_warmup_epochs\": 0, \"batch_size\": 4} <code>1.0.0</code> 0.855(4) 0.845 reference distilroberta-base conll2003 {\"from_preset\": \"original\", \"multiple_runs\": 3, \"max_epochs\": 6, \"lr_warmup_epochs\": 0, \"batch_size\": 32, \"lr_max\": 5e-5} <code>1.0.0</code> 0.953(1) 0.953 reference microsoft/deberta-base conll2003 {\"from_preset\": \"original\", \"multiple_runs\": 3, \"max_epochs\": 5, \"lr_warmup_epochs\": 0, \"batch_size\": 16, \"lr_max\": 5e-5} <code>1.0.0</code> 0.957(1) 0.961 reference <p>Each model was fine-tuned multiple times using nerblackbox and different random seeds.  The resulting uncertainty of the mean value is specified in parentheses and refers to the last digit.  In all cases, the evaluation was conducted on the respective validation dataset. Note that small, systematic differences are to be expected as the reported results were created using very similar  yet slightly different hyperparameters.</p> <p>The results may be reproduced using the following code:</p> reproduction of training results Python <p><pre><code>from nerblackbox import Dataset, Training\n\ndataset = Dataset(name=&lt;dataset&gt;, source=\"HF\")\ndataset.set_up()\n\nparameters = {\"from_preset\": \"original\", [..]}\ntraining = Training(\"training\", model=\"&lt;model&gt;\", dataset=\"&lt;dataset&gt;\", **parameters)\ntraining.run()\n\nresult = training.get_result(metric=\"f1\", level=\"entity\", phase=\"validation\")\nprint(result)\n</code></pre> Note that the first use of nerblackbox requires the creation of a Store.</p>"},{"location":"advanced/reproduction_of_results/#evaluation","title":"Evaluation","text":"Model Dataset Phase version nerblackbox evaluate reported dslim/bert-base-NER conll2003 validation <code>1.0.0</code> 0.951 0.951 0.951 reference jordyvl/biobert-base-cased-v1.2_ncbi_disease-softmax-labelall-ner ncbi_disease validation <code>1.0.0</code> 0.845 0.845 0.845 reference fhswf/bert_de_ner germeval_14 test <code>1.0.0</code> 0.818 0.818 0.829 reference philschmid/distilroberta-base-ner-conll2003 conll2003 validation <code>1.0.0</code> 0.955 0.955 0.953 reference philschmid/distilroberta-base-ner-conll2003 conll2003 test <code>1.0.0</code> 0.913 0.913 0.907 reference projecte-aina/roberta-base-ca-cased-ner projecte-aina/ancora-ca-ner test <code>1.0.0</code> 0.896 0.896 0.881 reference gagan3012/bert-tiny-finetuned-ner conll2003 validation <code>1.0.0</code> 0.847 --- 0.818 reference malduwais/distilbert-base-uncased-finetuned-ner conll2003 test <code>1.0.0</code> 0.894 --- 0.930 reference IIC/bert-base-spanish-wwm-cased-ehealth_kd ehealth_kd test <code>1.0.0</code> 0.825 --- 0.843 reference drAbreu/bioBERT-NER-BC2GM_corpus bc2gm_corpus test <code>1.0.0</code> 0.808 --- 0.815 reference geckos/deberta-base-fine-tuned-ner conll2003 validation <code>1.0.0</code> 0.962 --- 0.961 reference <p>The results may be reproduced using the following code:</p> reproduction of evaluation results Python <p><pre><code>###############\n# nerblackbox\n###############\nfrom nerblackbox import Model\nmodel = Model.from_huggingface(\"&lt;model&gt;\", \"&lt;dataset&gt;\")\nresults_nerblackbox = model.evaluate_on_dataset(\"&lt;dataset&gt;\", phase=\"&lt;phase&gt;\")\nprint(results_nerblackbox[\"micro\"][\"entity\"][\"f1_seqeval\"])\n\n###############\n# evaluate\n###############\nfrom datasets import load_dataset\nfrom evaluate import evaluator\nevaluator = evaluator(\"token-classification\")\ndata = load_dataset(\"&lt;dataset&gt;\", split=\"&lt;phase&gt;\")\nresults_evaluate = evaluator.compute(\n    model_or_pipeline=\"&lt;model&gt;\",\n    data=data,\n    metric=\"seqeval\",\n)\nprint(results_evaluate[\"overall_f1\"])\n</code></pre> Note that the first use of nerblackbox requires the creation of a Store.</p> <p>Evaluation using the evaluate library fails for some of the tested datasets (---).  For all cases where it works, we find that results from nerblackbox and evaluate are in agreement. In contrast, the self-reported results on HuggingFace sometimes differ. </p>"},{"location":"advanced/text_encoding/","title":"Text Encoding","text":"<p>Text may contain whitespace characters (e.g. \"\\n\", \"\\t\") or special characters (e.g. \"\u2022\", emojis) that a pre-trained model has never seen before.  While the whitespace characters are ignored in the tokenization process, the special characters lead to out-of-vocabulary tokens which get replaced by  <code>[UNK]</code> tokens before being sent to the model.  Sometimes, however, the ignored or replaced tokens contain semantic information  that is valuable for the model and thus should be preserved.</p> <p>Therefore, nerblackbox allows to customly map selected special characters to self-defined special tokens (\"encoding\").  The encoded text may then be used during training and inference.</p> <p>Say we want to have the following replacements:</p> encoding Python <pre><code># map special characters to special tokens\nencoding = {\n    '\\n': '[NEWLINE]',\n    '\\t': '[TAB]',\n    '\u2022': '[DOT]',\n}\n</code></pre> <p>The first step is to save the <code>encoding</code> in an <code>encoding.json</code> file which is located in the same folder <code>./store/datasets/&lt;dataset_name&gt;</code> that contains the data  (see Data).</p> create encoding.json Python <pre><code>import json\n\nwith open('./store/datasets/&lt;custom_dataset&gt;/encoding.json', 'w') as file:\n    json.dump(encoding, file)\n</code></pre> <p>This way, the special tokens are automatically added to the model's vocabulary during training.</p> <p>The second step is to apply the <code>encoding</code> to the data.  The TextEncoder class  takes care of this:</p> TextEncoder Python <pre><code>from nerblackbox import TextEncoder\n\ntext_encoder = TextEncoder(encoding)\n</code></pre> <p>For training, one needs to encode the input text like so:</p> text encoding (training) Python <pre><code># ..load input_text \n\n# ENCODE\n# e.g. input_text             = 'We\\n are in \u2022 Stockholm' \n#      input_text_encoded     = 'We[NEWLINE] are in [DOT] Stockholm'\ninput_text_encoded, _ = text_encoder.encode(input_text)  \n\n# ..save input_text_encoded and use it for training\n</code></pre> <p>For inference, the predictions also need to be mapped back to the original text, like so:</p> text encoding (inference) Python <pre><code># ENCODE\n# e.g. input_text             = 'We\\n are in \u2022 Stockholm'\n#      input_text_encoded     = 'We[NEWLINE] are in [DOT] Stockholm'\n#      encode_decode_mappings = [(2, \"\\n\", \"[NEWLINE]\"), (13, \"\u2022\", \"[DOT]\")]\ninput_text_encoded, encode_decode_mappings = text_encoder.encode(input_text)\n\n\n# PREDICT\n# e.g. predictions_encoded    = {'char_start': 25, 'char_end': 34, 'token': 'Stockholm', 'tag': 'LOC'}\npredictions_encoded = model.predict(input_text_encoded, level=\"entity\")\n\n\n# DECODE\n# e.g. input_text_decoded     = 'We\\n are in \u2022 Stockholm' \n#      predictions            = {'char_start': 13, 'char_end': 22, 'token': 'Stockholm', 'tag': 'LOC'}\ninput_text_decoded, predictions = text_encoder.decode(input_text_encoded,\n                                                      encode_decode_mappings,\n                                                      predictions_encoded)\n</code></pre>"},{"location":"python_api/annotation_tool/","title":"AnnotationTool","text":""},{"location":"python_api/annotation_tool/#nerblackbox.api.annotation_tool.AnnotationTool.from_config","title":"<code>from_config(dataset_name, config_file, verbose=False)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>e.g. 'strangnas_test'</p> required <code>config_file</code> <code>str</code> <p>path to config file</p> required <code>verbose</code> <code>bool</code> <p>output</p> <code>False</code>"},{"location":"python_api/annotation_tool/#nerblackbox.api.annotation_tool.AnnotationTool.download","title":"<code>download(project_name, verbose=False)</code>","text":"<p>download data from project to file_path <code>f\"{Store.get_path()}/datasets/&lt;dataset_name&gt;/&lt;project_name&gt;.jsonl\"</code></p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>e.g. 'batch_1'</p> required <code>verbose</code> <code>bool</code> <p>output</p> <code>False</code>"},{"location":"python_api/annotation_tool/#nerblackbox.api.annotation_tool.AnnotationTool.get_file_path","title":"<code>get_file_path(project_name)</code>","text":"<p>get path of local file (nerblackbox format) that corresponds to <code>dataset_name</code> and <code>project_name</code></p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>e.g. 'batch_1'</p> required <p>Returns:</p> Name Type Description <code>file_path</code> <code>str</code> <p>e.g. f\"{Store.get_path()}/datasets//batch_1.jsonl\""},{"location":"python_api/annotation_tool/#nerblackbox.api.annotation_tool.AnnotationTool.upload","title":"<code>upload(project_name, verbose=False)</code>","text":"<p>upload data from file_path <code>f\"{Store.get_path()}/datasets/&lt;dataset_name&gt;/&lt;project_name&gt;.jsonl\"</code> to project</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>e.g. 'batch_2'</p> required <code>verbose</code> <code>bool</code> <p>output</p> <code>False</code>"},{"location":"python_api/dataset/","title":"Dataset","text":"<p>class to download, set up and inspect a single dataset</p>"},{"location":"python_api/dataset/#nerblackbox.api.dataset.Dataset.__init__","title":"<code>__init__(name, source, pretokenized=False, split=False, file_path=None, subset=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of dataset, e.g. \"swedish_ner_corpus\"</p> required <code>source</code> <code>str</code> <p>source of dataset, e.g. \"HF\", \"BI\", \"LF\"</p> required <code>pretokenized</code> <code>bool</code> <p>[only for source = \"LF\"] whether the dataset is pretokenized. otherwise, it has the standard type.</p> <code>False</code> <code>split</code> <code>bool</code> <p>[only for source = \"LF\"] whether the dataset is split into train/val/test subsets. otherwise, it is a single file.</p> <code>False</code> <code>file_path</code> <code>Optional[str]</code> <p>[only for source = \"LF\"] absolute file_path</p> <code>None</code> <code>subset</code> <code>Optional[str]</code> <p>[only for source = \"HF\"] name of subset if applicable, e.g. \"simple_cased\"</p> <code>None</code>"},{"location":"python_api/dataset/#nerblackbox.api.dataset.Dataset.set_up","title":"<code>set_up(val_fraction=None, test_fraction=None)</code>","text":"<p>sets up the dataset and creates the following files (if needed):</p> <ul> <li><code>&lt;STORE_DIR&gt;/datasets/&lt;name&gt;/train.*</code></li> <li><code>&lt;STORE_DIR&gt;/datasets/&lt;name&gt;/val.*</code></li> <li><code>&lt;STORE_DIR&gt;/datasets/&lt;name&gt;/test.*</code></li> </ul> <p>where <code>* = jsonl</code> or <code>* = csv</code>, depending on whether the data is pretokenized or not.</p> <p>Parameters:</p> Name Type Description Default <code>val_fraction</code> <code>Optional[float]</code> <p>e.g. 0.1 (applicable if source = HF, BI, LF)</p> <code>None</code> <code>test_fraction</code> <code>Optional[float]</code> <p>e.g. 0.1 (applicable if source = LF)</p> <code>None</code>"},{"location":"python_api/model/","title":"Model","text":"<p>model that predicts tags for given input text</p>"},{"location":"python_api/model/#nerblackbox.api.model.Model.__init__","title":"<code>__init__(checkpoint_directory, batch_size=16, max_seq_length=None, dataset=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint_directory</code> <code>str</code> <p>path to the checkpoint directory</p> required <code>batch_size</code> <code>int</code> <p>batch size used by dataloader</p> <code>16</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>maximum sequence length (Optional). Loaded from checkpoint if not specified.</p> <code>None</code> <code>dataset</code> <code>Optional[str]</code> <p>should be provided in case model is missing information on id2label in config</p> <code>None</code>"},{"location":"python_api/model/#nerblackbox.api.model.Model.evaluate_on_dataset","title":"<code>evaluate_on_dataset(dataset_name, dataset_format='infer', phase='test', class_mapping=None, number=None, derived_from_jsonl=False, rounded_decimals=3)</code>","text":"<p>evaluate model on dataset from huggingface or local dataset in jsonl or csv format</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>e.g. 'conll2003'</p> required <code>dataset_format</code> <code>str</code> <p>'huggingface', 'jsonl', 'csv'</p> <code>'infer'</code> <code>phase</code> <code>str</code> <p>e.g. 'test'</p> <code>'test'</code> <code>class_mapping</code> <code>Optional[Dict[str, str]]</code> <p>e.g. {\"PER\": \"PI\", \"ORG\": \"PI}</p> <code>None</code> <code>number</code> <code>Optional[int]</code> <p>e.g. 100</p> <code>None</code> <code>derived_from_jsonl</code> <code>bool</code> <code>False</code> <code>rounded_decimals</code> <code>Optional[int]</code> <p>if not None, results will be rounded to provided decimals</p> <code>3</code> <p>Returns:</p> Name Type Description <code>evaluation_dict</code> <code>EVALUATION_DICT</code> <p>Dict with keys label[metric] where label in ['micro', 'macro'], level in ['entity', 'token'] metric in ['precision', 'recall', 'f1', 'precision_seqeval', 'recall_seqeval', 'f1_seqeval'] and values = float between 0 and 1</p>"},{"location":"python_api/model/#nerblackbox.api.model.Model.from_checkpoint","title":"<code>from_checkpoint(checkpoint_directory)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint_directory</code> <code>str</code> <p>path to the checkpoint directory</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>Optional[Model]</code> <p>best model from training</p>"},{"location":"python_api/model/#nerblackbox.api.model.Model.from_huggingface","title":"<code>from_huggingface(repo_id, dataset=None)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>id of the huggingface hub repo id, e.g. 'KB/bert-base-swedish-cased-ner'</p> required <code>dataset</code> <code>Optional[str]</code> <p>should be provided in case model is missing information on id2label in config</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>Optional[Model]</code> <p>model</p>"},{"location":"python_api/model/#nerblackbox.api.model.Model.from_training","title":"<code>from_training(training_name)</code>  <code>classmethod</code>","text":"<p>load best model from training.</p> <p>Parameters:</p> Name Type Description Default <code>training_name</code> <code>str</code> <p>name of the training, e.g. \"training0\"</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>Optional[Model]</code> <p>best model from training</p>"},{"location":"python_api/model/#nerblackbox.api.model.Model.predict","title":"<code>predict(input_texts, level='entity', autocorrect=False, is_pretokenized=False)</code>","text":"<p>predict tags for input texts. output on entity or word level.</p> <p>Examples:</p> <p><pre><code>predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=False)\n# [[\n#     {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"I-ORG\"},\n#     {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"},\n#     {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"},\n#     {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"},\n# ]]\n</code></pre> <pre><code>predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"word\", autocorrect=True)\n# [[\n#     {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"B-ORG\"},\n#     {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"tag\": \"O\"},\n#     {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"tag\": \"O\"},\n#     {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"B-LOC\"},\n# ]]\n</code></pre> <pre><code>predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=False)\n# [[\n#     {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"},\n# ]]\n</code></pre> <pre><code>predict([\"arbetsf\u00f6rmedlingen finns i stockholm\"], level=\"entity\", autocorrect=True)\n# [[\n#     {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"tag\": \"ORG\"},\n#     {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"tag\": \"LOC\"},\n# ]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>input_texts</code> <code>Union[str, List[str]]</code> <p>e.g. [\"example 1\", \"example 2\"]</p> required <code>level</code> <code>str</code> <p>\"entity\" or \"word\"</p> <code>'entity'</code> <code>autocorrect</code> <code>bool</code> <p>if True, autocorrect annotation scheme (e.g. B- and I- tags).</p> <code>False</code> <code>is_pretokenized</code> <code>bool</code> <p>True if input_texts are pretokenized</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>PREDICTIONS</code> <p>[list] of predictions for the different examples.          each list contains a [list] of [dict] w/ keys = char_start, char_end, word, tag</p>"},{"location":"python_api/model/#nerblackbox.api.model.Model.predict_on_file","title":"<code>predict_on_file(input_file, output_file)</code>","text":"<p>predict tags for all input texts in input file, write results to output file</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>e.g. strangnas/test.jsonl</p> required <code>output_file</code> <code>str</code> <p>e.g. strangnas/test_anonymized.jsonl</p> required"},{"location":"python_api/model/#nerblackbox.api.model.Model.predict_proba","title":"<code>predict_proba(input_texts, is_pretokenized=False)</code>","text":"<p>predict probability distributions for input texts. output on word level.</p> <p>Examples:</p> <pre><code>predict_proba([\"arbetsf\u00f6rmedlingen finns i stockholm\"])\n# [[\n#     {\"char_start\": \"0\", \"char_end\": \"18\", \"token\": \"arbetsf\u00f6rmedlingen\", \"proba_dist: {\"O\": 0.21, \"B-ORG\": 0.56, ..}},\n#     {\"char_start\": \"19\", \"char_end\": \"24\", \"token\": \"finns\", \"proba_dist: {\"O\": 0.87, \"B-ORG\": 0.02, ..}},\n#     {\"char_start\": \"25\", \"char_end\": \"26\", \"token\": \"i\", \"proba_dist: {\"O\": 0.95, \"B-ORG\": 0.01, ..}},\n#     {\"char_start\": \"27\", \"char_end\": \"36\", \"token\": \"stockholm\", \"proba_dist: {\"O\": 0.14, \"B-ORG\": 0.22, ..}},\n# ]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_texts</code> <code>Union[str, List[str]]</code> <p>e.g. [\"example 1\", \"example 2\"]</p> required <code>is_pretokenized</code> <code>bool</code> <p>True if input_texts are pretokenized</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>PREDICTIONS</code> <p>[list] of probability predictions for different examples.          each list contains a [list] of [dict] w/ keys = char_start, char_end, word, proba_dist          where proba_dist = [dict] that maps self.annotation.classes to probabilities</p>"},{"location":"python_api/overview/","title":"Python API","text":"<ul> <li> <p>Usage:</p> Python <pre><code>from nerblackbox import Store, Dataset, Training, Model, [..]\n</code></pre> </li> </ul> <ul> <li> <p>Main Classes:</p> <ul> <li>Store</li> <li>Dataset</li> <li>Training</li> <li>Model</li> </ul> </li> </ul> <ul> <li>Additional Classes:<ul> <li>AnnotationTool</li> <li>TextEncoder</li> </ul> </li> </ul>"},{"location":"python_api/store/","title":"Store","text":"<p>client for the store that contains all data (datasets, training configuration files, models, results)</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Optional[str]</code> <p>path to store's main directory</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.create","title":"<code>create(verbose=False)</code>  <code>classmethod</code>","text":"<p>create store at cls.path</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>output</p> <code>False</code>"},{"location":"python_api/store/#nerblackbox.api.store.Store.get_path","title":"<code>get_path()</code>  <code>classmethod</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>cls.path: path to store's main directory</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.get_training_results","title":"<code>get_training_results()</code>  <code>classmethod</code>","text":"<p>get results for all trainings</p> <p>Returns:</p> Name Type Description <code>training_results_list</code> <code>List[TrainingResults]</code> <p>TODO: instead of list return dict that maps training_name to TrainingResults?</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.get_training_results_single","title":"<code>get_training_results_single(training_name, update_trainings=True, verbose=False)</code>  <code>classmethod</code>","text":"<p>get results for single training</p> <p>Parameters:</p> Name Type Description Default <code>training_name</code> <code>str</code> <p>e.g. 'exp0'</p> required <code>update_trainings</code> <code>bool</code> <p>whether to update cls.training_id2name &amp; cls.training_name2id</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>output</p> <code>False</code> <p>Returns:</p> Name Type Description <code>training_results</code> <code>Tuple[bool, TrainingResults]</code> <p>for training with training_name</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.mlflow","title":"<code>mlflow(action)</code>  <code>classmethod</code>","text":"<p>start or stop the mlflow server at http://127.0.0.1:5000 or check its status</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>str</code> <p>\"start\", \"status\", \"stop\"</p> required"},{"location":"python_api/store/#nerblackbox.api.store.Store.parse_training_result_single","title":"<code>parse_training_result_single(results=None, metric='f1', level='entity', label='micro', phase='test', average=True)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>Optional[TrainingResults]</code> <p>TrainingResults gotten from get_training_results_single()</p> <code>None</code> <code>metric</code> <code>str</code> <p>\"f1\", \"precision\", \"recall\"</p> <code>'f1'</code> <code>level</code> <code>str</code> <p>\"entity\" or \"token\"</p> <code>'entity'</code> <code>label</code> <code>str</code> <p>\"micro\", \"macro\", \"PER\", ..</p> <code>'micro'</code> <code>phase</code> <code>str</code> <p>\"val\" or \"test\"</p> <code>'test'</code> <code>average</code> <code>bool</code> <p>if True, return average result of all runs. if False, return result of best run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Optional[str]</code> <p>e.g. \"0.9011 +- 0.0023\" (average = True) or \"0.9045\" (average = False)</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.set_path","title":"<code>set_path(path)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to store's main directory</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>cls.path: path to store's main directory</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.show_trainings","title":"<code>show_trainings(as_df=True)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>as_df</code> <code>bool</code> <p>if True, return pandas DataFrame. if False, return dict</p> <code>True</code> <p>Returns:</p> Name Type Description <code>trainings</code> <code>Union[pd.DataFrame, Dict[str, str]]</code> <p>overview of trainings that have been run</p>"},{"location":"python_api/store/#nerblackbox.api.store.Store.tensorboard","title":"<code>tensorboard(action)</code>  <code>classmethod</code>","text":"<p>start or stop the tensorboard server at http://127.0.0.1:6006 or check its status</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>str</code> <p>\"start\", \"status\", \"stop\"</p> required"},{"location":"python_api/text_encoder/","title":"TextEncoder","text":""},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.__init__","title":"<code>__init__(encoding, model_special_tokens=None)</code>","text":"<p>Examples:</p> <pre><code>TextEncoder(\n    encoding={\"\\n\": \"[NEWLINE]\", \"\\t\": \"[TAB]\"},\n    model_special_tokens=[\"[NEWLINE]\", \"[TAB]\"],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>Dict[str, str]</code> <p>mapping to special tokens</p> required <code>model_special_tokens</code> <code>Optional[List[str]]</code> <p>special tokens that the model was trained on</p> <code>None</code>"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.decode","title":"<code>decode(text_encoded_list, encode_decode_mappings_list, predictions_encoded_list)</code>","text":"<p>decodes list of text_encoded and predictions_encoded using encode_decode_mappings</p> <p>Examples:</p> <pre><code>text_list, predictions_list = decode(\n    text_encoded_list=[\"an[NEWLINE] example\"],\n    encode_decode_mappings_list=[[(2, \"\\n\", \"[NEWLINE]\")]]),\n    predictions_encoded_list=[[{\"char_start\": \"12\", \"char_end\": \"19\", \"token\": \"example\", \"tag\": \"TAG\"}]]\n)\n# text_list = [\"an\\n example\"]\n# predictions_list = [[{\"char_start\": \"4\", \"char_end\": \"11\", \"token\": \"example\", \"tag\": \"TAG\"}]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text_encoded_list</code> <code>List[str]</code> <p>encoded text</p> required <code>encode_decode_mappings_list</code> <code>List[EncodeDecodeMappings]</code> <p>mappings (char_start, original token, encoded token)</p> required <code>predictions_encoded_list</code> <code>List[Predictions]</code> <p>encoded predictions</p> required <p>Returns:</p> Name Type Description <code>text_list</code> <code>List[str]</code> <p>original / decoded text</p> <code>predictions_list</code> <code>List[Predictions]</code> <p>original / decoded predictions</p>"},{"location":"python_api/text_encoder/#nerblackbox.modules.ner_training.data_preprocessing.text_encoder.TextEncoder.encode","title":"<code>encode(text_list)</code>","text":"<p>encodes list of text using self.encoding</p> <p>Examples:</p> <pre><code>text_encoded_list, encode_decode_mappings_list = encode(text_list=[\"an\\n example\"])\n# text_encoded_list           = [\"an[NEWLINE] example\"]\n# encode_decode_mappings_list = [[(2, \"\\n\", \"[NEWLINE]\")]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text_list</code> <code>List[str]</code> <p>original text</p> required <p>Returns:</p> Name Type Description <code>text_encoded_list</code> <code>List[str]</code> <p>encoded text</p> <code>encode_decode_mappings_list</code> <code>List[EncodeDecodeMappings]</code> <p>mappings (char_start, original token, encoded token)</p>"},{"location":"python_api/training/","title":"Training","text":""},{"location":"python_api/training/#nerblackbox.api.training.Training.__init__","title":"<code>__init__(training_name, from_config=False, model=None, dataset=None, from_preset='adaptive', pytest=False, verbose=False, **kwargs_optional)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>training_name</code> <code>str</code> <p>e.g. 'my_training'</p> required <code>from_config</code> <code>bool</code> <p>True =&gt; read parameters from config file (static definition). False =&gt; use Training arguments (dynamic definition)</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>[equivalent to model_name] e.g. 'bert-base-cased'</p> <code>None</code> <code>dataset</code> <code>Optional[str]</code> <p>[equivalent to dataset_name] e.g. 'conll2003'</p> <code>None</code> <code>from_preset</code> <code>Optional[str]</code> <p>True =&gt; use parameters from preset</p> <code>'adaptive'</code> <code>pytest</code> <code>bool</code> <p>only for testing, don't specify</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>True =&gt; verbose output</p> <code>False</code> <code>**kwargs_optional</code> <code>Any</code> <p>parameters</p> <code>{}</code>"},{"location":"python_api/training/#nerblackbox.api.training.Training.get_result","title":"<code>get_result(metric='f1', level='entity', label='micro', phase='test', average=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>\"f1\", \"precision\", \"recall\"</p> <code>'f1'</code> <code>level</code> <code>str</code> <p>\"entity\" or \"token\"</p> <code>'entity'</code> <code>label</code> <code>str</code> <p>\"micro\", \"macro\", \"PER\", ..</p> <code>'micro'</code> <code>phase</code> <code>str</code> <p>\"val\" or \"test\"</p> <code>'test'</code> <code>average</code> <code>bool</code> <p>if True, return average result of all runs. if False, return result of best run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Optional[str]</code> <p>e.g. \"0.9011 +- 0.0023\" (average = True) or \"0.9045\" (average = False)</p>"},{"location":"python_api/training/#nerblackbox.api.training.Training.run","title":"<code>run()</code>","text":"<p>run a single training.</p> <p>Note:</p> <ul> <li> <p>from_config == True -&gt; training config file is used, no other optional arguments will be used</p> </li> <li> <p>from_config == False -&gt; training config file is created dynamically, optional arguments will be used</p> <ul> <li> <p>model and dataset are mandatory.</p> </li> <li> <p>All other arguments relate to hyperparameters and are optional. They are determined using the following hierarchy:</p> <p>1) optional argument</p> <p>2) from_preset (adaptive, original, stable),    which specifies e.g. the hyperparameters \"max_epochs\", \"early_stopping\", \"lr_schedule\"</p> <p>3) default training configuration</p> </li> </ul> </li> </ul>"},{"location":"python_api/training/#nerblackbox.api.training.Training.show_config","title":"<code>show_config()</code>","text":"<p>print training config</p>"}]}